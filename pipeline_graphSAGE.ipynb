{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d55f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ì¤‘: av_contents.csv\n",
      "ì²˜ë¦¬ ì¤‘: contents.csv\n",
      "ì²˜ë¦¬ ì¤‘: content_ratings.csv\n",
      "ì²˜ë¦¬ ì¤‘: game_contents.csv\n",
      "ì²˜ë¦¬ ì¤‘: llm_recommendation_requests.csv\n",
      "ì²˜ë¦¬ ì¤‘: naver_series_novel.csv\n",
      "ì²˜ë¦¬ ì¤‘: platform_data.csv\n",
      "ì²˜ë¦¬ ì¤‘: raw_items.csv\n",
      "ì²˜ë¦¬ ì¤‘: transform_runs.csv\n",
      "ì²˜ë¦¬ ì¤‘: users.csv\n",
      "ì²˜ë¦¬ ì¤‘: user_preferences.csv\n",
      "ì²˜ë¦¬ ì¤‘: user_preferred_content_types.csv\n",
      "ì²˜ë¦¬ ì¤‘: user_preferred_genres.csv\n",
      "ì²˜ë¦¬ ì¤‘: user_roles.csv\n",
      "ì²˜ë¦¬ ì¤‘: webnovel_contents.csv\n",
      "ì²˜ë¦¬ ì¤‘: webtoon_contents.csv\n",
      "âœ… ìš”ì•½ íŒŒì¼ ìƒì„± ì™„ë£Œ: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\csv_summary.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. ê¸°ë³¸ ê²½ë¡œ ì„¤ì •\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_dir = r\"C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\"\n",
    "output_file = os.path.join(base_dir, \"csv_summary.txt\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. CSV ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "csv_files = [f for f in os.listdir(base_dir) if f.lower().endswith(\".csv\")]\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"âŒ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    exit()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. ê²°ê³¼ íŒŒì¼ ì´ˆê¸°í™”\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    f_out.write(\"ğŸ“Š CSV íŒŒì¼ ìš”ì•½ ë³´ê³ ì„œ\\n\")\n",
    "    f_out.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. ê° CSV íŒŒì¼ì— ëŒ€í•œ ì •ë³´ ìˆ˜ì§‘\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    print(f\"ì²˜ë¦¬ ì¤‘: {file_name}\")\n",
    "\n",
    "    # íŒŒì¼ ì½ê¸° (UTF-8 â†’ CP949 ìˆœìœ¼ë¡œ ì‹œë„)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file_path, encoding=\"cp949\")\n",
    "    except Exception as e:\n",
    "        with open(output_file, \"a\", encoding=\"utf-8\") as f_out:\n",
    "            f_out.write(f\"âš ï¸ {file_name} ì½ê¸° ì˜¤ë¥˜: {e}\\n\\n\")\n",
    "        continue\n",
    "\n",
    "    # â”€â”€ ìƒìœ„ 5ê°œ ìƒ˜í”Œë§Œ í‘œì‹œ â”€â”€\n",
    "    sample = df.head(5)\n",
    "\n",
    "    # â”€â”€ ê²°ê³¼ ì‘ì„± â”€â”€\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f_out:\n",
    "        f_out.write(f\"ğŸ“„ íŒŒì¼ëª…: {file_name}\\n\")\n",
    "        f_out.write(f\"ì»¬ëŸ¼ ({len(df.columns)}ê°œ): {list(df.columns)}\\n\\n\")\n",
    "        f_out.write(\"ìƒ˜í”Œ 5í–‰:\\n\")\n",
    "        f_out.write(sample.to_string(index=False))\n",
    "        f_out.write(\"\\n\" + \"-\" * 60 + \"\\n\\n\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. ì™„ë£Œ ë©”ì‹œì§€\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"âœ… ìš”ì•½ íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "953bf7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ì¤‘: av_contents.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\av_contents.json (records: 13)\n",
      "ì²˜ë¦¬ ì¤‘: contents.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\contents.json (records: 139)\n",
      "ì²˜ë¦¬ ì¤‘: content_ratings.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\content_ratings.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: game_contents.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\game_contents.json (records: 22)\n",
      "ì²˜ë¦¬ ì¤‘: llm_recommendation_requests.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LG\\AppData\\Local\\Temp\\ipykernel_27168\\4233549663.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n",
      "C:\\Users\\LG\\AppData\\Local\\Temp\\ipykernel_27168\\4233549663.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n",
      "C:\\Users\\LG\\AppData\\Local\\Temp\\ipykernel_27168\\4233549663.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\llm_recommendation_requests.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: naver_series_novel.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\naver_series_novel.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: platform_data.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\platform_data.json (records: 123)\n",
      "ì²˜ë¦¬ ì¤‘: raw_items.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LG\\AppData\\Local\\Temp\\ipykernel_27168\\4233549663.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n",
      "C:\\Users\\LG\\AppData\\Local\\Temp\\ipykernel_27168\\4233549663.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\raw_items.json (records: 6)\n",
      "ì²˜ë¦¬ ì¤‘: transform_runs.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\transform_runs.json (records: 250)\n",
      "ì²˜ë¦¬ ì¤‘: users.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\users.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: user_preferences.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\user_preferences.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: user_preferred_content_types.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\user_preferred_content_types.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: user_preferred_genres.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\user_preferred_genres.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: user_roles.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\user_roles.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: webnovel_contents.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\webnovel_contents.json (records: 93)\n",
      "ì²˜ë¦¬ ì¤‘: webtoon_contents.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\webtoon_contents.json (records: 0)\n",
      "\n",
      "âœ… ì™„ë£Œ: ëª¨ë“  JSONì€ clean/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ====== ì„¤ì • ======\n",
    "BASE_DIR = r\"C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\"\n",
    "OUT_DIR  = os.path.join(BASE_DIR, \"clean\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ì¸ì½”ë”© ì‹œë„ ìˆœì„œ\n",
    "ENCODINGS = [\"utf-8-sig\", \"utf-8\", \"cp949\", \"euc-kr\", \"latin1\"]\n",
    "\n",
    "# ê´„í˜¸ ë‚´ë¶€ ì½¤ë§ˆ ì¹˜í™˜ìš© (í† í¬ë‚˜ì´ì§• ì˜¤ë¥˜ íšŒí”¼)\n",
    "COMMA_PLACEHOLDER = \"âŸ\"\n",
    "\n",
    "DATE_COL_PAT = re.compile(r\"(date|_at)$\", re.IGNORECASE)  # release_date, created_at ë“±\n",
    "\n",
    "\n",
    "# ========== ìœ í‹¸ ==========\n",
    "\n",
    "def read_text_with_encodings(path: str) -> str:\n",
    "    for enc in ENCODINGS:\n",
    "        try:\n",
    "            with io.open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n",
    "                return f.read()\n",
    "        except Exception:\n",
    "            continue\n",
    "    # ìµœí›„: ì—ëŸ¬ ë¬´ì‹œ ë¼í‹´1\n",
    "    with io.open(path, \"r\", encoding=\"latin1\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def protect_commas_inside_brackets(line: str) -> str:\n",
    "    \"\"\"{},[],() ë‚´ë¶€ì˜ ì½¤ë§ˆë¥¼ ì„ì‹œ ì¹˜í™˜í•´ì„œ í•„ë“œ ìˆ˜ ë¶ˆì¼ì¹˜ ë°©ì§€\"\"\"\n",
    "    res, stack = [], []\n",
    "    for ch in line:\n",
    "        if ch in \"{[(\":\n",
    "            stack.append(ch); res.append(ch)\n",
    "        elif ch in \"}])\":\n",
    "            if stack: stack.pop()\n",
    "            res.append(ch)\n",
    "        else:\n",
    "            if ch == \",\" and stack:\n",
    "                res.append(COMMA_PLACEHOLDER)\n",
    "            else:\n",
    "                res.append(ch)\n",
    "    return \"\".join(res)\n",
    "\n",
    "def make_safe_temp_csv_text(raw_text: str) -> str:\n",
    "    return \"\\n\".join(protect_commas_inside_brackets(ln) for ln in raw_text.splitlines())\n",
    "\n",
    "def try_read_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"ì¼ë°˜â†’ì¹˜í™˜í…ìŠ¤íŠ¸â†’bad_lines skip ìˆœì„œë¡œ ë¡œë“œ ì‹œë„\"\"\"\n",
    "    raw = read_text_with_encodings(path)\n",
    "\n",
    "    # 1) ì¼ë°˜ ì‹œë„\n",
    "    for enc in ENCODINGS + [\"latin1\"]:\n",
    "        try:\n",
    "            return pd.read_csv(io.StringIO(raw), encoding=enc, engine=\"python\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) ê´„í˜¸ ë‚´ë¶€ ì½¤ë§ˆ ì¹˜í™˜ í›„ ì¬ì‹œë„\n",
    "    safe = make_safe_temp_csv_text(raw)\n",
    "    for enc in ENCODINGS + [\"latin1\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(io.StringIO(safe), encoding=enc, engine=\"python\")\n",
    "            # ì¹˜í™˜ ì›ë³µ\n",
    "            df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n",
    "            return df\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) ìµœí›„: bad lines skip\n",
    "    df = pd.read_csv(io.StringIO(safe), engine=\"python\", on_bad_lines=\"skip\")\n",
    "    df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "def parse_json_cell(val):\n",
    "    \"\"\"CSV ì•ˆ JSON ë¬¸ìì—´(ì´ì¤‘ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ í¬í•¨)ì„ dict/listë¡œ íŒŒì‹±(ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€)\"\"\"\n",
    "    if not isinstance(val, str): return val\n",
    "    txt = val.strip()\n",
    "    if not txt: return val\n",
    "    # \"{\"\"k\"\":1}\" -> {\"k\":1}\n",
    "    if txt.startswith('\"') and txt.endswith('\"'):\n",
    "        txt = txt[1:-1]\n",
    "    txt = txt.replace('\"\"', '\"')\n",
    "    if (txt.startswith(\"{\") and txt.endswith(\"}\")) or (txt.startswith(\"[\") and txt.endswith(\"]\")):\n",
    "        try:\n",
    "            return json.loads(txt)\n",
    "        except Exception:\n",
    "            return val\n",
    "    return val\n",
    "\n",
    "def is_jsonish_series(s: pd.Series) -> bool:\n",
    "    \"\"\"ì‹œë¦¬ì¦ˆì˜ ì•ë¶€ë¶„ì„ ë³´ê³  JSON ë¬¸ìì—´ ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ True\"\"\"\n",
    "    sample = s.dropna().astype(str).head(20).tolist()\n",
    "    if not sample: return False\n",
    "    cnt = 0\n",
    "    for v in sample:\n",
    "        v = v.strip().strip('\"')\n",
    "        if (v.startswith(\"{\") and v.endswith(\"}\")) or (v.startswith(\"[\") and v.endswith(\"]\")):\n",
    "            cnt += 1\n",
    "    return cnt >= max(3, len(sample)//4)\n",
    "\n",
    "def normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # 1) JSON ë¬¸ìì—´ë¡œ ë³´ì´ëŠ” ì»¬ëŸ¼ íŒŒì‹±\n",
    "    for col in out.columns:\n",
    "        try:\n",
    "            if out[col].dtype == \"object\" and is_jsonish_series(out[col]):\n",
    "                out[col] = out[col].apply(parse_json_cell)\n",
    "        except Exception:\n",
    "            # ì»¬ëŸ¼ ë‹¨ìœ„ ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ì§„í–‰\n",
    "            pass\n",
    "\n",
    "    # 2) ë‚ ì§œ ì»¬ëŸ¼ ISO ë¬¸ìì—´í™”\n",
    "    for col in out.columns:\n",
    "        if DATE_COL_PAT.search(col):\n",
    "            try:\n",
    "                out[col] = pd.to_datetime(out[col], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 3) NaN/NaT -> None, numpy íƒ€ì… -> íŒŒì´ì¬ ê¸°ë³¸ íƒ€ì…í™”\n",
    "    out = out.where(pd.notnull(out), None)\n",
    "\n",
    "    return out\n",
    "\n",
    "def df_to_json_records(df: pd.DataFrame):\n",
    "    \"\"\"JSON ì§ë ¬í™” ì¹œí™”ì ì¸ records ìƒì„±\"\"\"\n",
    "    # pandas ê¸°ë³¸ dict ë³€í™˜\n",
    "    recs = df.to_dict(orient=\"records\")\n",
    "\n",
    "    # numpy íƒ€ì…, Timestamp ë“± ì²˜ë¦¬\n",
    "    def py_convert(v):\n",
    "        if isinstance(v, (np.integer,)): return int(v)\n",
    "        if isinstance(v, (np.floating,)) and not np.isnan(v): return float(v)\n",
    "        if isinstance(v, (np.bool_,)): return bool(v)\n",
    "        # Timestamp/Datetime -> ISO ë¬¸ìì—´\n",
    "        if isinstance(v, (pd.Timestamp,)): return v.strftime(\"%Y-%m-%dT%H:%M:%S\") if pd.notna(v) else None\n",
    "        return v\n",
    "\n",
    "    def walk(o):\n",
    "        if isinstance(o, dict):\n",
    "            return {k: walk(py_convert(v)) for k, v in o.items()}\n",
    "        if isinstance(o, list):\n",
    "            return [walk(py_convert(x)) for x in o]\n",
    "        return py_convert(o)\n",
    "\n",
    "    return [walk(r) for r in recs]\n",
    "\n",
    "\n",
    "# ========== ë©”ì¸ ==========\n",
    "\n",
    "def main():\n",
    "    csv_files = [f for f in os.listdir(BASE_DIR) if f.lower().endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        print(\"âŒ CSVê°€ ì—†ìŠµë‹ˆë‹¤.\"); return\n",
    "\n",
    "    for fn in csv_files:\n",
    "        src = os.path.join(BASE_DIR, fn)\n",
    "        stem = os.path.splitext(fn)[0]\n",
    "        dst_json = os.path.join(OUT_DIR, f\"{stem}.json\")\n",
    "\n",
    "        try:\n",
    "            print(f\"ì²˜ë¦¬ ì¤‘: {fn}\")\n",
    "            df = try_read_csv(src)\n",
    "            df_clean = normalize_df(df)\n",
    "            records = df_to_json_records(df_clean)\n",
    "\n",
    "            with open(dst_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"  âœ“ ì €ì¥: {dst_json} (records: {len(records)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ ì‹¤íŒ¨: {fn} -> {e}\")\n",
    "\n",
    "    print(\"\\nâœ… ì™„ë£Œ: ëª¨ë“  JSONì€ clean/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da84b533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìš”ì•½ íŒŒì¼ ìƒì„±: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\csv_quick_summary.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\"\n",
    "OUT_PATH = os.path.join(BASE_DIR, \"csv_quick_summary.txt\")\n",
    "\n",
    "# ì¸ì½”ë”© ì¬ì‹œë„ ìˆœì„œ\n",
    "ENCODINGS = [\"utf-8-sig\", \"utf-8\", \"cp949\", \"euc-kr\", \"latin1\"]\n",
    "\n",
    "def read_one_row(path):\n",
    "    \"\"\"ì—¬ëŸ¬ ì¸ì½”ë”©/ì˜µì…˜ìœ¼ë¡œ CSV 1í–‰ë§Œ ì½ê¸°.\"\"\"\n",
    "    last_err = None\n",
    "    for enc in ENCODINGS:\n",
    "        try:\n",
    "            # engine='python'ìœ¼ë¡œ í† í¬ë‚˜ì´ì§•ì— ì¢€ ë” ê´€ëŒ€í•˜ê²Œ\n",
    "            df = pd.read_csv(path, nrows=1, encoding=enc, engine=\"python\", on_bad_lines=\"skip\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    # ìµœí›„: ì—”ì§„ ê¸°ë³¸ìœ¼ë¡œ ì¬ì‹œë„\n",
    "    try:\n",
    "        df = pd.read_csv(path, nrows=1, engine=\"python\", on_bad_lines=\"skip\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "        raise last_err\n",
    "\n",
    "def trim_val(v, maxlen=200):\n",
    "    \"\"\"ìƒ˜í”Œ ê°’ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°.\"\"\"\n",
    "    s = \"\" if (v is None or (isinstance(v, float) and pd.isna(v))) else str(v)\n",
    "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "    if len(s) > maxlen:\n",
    "        s = s[:maxlen] + \"â€¦\"\n",
    "    return s\n",
    "\n",
    "def summarize_csvs():\n",
    "    csv_files = [f for f in os.listdir(BASE_DIR) if f.lower().endswith(\".csv\")]\n",
    "    csv_files.sort()\n",
    "\n",
    "    with io.open(OUT_PATH, \"w\", encoding=\"utf-8\") as out:\n",
    "        out.write(\"ğŸ“„ CSV Quick Summary\\n\")\n",
    "        out.write(\"=\"*60 + \"\\n\\n\")\n",
    "\n",
    "        if not csv_files:\n",
    "            out.write(\"âŒ í´ë”ì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\\n\")\n",
    "            return\n",
    "\n",
    "        for fn in csv_files:\n",
    "            path = os.path.join(BASE_DIR, fn)\n",
    "            out.write(f\"íŒŒì¼ëª…: {fn}\\n\")\n",
    "            try:\n",
    "                df = read_one_row(path)\n",
    "                cols = list(df.columns)\n",
    "                out.write(f\"ì»¬ëŸ¼({len(cols)}): {cols}\\n\")\n",
    "\n",
    "                if df.shape[0] == 0:\n",
    "                    out.write(\"ìƒ˜í”Œ 1í–‰: (ë¹ˆ íŒŒì¼ ë˜ëŠ” í—¤ë”ë§Œ ì¡´ì¬)\\n\")\n",
    "                else:\n",
    "                    row = df.iloc[0].to_dict()\n",
    "                    # ê°’ íŠ¸ë¦¬ë°\n",
    "                    row = {k: trim_val(v) for k, v in row.items()}\n",
    "                    out.write(\"ìƒ˜í”Œ 1í–‰:\\n\")\n",
    "                    # ë³´ê¸° ì¢‹ê²Œ key=value í˜•íƒœë¡œ í•œ ì¤„\n",
    "                    row_str = \" | \".join(f\"{k}={row.get(k, '')}\" for k in cols)\n",
    "                    out.write(row_str + \"\\n\")\n",
    "            except Exception as e:\n",
    "                out.write(f\"âš ï¸ ì½ê¸° ì˜¤ë¥˜: {e}\\n\")\n",
    "            out.write(\"-\"*60 + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… ìš”ì•½ íŒŒì¼ ìƒì„±: {OUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    summarize_csvs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54257923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>finished_at</th>\n",
       "      <th>status</th>\n",
       "      <th>domain</th>\n",
       "      <th>platform_name</th>\n",
       "      <th>produced_content_id</th>\n",
       "      <th>raw_id</th>\n",
       "      <th>rule_path</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>AV</td>\n",
       "      <td>TMDB</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>rules/av/tmdb.yml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>AV</td>\n",
       "      <td>TMDB</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>rules/av/tmdb.yml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>AV</td>\n",
       "      <td>TMDB</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>rules/av/tmdb.yml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>AV</td>\n",
       "      <td>TMDB</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>rules/av/tmdb.yml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>AV</td>\n",
       "      <td>TMDB</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>rules/av/tmdb.yml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>246</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>WEBNOVEL</td>\n",
       "      <td>KakaoPage</td>\n",
       "      <td>240</td>\n",
       "      <td>246</td>\n",
       "      <td>rules/webnovel/kakaopage.yml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>247</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>WEBNOVEL</td>\n",
       "      <td>KakaoPage</td>\n",
       "      <td>241</td>\n",
       "      <td>247</td>\n",
       "      <td>rules/webnovel/kakaopage.yml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>248</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>WEBNOVEL</td>\n",
       "      <td>KakaoPage</td>\n",
       "      <td>242</td>\n",
       "      <td>248</td>\n",
       "      <td>rules/webnovel/kakaopage.yml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>249</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>WEBNOVEL</td>\n",
       "      <td>KakaoPage</td>\n",
       "      <td>243</td>\n",
       "      <td>249</td>\n",
       "      <td>rules/webnovel/kakaopage.yml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>250</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>WEBNOVEL</td>\n",
       "      <td>KakaoPage</td>\n",
       "      <td>244</td>\n",
       "      <td>250</td>\n",
       "      <td>rules/webnovel/kakaopage.yml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     run_id  created_at finished_at   status    domain platform_name  \\\n",
       "0         1  2025-10-02  2025-10-02  SUCCESS        AV          TMDB   \n",
       "1         2  2025-10-02  2025-10-02  SUCCESS        AV          TMDB   \n",
       "2         3  2025-10-02  2025-10-02  SUCCESS        AV          TMDB   \n",
       "3         4  2025-10-02  2025-10-02  SUCCESS        AV          TMDB   \n",
       "4         5  2025-10-02  2025-10-02  SUCCESS        AV          TMDB   \n",
       "..      ...         ...         ...      ...       ...           ...   \n",
       "245     246  2025-10-02  2025-10-02  SUCCESS  WEBNOVEL     KakaoPage   \n",
       "246     247  2025-10-02  2025-10-02  SUCCESS  WEBNOVEL     KakaoPage   \n",
       "247     248  2025-10-02  2025-10-02  SUCCESS  WEBNOVEL     KakaoPage   \n",
       "248     249  2025-10-02  2025-10-02  SUCCESS  WEBNOVEL     KakaoPage   \n",
       "249     250  2025-10-02  2025-10-02  SUCCESS  WEBNOVEL     KakaoPage   \n",
       "\n",
       "     produced_content_id  raw_id                     rule_path  error  \n",
       "0                      1       1             rules/av/tmdb.yml    NaN  \n",
       "1                      2       2             rules/av/tmdb.yml    NaN  \n",
       "2                      3       3             rules/av/tmdb.yml    NaN  \n",
       "3                      4       4             rules/av/tmdb.yml    NaN  \n",
       "4                      5       5             rules/av/tmdb.yml    NaN  \n",
       "..                   ...     ...                           ...    ...  \n",
       "245                  240     246  rules/webnovel/kakaopage.yml    NaN  \n",
       "246                  241     247  rules/webnovel/kakaopage.yml    NaN  \n",
       "247                  242     248  rules/webnovel/kakaopage.yml    NaN  \n",
       "248                  243     249  rules/webnovel/kakaopage.yml    NaN  \n",
       "249                  244     250  rules/webnovel/kakaopage.yml    NaN  \n",
       "\n",
       "[250 rows x 10 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV ê²½ë¡œ\n",
    "csv_path = r\"C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\transform_runs.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67294444",
   "metadata": {},
   "source": [
    "# ì´ë¶„ ê·¸ë˜í”„ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad23ee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… graph_nodes ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\graph_nodes.csv (rows=139)\n",
      "âœ… content_raw_genres ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\content_raw_genres.csv (rows=33)\n",
      "âœ… meta_nodes ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\meta_nodes.csv (rows=12)\n",
      "âœ… graph_edges_bipartite ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\graph_edges_bipartite.csv (edges=172)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "BASE = r\"C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\"\n",
    "PATH_CONTENTS = os.path.join(BASE, \"contents.csv\")\n",
    "PATH_AV       = os.path.join(BASE, \"av_contents.csv\")\n",
    "PATH_GAME     = os.path.join(BASE, \"game_contents.csv\")\n",
    "PATH_WEBNOVEL = os.path.join(BASE, \"webnovel_contents.csv\")  # ìˆìœ¼ë©´ ì‚¬ìš©\n",
    "\n",
    "OUT_NODES     = os.path.join(BASE, \"graph_nodes.csv\")              # ì½˜í…ì¸  ë…¸ë“œ\n",
    "OUT_RAWGEN    = os.path.join(BASE, \"content_raw_genres.csv\")       # ì›ë³¸ ì¥ë¥´ ì„¸ë¡œí‘œ\n",
    "OUT_META      = os.path.join(BASE, \"meta_nodes.csv\")               # ë©”íƒ€ë…¸ë“œ ëª©ë¡\n",
    "OUT_EDGES_BI  = os.path.join(BASE, \"graph_edges_bipartite.csv\")    # ì½˜í…ì¸ â†’ë©”íƒ€ ì—£ì§€\n",
    "\n",
    "# ì—£ì§€ ê°€ì¤‘ì¹˜(í•„ìš” ì‹œ ì¡°ì •)\n",
    "DOMAIN_EDGE_WEIGHT = 1.0\n",
    "GENRE_EDGE_WEIGHT  = 1.0\n",
    "\n",
    "ENCODINGS = [\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"]\n",
    "def read_csv_retry(path, **kwargs):\n",
    "    last = None\n",
    "    for enc in ENCODINGS:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kwargs)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "def safe_read(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"âš ï¸ ì—†ìŒ: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        return read_csv_retry(path)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì½ê¸° ì‹¤íŒ¨: {path} -> {e}\")\n",
    "        return None\n",
    "\n",
    "def add_prefix_except_key(df, prefix, key=\"content_id\"):\n",
    "    if df is None: return None\n",
    "    return df.rename(columns={c: (c if c==key else f\"{prefix}{c}\") for c in df.columns})\n",
    "\n",
    "def norm_for_id(s: str) -> str:\n",
    "    \"\"\"\n",
    "    ë©”íƒ€ë…¸ë“œ IDìš© ê°„ë‹¨ ì •ê·œí™” (ê³µë°±/êµ¬ë‘ì  ì œê±°, ì†Œë¬¸ì)\n",
    "    ì˜ˆ: 'Science Fiction' -> 'science_fiction'\n",
    "    \"\"\"\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    # í•œ/ì˜/ìˆ«ì ì™¸ëŠ” ê³µë°± ì¹˜í™˜\n",
    "    s = re.sub(r\"[^\\w\\sê°€-í£]\", \" \", s)\n",
    "    s = \"_\".join(s.split())\n",
    "    return s\n",
    "\n",
    "# 1) ì½˜í…ì¸  ë…¸ë“œ ìƒì„±(ê·¸ëŒ€ë¡œ ìœ ì§€)\n",
    "def build_nodes():\n",
    "    contents = safe_read(PATH_CONTENTS)\n",
    "    if contents is None or contents.empty:\n",
    "        raise RuntimeError(\"âŒ contents.csv ë¹„ì–´ìˆìŒ/ë¡œë“œ ì‹¤íŒ¨\")\n",
    "    if \"content_id\" not in contents.columns or \"domain\" not in contents.columns:\n",
    "        raise RuntimeError(\"âŒ contents.csvì— content_id/domain ì»¬ëŸ¼ í•„ìš”\")\n",
    "\n",
    "    av   = add_prefix_except_key(safe_read(PATH_AV), \"av_\")\n",
    "    game = add_prefix_except_key(safe_read(PATH_GAME), \"game_\")\n",
    "    web  = add_prefix_except_key(safe_read(PATH_WEBNOVEL), \"webnovel_\")\n",
    "\n",
    "    def typed_left_merge(left, right, key=\"content_id\"):\n",
    "        if right is None or right.empty:\n",
    "            return left.copy()\n",
    "        L = left.copy(); R = right.copy()\n",
    "        if key in L.columns: L[key] = pd.to_numeric(L[key], errors=\"coerce\").astype(\"Int64\")\n",
    "        if key in R.columns: R[key] = pd.to_numeric(R[key], errors=\"coerce\").astype(\"Int64\")\n",
    "        return L.merge(R, on=key, how=\"left\")\n",
    "\n",
    "    parts = []\n",
    "    doms = contents[\"domain\"].dropna().unique().tolist()\n",
    "    if \"AV\" in doms:\n",
    "        sub = contents[contents[\"domain\"]==\"AV\"].copy()\n",
    "        sub = typed_left_merge(sub, av); parts.append(sub)\n",
    "    if \"GAME\" in doms:\n",
    "        sub = contents[contents[\"domain\"]==\"GAME\"].copy()\n",
    "        sub = typed_left_merge(sub, game); parts.append(sub)\n",
    "    if \"WEBNOVEL\" in doms:\n",
    "        sub = contents[contents[\"domain\"]==\"WEBNOVEL\"].copy()\n",
    "        sub = typed_left_merge(sub, web); parts.append(sub)\n",
    "\n",
    "    known = {\"AV\",\"GAME\",\"WEBNOVEL\"}\n",
    "    nodes = pd.concat(parts, ignore_index=True) if parts else contents[contents[\"domain\"].isin(known)].copy()\n",
    "    nodes = nodes.drop_duplicates(subset=[\"content_id\"], keep=\"first\")\n",
    "\n",
    "    # ë³´ê¸° ì¢‹ê²Œ ì •ë ¬\n",
    "    first = [c for c in [\"content_id\",\"domain\",\"master_title\",\"original_title\",\"release_year\",\n",
    "                         \"poster_image_url\",\"created_at\",\"updated_at\",\"synopsis\"] if c in nodes.columns]\n",
    "    nodes = nodes[first + [c for c in nodes.columns if c not in first]]\n",
    "\n",
    "    nodes.to_csv(OUT_NODES, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… graph_nodes ì €ì¥: {OUT_NODES} (rows={len(nodes)})\")\n",
    "    return nodes\n",
    "\n",
    "# 2) ì›ë³¸ ì¥ë¥´ ì„¸ë¡œí‘œ(content_id, source, raw_genre)\n",
    "def build_raw_genres(nodes):\n",
    "    valid_ids = set(pd.to_numeric(nodes[\"content_id\"], errors=\"coerce\").dropna().astype(int))\n",
    "    rows = []\n",
    "\n",
    "    # AV: TMDB ì¥ë¥´ (genres.tmdb_genres.N.name)\n",
    "    av = safe_read(PATH_AV)\n",
    "    if av is not None and not av.empty:\n",
    "        name_cols = [c for c in av.columns if c.startswith(\"genres.tmdb_genres.\") and c.endswith(\".name\")]\n",
    "        if name_cols:\n",
    "            tmp = av.melt(id_vars=[\"content_id\"], value_vars=name_cols, value_name=\"raw_genre\").dropna(subset=[\"raw_genre\"])\n",
    "            tmp[\"raw_genre\"] = tmp[\"raw_genre\"].astype(str).str.strip()\n",
    "            tmp = tmp[(tmp[\"raw_genre\"]!=\"\") & (tmp[\"content_id\"].isin(valid_ids))]\n",
    "            tmp[\"source\"] = \"tmdb\"\n",
    "            rows.append(tmp[[\"content_id\",\"source\",\"raw_genre\"]])\n",
    "\n",
    "    # GAME: Steam ì¥ë¥´ (genres_str: \"A;B;C\")\n",
    "    game = safe_read(PATH_GAME)\n",
    "    if game is not None and not game.empty and \"genres_str\" in game.columns:\n",
    "        g2 = game.dropna(subset=[\"genres_str\"]).copy()\n",
    "        g2[\"raw_genre\"] = g2[\"genres_str\"].astype(str).str.split(r\"\\s*;\\s*\")\n",
    "        g2 = g2.explode(\"raw_genre\").dropna(subset=[\"raw_genre\"])\n",
    "        g2[\"raw_genre\"] = g2[\"raw_genre\"].astype(str).str.strip()\n",
    "        g2 = g2[(g2[\"raw_genre\"]!=\"\") & (g2[\"content_id\"].isin(valid_ids))]\n",
    "        g2[\"source\"] = \"steam\"\n",
    "        rows.append(g2[[\"content_id\",\"source\",\"raw_genre\"]])\n",
    "\n",
    "    # WEBNOVEL: ì¥ë¥´ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ë¶„í•´ (ì˜ˆ: 'genres'ê°€ \"íŒíƒ€ì§€;ë¡œë§¨ìŠ¤\")\n",
    "    web = safe_read(PATH_WEBNOVEL)\n",
    "    if web is not None and not web.empty:\n",
    "        # í›„ë³´ ì»¬ëŸ¼ë“¤ ì¤‘ ì¡´ì¬í•˜ëŠ” ê²ƒ ì„ íƒ\n",
    "        cand_cols = [c for c in [\"genres\",\"genre\",\"genre_str\"] if c in web.columns]\n",
    "        if cand_cols:\n",
    "            col = cand_cols[0]\n",
    "            w2 = web.dropna(subset=[col]).copy()\n",
    "            # êµ¬ë¶„ì: ; , / | (í˜¼ìš© ë°©ì§€ìš© ì •ê·œì‹)\n",
    "            w2[\"raw_genre\"] = w2[col].astype(str).str.split(r\"\\s*[;|/,]\\s*\")\n",
    "            w2 = w2.explode(\"raw_genre\").dropna(subset=[\"raw_genre\"])\n",
    "            w2[\"raw_genre\"] = w2[\"raw_genre\"].astype(str).str.strip()\n",
    "            if \"content_id\" in w2.columns:\n",
    "                w2 = w2[w2[\"raw_genre\"]!=\"\"]\n",
    "                w2 = w2[w2[\"content_id\"].isin(valid_ids)]\n",
    "                w2[\"source\"] = \"webnovel\"\n",
    "                rows.append(w2[[\"content_id\",\"source\",\"raw_genre\"]])\n",
    "\n",
    "    df = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=[\"content_id\",\"source\",\"raw_genre\"])\n",
    "    if not df.empty:\n",
    "        df[\"content_id\"] = pd.to_numeric(df[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df = df[df[\"content_id\"].notna()]\n",
    "        df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    df.to_csv(OUT_RAWGEN, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… content_raw_genres ì €ì¥: {OUT_RAWGEN} (rows={len(df)})\")\n",
    "    return df\n",
    "\n",
    "# 3) ë©”íƒ€ë…¸ë“œ + ì´ë¶„ ì—£ì§€ ìƒì„± (ë„ë©”ì¸/ì¥ë¥´ ëª¨ë‘ 'ì›ë³¸' ê¸°ì¤€)\n",
    "def build_bipartite(nodes, raw_genres):\n",
    "    # ë©”íƒ€ë…¸ë“œ: ë„ë©”ì¸\n",
    "    domain_nodes = sorted(nodes[\"domain\"].dropna().unique().tolist())\n",
    "\n",
    "    meta_rows = []\n",
    "    for d in domain_nodes:\n",
    "        meta_rows.append({\"meta_id\": f\"DOM:{d}\", \"meta_type\": \"domain\", \"label\": d, \"source\": \"\"})\n",
    "\n",
    "    # ë©”íƒ€ë…¸ë“œ: ì¥ë¥´ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì ìš©)\n",
    "    if raw_genres is not None and not raw_genres.empty:\n",
    "        # (source, raw_genre_norm)ë³„ í•˜ë‚˜ì˜ ë©”íƒ€ë…¸ë“œ\n",
    "        rg = raw_genres.copy()\n",
    "        rg[\"raw_genre_norm\"] = rg[\"raw_genre\"].astype(str).str.strip()\n",
    "        rg[\"raw_genre_norm\"] = rg[\"raw_genre_norm\"].replace({\"\": None})\n",
    "        rg = rg.dropna(subset=[\"raw_genre_norm\"])\n",
    "        uniq = rg[[\"source\",\"raw_genre_norm\"]].drop_duplicates()\n",
    "        for src, g in uniq.itertuples(index=False):\n",
    "            meta_id = f\"GEN:{src}:{norm_for_id(g)}\" if g else None\n",
    "            if meta_id:\n",
    "                meta_rows.append({\"meta_id\": meta_id, \"meta_type\": \"genre\", \"label\": g, \"source\": src})\n",
    "\n",
    "    df_meta = pd.DataFrame(meta_rows, columns=[\"meta_id\",\"meta_type\",\"label\",\"source\"]).drop_duplicates()\n",
    "    df_meta.to_csv(OUT_META, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… meta_nodes ì €ì¥: {OUT_META} (rows={len(df_meta)})\")\n",
    "\n",
    "    # ì´ë¶„ ì—£ì§€: ì½˜í…ì¸  â†’ ë„ë©”ì¸\n",
    "    edges = []\n",
    "    sub = nodes[[\"content_id\",\"domain\"]].dropna()\n",
    "    for cid, dom in sub.itertuples(index=False):\n",
    "        edges.append({\n",
    "            \"src_content_id\": int(cid),\n",
    "            \"dst_meta_id\":    f\"DOM:{dom}\",\n",
    "            \"edge_type\":      \"content-domain\",\n",
    "            \"weight\":         DOMAIN_EDGE_WEIGHT\n",
    "        })\n",
    "\n",
    "    # ì´ë¶„ ì—£ì§€: ì½˜í…ì¸  â†’ (sourceë³„ ì›ë³¸ì¥ë¥´)\n",
    "    if raw_genres is not None and not raw_genres.empty:\n",
    "        for cid, src, g in raw_genres[[\"content_id\",\"source\",\"raw_genre\"]].itertuples(index=False):\n",
    "            g_norm = norm_for_id(g)\n",
    "            if not g_norm:\n",
    "                continue\n",
    "            meta_id = f\"GEN:{src}:{g_norm}\"\n",
    "            edges.append({\n",
    "                \"src_content_id\": int(cid),\n",
    "                \"dst_meta_id\":    meta_id,\n",
    "                \"edge_type\":      f\"content-genre-{src}\",\n",
    "                \"weight\":         GENRE_EDGE_WEIGHT\n",
    "            })\n",
    "\n",
    "    df_edges = pd.DataFrame(edges, columns=[\"src_content_id\",\"dst_meta_id\",\"edge_type\",\"weight\"]).drop_duplicates()\n",
    "    df_edges.to_csv(OUT_EDGES_BI, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… graph_edges_bipartite ì €ì¥: {OUT_EDGES_BI} (edges={len(df_edges)})\")\n",
    "    return df_meta, df_edges\n",
    "\n",
    "def main():\n",
    "    nodes = build_nodes()\n",
    "    rawg  = build_raw_genres(nodes)\n",
    "    build_bipartite(nodes, rawg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b35add",
   "metadata": {},
   "source": [
    "# ì•„ì´í…œ-ì•„ì´í…œ ê·¸ë˜í”„ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04945d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\graph_edges_item_item.csv (edges=3,531, items=139)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE = r\"C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\"\n",
    "PATH_NODES   = os.path.join(BASE, \"graph_nodes.csv\")\n",
    "PATH_RAWGEN  = os.path.join(BASE, \"content_raw_genres.csv\")\n",
    "OUT_EDGES    = os.path.join(BASE, \"graph_edges_item_item.csv\")\n",
    "\n",
    "# ========= í•˜ì´í¼íŒŒë¼ë¯¸í„°(ì¡°ì ˆ í¬ì¸íŠ¸) =========\n",
    "DOMAIN_BASE_WEIGHT    = 1.0   # ê°™ì€ ë„ë©”ì¸ ê³µìœ  ê¸°ì—¬ì¹˜\n",
    "GENRE_BASE_WEIGHT     = 1.0   # ê°™ì€ 'ë„ë©”ì¸ë³„ ì›ë³¸ì¥ë¥´' ê³µìœ  ê¸°ì—¬ì¹˜\n",
    "ALPHA                 = 0.7   # í—ˆë¸Œ ì™„í™” ì§€ìˆ˜(0=ì™„í™” ì—†ìŒ, 1=ê°•í•œ ì™„í™”). ë³´í†µ 0.5~0.8\n",
    "USE_IDF               = True  # ë©”íƒ€ë…¸ë“œ ì •ë³´ëŸ‰ ê°€ì¤‘ì¹˜(log(1+N/deg)) ì‚¬ìš©\n",
    "TOPK_PER_ITEM         = 100   # ì•„ì´í…œë‹¹ ìƒìœ„ K ì´ì›ƒë§Œ ìœ ì§€ (í¬ì†Œí™”)\n",
    "MAX_MEMBERS_PER_META  = 2000  # ë©”íƒ€ë…¸ë“œ(ë„ë©”ì¸/ì¥ë¥´) ë©¤ë²„ê°€ ë„ˆë¬´ í´ ë•Œ ìƒ˜í”Œë§ ìƒí•œ (ë©”ëª¨ë¦¬/ì‹œê°„ ë³´í˜¸)\n",
    "\n",
    "ENCODINGS = [\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"]\n",
    "def read_csv_retry(path, **kwargs):\n",
    "    last = None\n",
    "    for enc in ENCODINGS:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kwargs)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "def main():\n",
    "    # 1) ë¡œë“œ\n",
    "    nodes = read_csv_retry(PATH_NODES)\n",
    "    rawg  = read_csv_retry(PATH_RAWGEN) if os.path.exists(PATH_RAWGEN) else pd.DataFrame(columns=[\"content_id\",\"source\",\"raw_genre\"])\n",
    "\n",
    "    if nodes.empty:\n",
    "        raise RuntimeError(\"graph_nodes.csv ë¹„ì–´ìˆìŒ\")\n",
    "    if \"content_id\" not in nodes.columns or \"domain\" not in nodes.columns:\n",
    "        raise RuntimeError(\"graph_nodes.csvì— content_id/domain í•„ìš”\")\n",
    "\n",
    "    nodes = nodes.dropna(subset=[\"content_id\",\"domain\"]).copy()\n",
    "    nodes[\"content_id\"] = pd.to_numeric(nodes[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    nodes = nodes[nodes[\"content_id\"].notna()]\n",
    "    nodes[\"content_id\"] = nodes[\"content_id\"].astype(int)\n",
    "\n",
    "    # ë„ë©”ì¸ ë§µ\n",
    "    domain_of = dict(nodes[[\"content_id\",\"domain\"]].values)\n",
    "    all_items = set(domain_of.keys())\n",
    "    N_items   = max(1, len(all_items))\n",
    "\n",
    "    # 2) ë©”íƒ€ë…¸ë“œ ë©¤ë²„ êµ¬ì„±\n",
    "    # (a) ë„ë©”ì¸ ë©”íƒ€ë…¸ë“œ: DOM:<domain>\n",
    "    members_of_meta = defaultdict(list)\n",
    "    for cid, dom in domain_of.items():\n",
    "        members_of_meta[f\"DOM:{dom}\"].append(cid)\n",
    "\n",
    "    # (b) 'ë„ë©”ì¸ë³„ ì›ë³¸ ì¥ë¥´' ë©”íƒ€ë…¸ë“œ: GEN:<source>:<raw_genre>\n",
    "    if not rawg.empty:\n",
    "        rawg = rawg.dropna(subset=[\"content_id\",\"source\",\"raw_genre\"]).copy()\n",
    "        rawg[\"content_id\"] = pd.to_numeric(rawg[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        rawg = rawg[rawg[\"content_id\"].notna()]\n",
    "        rawg[\"content_id\"] = rawg[\"content_id\"].astype(int)\n",
    "        # ê°™ì€ ë„ë©”ì¸ì—ì„œë§Œ ì˜ë¯¸ë¥¼ ì£¼ë ¤ë©´, ì•„ì´í…œì˜ ë„ë©”ì¸ì„ í™•ì¸í•´ì„œ ë¬¶ê¸°ë§Œ í•˜ë©´ ë¨.\n",
    "        # ì—¬ê¸°ì„  ë©”íƒ€ë…¸ë“œ IDì— ë„ë©”ì¸ê¹Œì§€ ë„£ì–´ 'ë„ë©”ì¸ë³„ ì¥ë¥´'ë¡œ ë¶„ë¦¬í•œë‹¤.\n",
    "        # ì˜ˆ) GEN:tmdb:ê³µí¬@AV, GEN:steam:Action@GAME\n",
    "        for cid, src, g in rawg[[\"content_id\",\"source\",\"raw_genre\"]].drop_duplicates().itertuples(index=False):\n",
    "            dom = domain_of.get(cid)\n",
    "            if not dom: \n",
    "                continue\n",
    "            meta_id = f\"GEN:{src}:{g}@{dom}\"\n",
    "            members_of_meta[meta_id].append(cid)\n",
    "\n",
    "    # 3) íˆ¬ì˜(ì•„ì´í…œâ†”ì•„ì´í…œ): ë©”íƒ€ë…¸ë“œ ê³µë™ì†Œì† ê¸°ë°˜ ê°€ì¤‘ì¹˜\n",
    "    def meta_contrib(meta_id, deg):\n",
    "        # deg = ë©”íƒ€ë…¸ë“œ ë©¤ë²„ ìˆ˜\n",
    "        base = DOMAIN_BASE_WEIGHT if meta_id.startswith(\"DOM:\") else GENRE_BASE_WEIGHT\n",
    "        if deg <= 1:\n",
    "            return 0.0\n",
    "        # í—ˆë¸Œ ì™„í™”: 1 / (deg^ALPHA)\n",
    "        w = base / (deg ** ALPHA)\n",
    "        # ì •ë³´ëŸ‰(IDF) ë³´ì •: log(1 + N / deg)\n",
    "        if USE_IDF:\n",
    "            w *= math.log(1.0 + (N_items / float(deg)))\n",
    "        return w\n",
    "\n",
    "    edges = defaultdict(float)\n",
    "\n",
    "    for meta_id, members in members_of_meta.items():\n",
    "        members = list(set(members))\n",
    "        deg = len(members)\n",
    "        if deg <= 1:\n",
    "            continue\n",
    "\n",
    "        # ë„ˆë¬´ í° ë©”íƒ€ë…¸ë“œ(ì˜ˆ: íŠ¹ì • ë„ë©”ì¸ ì „ì²´)ê°€ í­ë°œí•˜ì§€ ì•Šê²Œ ìƒ˜í”Œë§/ìƒí•œ\n",
    "        if deg > MAX_MEMBERS_PER_META:\n",
    "            # ê· ì¼ ìƒ˜í”Œë§ (ì¬í˜„ì„±ì€ ë³´ì¥X. í•„ìš”ì‹œ random.seed() ì¶”ê°€)\n",
    "            members = members[:MAX_MEMBERS_PER_META]\n",
    "            deg = len(members)\n",
    "\n",
    "        contrib = meta_contrib(meta_id, deg)\n",
    "        if contrib == 0.0:\n",
    "            continue\n",
    "\n",
    "        # ê·¼ë¦° ì œí•œ ë°©ì‹: ê° ë©¤ë²„ë§ˆë‹¤ ì•ìª½ Kê°œì™€ë§Œ ì—°ê²° (ì „ìŒ O(n^2) ë°©ì§€)\n",
    "        K = min(TOPK_PER_ITEM, deg - 1)\n",
    "        members.sort()\n",
    "        for i, a in enumerate(members):\n",
    "            # ë„ë©”ì¸ ë‹¤ë¥¸ ìŒì€ ì œì™¸ (ë„ë©”ì¸ ì¼ì¹˜ ê°•ì œ) â€” ë„ë©”ì¸ë³„ ì¥ë¥´ ë©”íƒ€ëŠ” ì´ë¯¸ ë¶„ë¦¬ë˜ì–´ ìˆì§€ë§Œ,\n",
    "            # ë„ë©”ì¸ ë©”íƒ€ì—ì„œë„ ì•ˆì „í•˜ê²Œ ë™ì¼ ë„ë©”ì¸ë§Œ ìœ ì§€\n",
    "            for b in members[i+1 : i+1+K]:\n",
    "                if domain_of.get(a) != domain_of.get(b):\n",
    "                    continue\n",
    "                if a > b:\n",
    "                    a, b = b, a\n",
    "                edges[(a,b)] += contrib\n",
    "\n",
    "    # 4) ì•„ì´í…œë‹¹ TOPK ì´ì›ƒë§Œ ìœ ì§€(ì–‘ë°©í–¥ ê¸°ì¤€ ìµœëŒ€ ê°€ì¤‘ì¹˜ ìœ ì§€)\n",
    "    nbrs = defaultdict(list)\n",
    "    for (a,b), w in edges.items():\n",
    "        nbrs[a].append((b,w))\n",
    "        nbrs[b].append((a,w))\n",
    "\n",
    "    pruned = {}\n",
    "    for a, lst in nbrs.items():\n",
    "        lst.sort(key=lambda x: x[1], reverse=True)\n",
    "        for b, w in lst[:TOPK_PER_ITEM]:\n",
    "            key = (a,b) if a < b else (b,a)\n",
    "            if key not in pruned or pruned[key] < w:\n",
    "                pruned[key] = w\n",
    "\n",
    "    df_edges = pd.DataFrame(\n",
    "        [(a,b,w) for (a,b), w in pruned.items()],\n",
    "        columns=[\"src_content_id\",\"dst_content_id\",\"weight\"]\n",
    "    )\n",
    "    df_edges.to_csv(OUT_EDGES, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì €ì¥: {OUT_EDGES} (edges={len(df_edges):,}, items={len(all_items):,})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9d5f7",
   "metadata": {},
   "source": [
    "# ì•„ì´í…œ-ì•„ì´í…œ ê°€ì¤‘ ê·¸ë˜í”„ graphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b6ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ ì—£ì§€ ë¡œë”©...\n",
      "nodes=139, edges=3,531\n",
      "ğŸ§  GraphSAGE í•™ìŠµ...\n",
      "[01/20] loss=0.7974  cos(pos)â‰ˆ0.845  cos(neg)â‰ˆ0.837\n",
      "[02/20] loss=0.7441  cos(pos)â‰ˆ0.741  cos(neg)â‰ˆ0.727\n",
      "[03/20] loss=0.7168  cos(pos)â‰ˆ0.582  cos(neg)â‰ˆ0.559\n",
      "[04/20] loss=0.7049  cos(pos)â‰ˆ0.421  cos(neg)â‰ˆ0.394\n",
      "[05/20] loss=0.7009  cos(pos)â‰ˆ0.336  cos(neg)â‰ˆ0.303\n",
      "[06/20] loss=0.7004  cos(pos)â‰ˆ0.330  cos(neg)â‰ˆ0.290\n",
      "[07/20] loss=0.7011  cos(pos)â‰ˆ0.355  cos(neg)â‰ˆ0.312\n",
      "[08/20] loss=0.7016  cos(pos)â‰ˆ0.378  cos(neg)â‰ˆ0.327\n",
      "[09/20] loss=0.7022  cos(pos)â‰ˆ0.403  cos(neg)â‰ˆ0.345\n",
      "[10/20] loss=0.7019  cos(pos)â‰ˆ0.410  cos(neg)â‰ˆ0.342\n",
      "[11/20] loss=0.7010  cos(pos)â‰ˆ0.414  cos(neg)â‰ˆ0.325\n",
      "[12/20] loss=0.6998  cos(pos)â‰ˆ0.389  cos(neg)â‰ˆ0.296\n",
      "[13/20] loss=0.6982  cos(pos)â‰ˆ0.363  cos(neg)â‰ˆ0.255\n",
      "[14/20] loss=0.6969  cos(pos)â‰ˆ0.347  cos(neg)â‰ˆ0.216\n",
      "[15/20] loss=0.6955  cos(pos)â‰ˆ0.319  cos(neg)â‰ˆ0.167\n",
      "[16/20] loss=0.6947  cos(pos)â‰ˆ0.315  cos(neg)â‰ˆ0.136\n",
      "[17/20] loss=0.6938  cos(pos)â‰ˆ0.317  cos(neg)â‰ˆ0.100\n",
      "[18/20] loss=0.6932  cos(pos)â‰ˆ0.309  cos(neg)â‰ˆ0.079\n",
      "[19/20] loss=0.6929  cos(pos)â‰ˆ0.322  cos(neg)â‰ˆ0.070\n",
      "[20/20] loss=0.6920  cos(pos)â‰ˆ0.352  cos(neg)â‰ˆ0.043\n",
      "ğŸ’¾ ì„ë² ë”© ì €ì¥...\n",
      "âœ… ì„ë² ë”© ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\item_embeddings_torch.csv (nodes=139, dim=128)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ì•„ì´í…œ-ì•„ì´í…œ ê°€ì¤‘ ê·¸ë˜í”„ -> GraphSAGE ì„ë² ë”© (PyTorch Geometric)\n",
    "# ì…ë ¥ : C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\graph_edges_item_item.csv\n",
    "#        (columns: src_content_id, dst_content_id, weight)\n",
    "# ì¶œë ¥ : C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\item_embeddings_torch.csv\n",
    "# ì‚¬ìš© : python learn_item_embeddings_sage.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Dict, Tuple\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "\n",
    "# ===== ê²½ë¡œ/í•˜ì´í¼ =====\n",
    "BASE = r\"C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\"\n",
    "EDGES_CSV = os.path.join(BASE, \"graph_edges_item_item.csv\")\n",
    "OUT_CSV   = os.path.join(BASE, \"item_embeddings_torch.csv\")  # downstreamê³¼ í˜¸í™˜\n",
    "\n",
    "SEED         = 42\n",
    "EMB_DIM      = 128      # ìµœì¢… ì„ë² ë”© ì°¨ì›\n",
    "HIDDEN       = 256\n",
    "LAYERS       = 2        # SAGE ë ˆì´ì–´ ìˆ˜\n",
    "DROPOUT      = 0.1\n",
    "LR           = 1e-3\n",
    "EPOCHS       = 20\n",
    "NEG_MULT     = 5        # ë¯¸ë‹ˆë°°ì¹˜ ìŒì„± ì—£ì§€ ë¹„ìœ¨\n",
    "USE_EDGE_W   = True     # ì—£ì§€ weight ë°˜ì˜(BCE ê°€ì¤‘ì¹˜)\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ---------- CSV ë¡œë”© ----------\n",
    "def read_edges(path: str) -> pd.DataFrame:\n",
    "    encs = [\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"]\n",
    "    last=None\n",
    "    for enc in encs:\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=enc)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last=e\n",
    "    else:\n",
    "        raise last\n",
    "\n",
    "    need = {\"src_content_id\",\"dst_content_id\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise ValueError(f\"{path}ì— {need} ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ì •ë¦¬\n",
    "    for c in [\"src_content_id\",\"dst_content_id\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"src_content_id\",\"dst_content_id\"]).copy()\n",
    "    df[\"src_content_id\"] = df[\"src_content_id\"].astype(int)\n",
    "    df[\"dst_content_id\"] = df[\"dst_content_id\"].astype(int)\n",
    "    df = df[df[\"src_content_id\"] != df[\"dst_content_id\"]]\n",
    "    df = df.drop_duplicates(subset=[\"src_content_id\",\"dst_content_id\"])\n",
    "\n",
    "    # weight\n",
    "    if \"weight\" in df.columns:\n",
    "        df[\"weight\"] = pd.to_numeric(df[\"weight\"], errors=\"coerce\").fillna(1.0).astype(float)\n",
    "    else:\n",
    "        df[\"weight\"] = 1.0\n",
    "    return df\n",
    "\n",
    "def build_index(df: pd.DataFrame):\n",
    "    nodes = sorted(set(df[\"src_content_id\"]) | set(df[\"dst_content_id\"]))\n",
    "    id2idx = {nid:i for i,nid in enumerate(nodes)}\n",
    "    idx2id = {i:nid for nid,i in id2idx.items()}\n",
    "    idx_df = df.replace({\"src_content_id\": id2idx, \"dst_content_id\": id2idx})\n",
    "    return idx_df, id2idx, idx2id\n",
    "\n",
    "# ---------- ëª¨ë¸ ----------\n",
    "class GraphSAGEEncoder(nn.Module):\n",
    "    def __init__(self, num_nodes: int, in_dim: int, hidden: int, out_dim: int, layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        # ì´ˆê¸° ë…¸ë“œ í”¼ì²˜ê°€ ì—†ìœ¼ë¯€ë¡œ í•™ìŠµ ê°€ëŠ¥ ì„ë² ë”©ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©\n",
    "        self.node_emb = nn.Embedding(num_nodes, in_dim)\n",
    "        nn.init.xavier_uniform_(self.node_emb.weight)\n",
    "\n",
    "        convs = []\n",
    "        dims = [in_dim] + [hidden]*(layers-1) + [out_dim]\n",
    "        for li in range(layers):\n",
    "            convs.append(SAGEConv(dims[li], dims[li+1]))\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, edge_index: torch.Tensor):\n",
    "        x = self.node_emb.weight  # [N, in_dim]\n",
    "        for li, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if li < len(self.convs)-1:\n",
    "                x = self.act(x)\n",
    "                x = self.drop(x)\n",
    "        return x  # [N, out_dim]\n",
    "\n",
    "def bce_link_loss(z: torch.Tensor,\n",
    "                  pos_ei: torch.Tensor,\n",
    "                  neg_ei: torch.Tensor,\n",
    "                  pos_w: torch.Tensor = None):\n",
    "    # ì ê³± ë¡œì§“\n",
    "    pos_logits = (z[pos_ei[0]] * z[pos_ei[1]]).sum(dim=-1)      # [P]\n",
    "    neg_logits = (z[neg_ei[0]] * z[neg_ei[1]]).sum(dim=-1)      # [N]\n",
    "\n",
    "    logits = torch.cat([pos_logits, neg_logits], dim=0)\n",
    "    labels = torch.cat([torch.ones_like(pos_logits),\n",
    "                        torch.zeros_like(neg_logits)], dim=0)\n",
    "\n",
    "    if pos_w is not None:\n",
    "        # positiveì—ë§Œ ê°€ì¤‘(ì •ê·œí™”)\n",
    "        pos_w = pos_w / (pos_w.mean() + 1e-12)\n",
    "        w = torch.cat([pos_w, torch.ones_like(neg_logits)], dim=0)\n",
    "        return nn.functional.binary_cross_entropy_with_logits(logits, labels, weight=w)\n",
    "    else:\n",
    "        return nn.functional.binary_cross_entropy_with_logits(logits, labels)\n",
    "\n",
    "# --- í˜¸í™˜ í—¬í¼: í•™ìŠµìš© ì–‘ì„± ì—£ì§€ ê°€ì ¸ì˜¤ê¸° ---\n",
    "def get_pos_edge_label_index(d: Data) -> torch.Tensor:\n",
    "    # êµ¬ë²„ì „ í˜¸í™˜\n",
    "    if hasattr(d, 'pos_edge_label_index'):\n",
    "        return d.pos_edge_label_index\n",
    "    # ìµœì‹ : edge_label_index(+ edge_label)\n",
    "    if hasattr(d, 'edge_label_index'):\n",
    "        if hasattr(d, 'edge_label') and d.edge_label is not None:\n",
    "            mask = (d.edge_label == 1)\n",
    "            if mask.dim() > 0:\n",
    "                return d.edge_label_index[:, mask]\n",
    "        return d.edge_label_index\n",
    "    # ìµœí›„ ìˆ˜ë‹¨: í˜„ì¬ ê·¸ë˜í”„ì˜ edge_index ì‚¬ìš©\n",
    "    return d.edge_index\n",
    "\n",
    "def train(model, data, train_pos_ei, pos_weights=None):\n",
    "    model = model.to(DEVICE)\n",
    "    edge_index = data.edge_index.to(DEVICE)\n",
    "    train_pos_ei = train_pos_ei.to(DEVICE)\n",
    "    if pos_weights is not None:\n",
    "        pos_weights = pos_weights.to(DEVICE)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        z = model(edge_index)   # [N, D]\n",
    "\n",
    "        # negative sampling: positive ìˆ˜ * NEG_MULT\n",
    "        neg_ei = negative_sampling(\n",
    "            edge_index=train_pos_ei,\n",
    "            num_nodes=z.size(0),\n",
    "            num_neg_samples=train_pos_ei.size(1) * NEG_MULT,\n",
    "            method='sparse'\n",
    "        )\n",
    "\n",
    "        loss = bce_link_loss(z, train_pos_ei, neg_ei, pos_weights if USE_EDGE_W else None)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            zn = nn.functional.normalize(z, p=2, dim=1)\n",
    "            pos_cos = (zn[train_pos_ei[0]] * zn[train_pos_ei[1]]).sum(dim=1).mean().item()\n",
    "            neg_cos = (zn[neg_ei[0]] * zn[neg_ei[1]]).sum(dim=1).mean().item()\n",
    "        print(f\"[{ep:02d}/{EPOCHS}] loss={loss.item():.4f}  cos(pos)â‰ˆ{pos_cos:.3f}  cos(neg)â‰ˆ{neg_cos:.3f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸ“¥ ì—£ì§€ ë¡œë”©...\")\n",
    "    df = read_edges(EDGES_CSV)\n",
    "    df_idx, id2idx, idx2id = build_index(df)\n",
    "    num_nodes = len(id2idx)\n",
    "    print(f\"nodes={num_nodes:,}, edges={len(df_idx):,}\")\n",
    "\n",
    "    # PyG Data\n",
    "    edge_index = torch.as_tensor(\n",
    "        df_idx[[\"src_content_id\",\"dst_content_id\"]].to_numpy().T, dtype=torch.long\n",
    "    )\n",
    "    edge_index = to_undirected(edge_index, num_nodes=num_nodes)\n",
    "    data = Data(num_nodes=num_nodes, edge_index=edge_index)\n",
    "\n",
    "    # positive edge weight ë§µ(ì˜µì…˜)\n",
    "    w_map: Dict[Tuple[int,int], float] = {}\n",
    "    for a, b, w in df_idx[[\"src_content_id\",\"dst_content_id\",\"weight\"]].itertuples(index=False):\n",
    "        if a > b: a, b = b, a\n",
    "        w_map[(a,b)] = float(w)\n",
    "\n",
    "    # ë§í¬ ë¶„í•  (í•™ìŠµìš© positive ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©)\n",
    "    splitter = RandomLinkSplit(is_undirected=True, add_negative_train_samples=False, neg_sampling_ratio=0.0)\n",
    "    train_data, _, _ = splitter(data)\n",
    "\n",
    "    # â¬‡ï¸ í˜¸í™˜ í—¬í¼ ì‚¬ìš©\n",
    "    train_pos = get_pos_edge_label_index(train_data)  # [2, P]\n",
    "\n",
    "    pos_w = None\n",
    "    if USE_EDGE_W:\n",
    "        a = train_pos[0].cpu().numpy()\n",
    "        b = train_pos[1].cpu().numpy()\n",
    "        kk = []\n",
    "        for x, y in zip(a, b):\n",
    "            if x > y: x, y = y, x\n",
    "            kk.append(w_map.get((int(x), int(y)), 1.0))\n",
    "        pos_w = torch.tensor(kk, dtype=torch.float32)\n",
    "\n",
    "    print(\"ğŸ§  GraphSAGE í•™ìŠµ...\")\n",
    "    model = GraphSAGEEncoder(\n",
    "        num_nodes=num_nodes,\n",
    "        in_dim=EMB_DIM, hidden=HIDDEN, out_dim=EMB_DIM,\n",
    "        layers=LAYERS, dropout=DROPOUT\n",
    "    )\n",
    "    model = train(model, data, train_pos, pos_w)\n",
    "\n",
    "    print(\"ğŸ’¾ ì„ë² ë”© ì €ì¥...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(data.edge_index.to(DEVICE)).detach().cpu()\n",
    "        z = nn.functional.normalize(z, p=2, dim=1)\n",
    "\n",
    "    rows = []\n",
    "    for idx in range(num_nodes):\n",
    "        cid = idx2id[idx]\n",
    "        rows.append([cid] + list(map(float, z[idx].numpy().tolist())))\n",
    "    cols = [\"content_id\"] + [f\"emb_{i}\" for i in range(EMB_DIM)]\n",
    "    out = pd.DataFrame(rows, columns=cols).sort_values(\"content_id\")\n",
    "    out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì„ë² ë”© ì €ì¥: {OUT_CSV} (nodes={len(out):,}, dim={EMB_DIM})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ffdb17",
   "metadata": {},
   "source": [
    "# Noce2vec ì•„ì´í…œ ì„ë² ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23d8616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ ê·¸ë˜í”„ ë¡œë”©...\n",
      "nodes=139, edges(undirected)â‰ˆ3,531\n",
      "ğŸš¶ ëœë¤ì›Œí¬ ìƒì„±...\n",
      "walks=1,390, avg_lenâ‰ˆ40.0\n",
      "ğŸ§© í•™ìŠµ ìŒ ìƒì„±(Skip-gram window)...\n",
      "pairs=514,300\n",
      "ğŸ§  Skip-gram(NS) í•™ìŠµ ì‹œì‘...\n",
      "[Epoch 1/3] loss=1.3863\n",
      "[Epoch 2/3] loss=1.3863\n",
      "[Epoch 3/3] loss=1.3863\n",
      "âœ… ì„ë² ë”© ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\item_embeddings_torch.csv (nodes=139, dim=64)\n"
     ]
    }
   ],
   "source": [
    "# learn_item_embeddings.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#ì•„ì´í…œ-ì•„ì´í…œ ê°€ì¤‘ ê·¸ë˜í”„ -> Node2Vec ìŠ¤íƒ€ì¼ ì„ë² ë”© (Pure PyTorch, gensim/scipy ë¶ˆí•„ìš”)\n",
    "#ì…ë ¥ : C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\graph_edges_item_item.csv\n",
    "#ì¶œë ¥ : C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\item_embeddings_torch.csv\n",
    "#ì‚¬ìš© : python learn_item_embeddings.py\n",
    "#í•„ìš” : pip install torch pandas numpy\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===== ê²½ë¡œ =====\n",
    "BASE = r\"C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\"\n",
    "EDGES_PATH = os.path.join(BASE, \"graph_edges_item_item.csv\")\n",
    "OUT_EMB_CSV = os.path.join(BASE, \"item_embeddings_torch.csv\")\n",
    "\n",
    "# ===== í•˜ì´í¼íŒŒë¼ë¯¸í„° =====\n",
    "DIM            = 64       # ì„ë² ë”© ì°¨ì›\n",
    "WALK_LENGTH    = 40       # ëœë¤ì›Œí¬ ê¸¸ì´\n",
    "NUM_WALKS      = 10       # ë…¸ë“œë‹¹ ì›Œí¬ ìˆ˜\n",
    "P_RETURN       = 1.0      # node2vec p (ë˜ëŒì•„ê°€ê¸° ì„ í˜¸ pâ†“)\n",
    "Q_INOUT        = 1.0      # node2vec q (ì›ê±°ë¦¬ íƒìƒ‰ ì„ í˜¸ qâ†‘)\n",
    "WINDOW         = 5        # Skip-gram ìœˆë„ìš°\n",
    "NEGATIVE_K     = 5        # ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ ìˆ˜\n",
    "EPOCHS         = 3\n",
    "BATCH_SIZE     = 8192\n",
    "LR             = 0.025\n",
    "SEED           = 42\n",
    "DEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ---------- ê·¸ë˜í”„ ë¡œë“œ & ê°€ì¤‘ ë£°ë › ì¤€ë¹„ ----------\n",
    "def load_graph(edges_csv):\n",
    "    df = pd.read_csv(edges_csv, encoding=\"utf-8-sig\")\n",
    "    need = {\"src_content_id\",\"dst_content_id\",\"weight\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise ValueError(f\"{edges_csv}ì— {need} ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    adj = defaultdict(list)   # node -> [(nbr, w), ...]\n",
    "    nodes = set()\n",
    "    for a, b, w in df[[\"src_content_id\",\"dst_content_id\",\"weight\"]].itertuples(index=False):\n",
    "        if a == b: \n",
    "            continue\n",
    "        w = float(w) if pd.notna(w) else 1.0\n",
    "        adj[a].append((b, w))\n",
    "        adj[b].append((a, w))\n",
    "        nodes.add(a); nodes.add(b)\n",
    "\n",
    "    # ëˆ„ì í™•ë¥ (ë£°ë ›íœ ) ì‚¬ì „\n",
    "    cumsums = {}\n",
    "    for u, lst in adj.items():\n",
    "        if not lst:\n",
    "            continue\n",
    "        nbrs, ws = zip(*lst)\n",
    "        ws = np.maximum(np.array(ws, dtype=float), 1e-12)\n",
    "        csum = np.cumsum(ws); cumsums[u] = (np.array(nbrs, dtype=int), csum / csum[-1])\n",
    "    return adj, cumsums, sorted(nodes)\n",
    "\n",
    "def weighted_choice(nbrs, cprobs):\n",
    "    r = random.random()\n",
    "    lo, hi = 0, len(cprobs)-1\n",
    "    while lo < hi:\n",
    "        mid = (lo + hi) // 2\n",
    "        if cprobs[mid] < r: lo = mid + 1\n",
    "        else: hi = mid\n",
    "    return int(nbrs[lo])\n",
    "\n",
    "# ---------- Node2Vec ê°€ì¤‘ ëœë¤ì›Œí¬ ----------\n",
    "def node2vec_walks(adj, cumsums, nodes, walk_length, num_walks, p, q):\n",
    "    walks = []\n",
    "    base_nodes = list(nodes)\n",
    "    for _ in range(num_walks):\n",
    "        random.shuffle(base_nodes)\n",
    "        for start in base_nodes:\n",
    "            if start not in cumsums:\n",
    "                continue\n",
    "            walk = [start]\n",
    "            if walk_length == 1:\n",
    "                walks.append(walk); continue\n",
    "\n",
    "            # ì²« ìŠ¤í…: ê°€ì¤‘ì¹˜ ë¹„ë¡€ ì„ íƒ\n",
    "            nbrs, cprobs = cumsums[start]\n",
    "            if len(nbrs) == 0:\n",
    "                walks.append(walk); continue\n",
    "            curr = weighted_choice(nbrs, cprobs)\n",
    "            walk.append(curr); prev = start\n",
    "\n",
    "            for _ in range(2, walk_length):\n",
    "                cand = adj.get(curr, [])\n",
    "                if not cand: break\n",
    "                cand_nodes = [n for n,_ in cand]\n",
    "                cand_w = []\n",
    "                prev_nbrs = {n for n,_ in adj.get(prev, [])}\n",
    "                for nxt, w in cand:\n",
    "                    # p/q ë°”ì´ì–´ìŠ¤(ê·¼ì‚¬)\n",
    "                    if nxt == prev: bias = 1.0 / p\n",
    "                    elif nxt in prev_nbrs: bias = 1.0\n",
    "                    else: bias = 1.0 / q\n",
    "                    cand_w.append(max(w,1e-12) * bias)\n",
    "                cw = np.array(cand_w, dtype=float)\n",
    "                cs = np.cumsum(cw); cs /= cs[-1]\n",
    "                idx = np.searchsorted(cs, random.random())\n",
    "                nxt = cand_nodes[min(idx, len(cand_nodes)-1)]\n",
    "                walk.append(nxt); prev, curr = curr, nxt\n",
    "            walks.append(walk)\n",
    "    return walks\n",
    "\n",
    "# ---------- Skip-gram(ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§) ----------\n",
    "def generate_pairs(walks, window):\n",
    "    pairs = []\n",
    "    for walk in walks:\n",
    "        L = len(walk)\n",
    "        for i in range(L):\n",
    "            c = walk[i]\n",
    "            l = max(0, i-window); r = min(L, i+window+1)\n",
    "            for j in range(l, r):\n",
    "                if j == i: continue\n",
    "                pairs.append((c, walk[j]))\n",
    "    return pairs\n",
    "\n",
    "class SkipGramNS(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.in_emb  = torch.nn.Embedding(vocab_size, dim)\n",
    "        self.out_emb = torch.nn.Embedding(vocab_size, dim)\n",
    "        torch.nn.init.uniform_(self.in_emb.weight,  -0.5/dim, 0.5/dim)\n",
    "        torch.nn.init.uniform_(self.out_emb.weight, -0.5/dim, 0.5/dim)\n",
    "\n",
    "    def forward(self, center, pos, neg):\n",
    "        # center:[B], pos:[B], neg:[B,K]\n",
    "        v  = self.in_emb(center)           # [B, D]\n",
    "        u  = self.out_emb(pos)             # [B, D]\n",
    "        uv = (v * u).sum(dim=1)            # [B]\n",
    "        pos_loss = torch.nn.functional.logsigmoid(uv).mean()\n",
    "\n",
    "        neg_u = self.out_emb(neg)          # [B, K, D]\n",
    "        neg_uv = torch.bmm(neg_u, v.unsqueeze(2)).squeeze(2)  # [B, K]\n",
    "        neg_loss = torch.nn.functional.logsigmoid(-neg_uv).mean()\n",
    "        return -(pos_loss + neg_loss)      # minimize\n",
    "\n",
    "def train_skipgram_ns(pairs, id2idx, epochs=3, batch_size=8192, dim=64, neg_k=5, lr=0.025):\n",
    "    # ë…¸ë“œ ì¸ë±ì‹±\n",
    "    vocab = sorted(id2idx.keys())\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # ë¹ˆë„ ê¸°ë°˜ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ë¶„í¬(0.75 ìŠ¹)\n",
    "    counts = defaultdict(int)\n",
    "    for a,b in pairs:\n",
    "        counts[a] += 1; counts[b] += 1\n",
    "    idx_counts = np.zeros(vocab_size, dtype=np.float64)\n",
    "    for nid, cnt in counts.items():\n",
    "        idx_counts[id2idx[nid]] = cnt\n",
    "    prob = idx_counts ** 0.75\n",
    "    prob = prob / prob.sum()\n",
    "    alias_table = np.cumsum(prob)\n",
    "\n",
    "    def sample_neg(B, K):\n",
    "        # ëˆ„ì ë¶„í¬ ê¸°ë°˜ ë²¡í„°í™” ìƒ˜í”Œë§\n",
    "        r = np.random.rand(B, K)\n",
    "        idx = np.searchsorted(alias_table, r, side=\"right\")\n",
    "        return torch.from_numpy(idx.astype(np.int64))\n",
    "\n",
    "    # í•™ìŠµ ë°ì´í„° í…ì„œí™”(ì¸ë±ìŠ¤ ë³€í™˜)\n",
    "    centers = torch.tensor([id2idx[a] for a,_ in pairs], dtype=torch.long)\n",
    "    contexts= torch.tensor([id2idx[b] for _,b in pairs], dtype=torch.long)\n",
    "\n",
    "    ds_size = len(pairs)\n",
    "    model = SkipGramNS(vocab_size, dim).to(DEVICE)\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        perm = torch.randperm(ds_size)\n",
    "        centers = centers[perm]; contexts = contexts[perm]\n",
    "        total_loss = 0.0; steps = 0\n",
    "\n",
    "        for i in range(0, ds_size, batch_size):\n",
    "            c_batch = centers[i:i+batch_size].to(DEVICE)\n",
    "            p_batch = contexts[i:i+batch_size].to(DEVICE)\n",
    "            B = c_batch.size(0)\n",
    "            n_batch = sample_neg(B, neg_k).to(DEVICE)\n",
    "\n",
    "            loss = model(c_batch, p_batch, n_batch)\n",
    "            optim.zero_grad(); loss.backward(); optim.step()\n",
    "\n",
    "            total_loss += loss.item(); steps += 1\n",
    "\n",
    "        avg = total_loss / max(1, steps)\n",
    "        print(f\"[Epoch {epoch}/{epochs}] loss={avg:.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = model.in_emb.weight.detach().cpu().numpy()\n",
    "    return emb  # shape [vocab_size, dim]\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸ“¥ ê·¸ë˜í”„ ë¡œë”©...\")\n",
    "    adj, cumsums, nodes = load_graph(EDGES_PATH)\n",
    "    print(f\"nodes={len(nodes):,}, edges(undirected)â‰ˆ{sum(len(v) for v in adj.values())//2:,}\")\n",
    "\n",
    "    print(\"ğŸš¶ ëœë¤ì›Œí¬ ìƒì„±...\")\n",
    "    walks = node2vec_walks(\n",
    "        adj, cumsums, nodes,\n",
    "        walk_length=WALK_LENGTH,\n",
    "        num_walks=NUM_WALKS,\n",
    "        p=P_RETURN, q=Q_INOUT\n",
    "    )\n",
    "    avg_len = np.mean([len(w) for w in walks]) if walks else 0\n",
    "    print(f\"walks={len(walks):,}, avg_lenâ‰ˆ{avg_len:.1f}\")\n",
    "\n",
    "    print(\"ğŸ§© í•™ìŠµ ìŒ ìƒì„±(Skip-gram window)...\")\n",
    "    pairs = generate_pairs(walks, WINDOW)\n",
    "    print(f\"pairs={len(pairs):,}\")\n",
    "\n",
    "    # ë…¸ë“œ id â†’ ì—°ì† index ë§¤í•‘\n",
    "    id2idx = {nid:i for i, nid in enumerate(sorted(nodes))}\n",
    "    print(\"ğŸ§  Skip-gram(NS) í•™ìŠµ ì‹œì‘...\")\n",
    "    emb = train_skipgram_ns(\n",
    "        pairs, id2idx,\n",
    "        epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        dim=DIM, neg_k=NEGATIVE_K, lr=LR\n",
    "    )\n",
    "\n",
    "    # CSV ì €ì¥\n",
    "    idx2id = {i:nid for nid,i in id2idx.items()}\n",
    "    rows = [[idx2id[i]] + list(map(float, emb[i])) for i in range(len(idx2id))]\n",
    "    cols = [\"content_id\"] + [f\"emb_{i}\" for i in range(DIM)]\n",
    "    out = pd.DataFrame(rows, columns=cols).sort_values(\"content_id\")\n",
    "    out.to_csv(OUT_EMB_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì„ë² ë”© ì €ì¥: {OUT_EMB_CSV} (nodes={len(out)}, dim={DIM})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7dbca",
   "metadata": {},
   "source": [
    "# ìœ ì € ì„ë² ë”© ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73c2079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: item_embeddings_torch.csv (items=139, dim=64)\n",
      "âœ… ë„ë©”ì¸ ì„¼íŠ¸ë¡œì´ë“œ: {'AV': 60, 'GAME': 24, 'WEBNOVEL': 55}\n",
      "âœ… ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìˆ˜: 8\n",
      "âœ… user_embeddings.csv ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\user_embeddings.csv (users=10, skipped_rows=0)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ìœ ì € ì„ë² ë”© ìƒì„± (ì•„ì´í…œ ì„ë² ë”© + ì„ í˜¸ ë„ë©”ì¸/ì¥ë¥´ centroid)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, List\n",
    "\n",
    "BASE = r\"C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\"\n",
    "ITEM_EMB_1 = os.path.join(BASE, \"item_embeddings_torch.csv\")\n",
    "ITEM_EMB_2 = os.path.join(BASE, \"item_embeddings.csv\")\n",
    "CONTENTS   = os.path.join(BASE, \"contents.csv\")\n",
    "RAW_GENRES = os.path.join(BASE, \"content_raw_genres.csv\")\n",
    "UPREF      = os.path.join(BASE, \"user_preferences.csv\")\n",
    "UP_CTYPES  = os.path.join(BASE, \"user_preferred_content_types.csv\")\n",
    "UP_GENRES  = os.path.join(BASE, \"user_preferred_genres.csv\")\n",
    "OUT_USER   = os.path.join(BASE, \"user_embeddings.csv\")\n",
    "\n",
    "def read_csv_retry(path, encodings=(\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"), **kw) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    last = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kw)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "def load_item_embeddings() -> pd.DataFrame:\n",
    "    # ëª…ì‹œì  ì„ íƒ (DataFrameì— or ê¸ˆì§€)\n",
    "    df1 = read_csv_retry(ITEM_EMB_1)\n",
    "    if df1 is not None and not df1.empty:\n",
    "        df = df1\n",
    "        src = os.path.basename(ITEM_EMB_1)\n",
    "    else:\n",
    "        df2 = read_csv_retry(ITEM_EMB_2)\n",
    "        if df2 is not None and not df2.empty:\n",
    "            df = df2\n",
    "            src = os.path.basename(ITEM_EMB_2)\n",
    "        else:\n",
    "            raise RuntimeError(\"ì•„ì´í…œ ì„ë² ë”© CSVê°€ ì—†ìŠµë‹ˆë‹¤: item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv\")\n",
    "\n",
    "    if \"content_id\" not in df.columns:\n",
    "        raise RuntimeError(\"ì•„ì´í…œ ì„ë² ë”© CSVì— content_id ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    emb_cols = [c for c in df.columns if c.startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        raise RuntimeError(\"ì•„ì´í…œ ì„ë² ë”© CSVì— emb_ë¡œ ì‹œì‘í•˜ëŠ” ì„ë² ë”© ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    df[\"content_id\"] = pd.to_numeric(df[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df[df[\"content_id\"].notna()].copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    for c in emb_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df.dropna(subset=emb_cols, inplace=True)\n",
    "\n",
    "    # L2 normalize\n",
    "    M = df[emb_cols].to_numpy(np.float32)\n",
    "    M = M / (np.linalg.norm(M, axis=1, keepdims=True) + 1e-12)\n",
    "    df.loc[:, emb_cols] = M\n",
    "\n",
    "    print(f\"âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: {src} (items={len(df)}, dim={len(emb_cols)})\")\n",
    "    return df[[\"content_id\"] + emb_cols]\n",
    "\n",
    "# content_type â†’ domain ë§¤í•‘\n",
    "CTYPE2DOMAIN = {\n",
    "    \"movie\": \"AV\", \"tv\": \"AV\",\n",
    "    \"game\": \"GAME\",\n",
    "    \"webnovel\": \"WEBNOVEL\", \"webtoon\": \"WEBNOVEL\"\n",
    "}\n",
    "\n",
    "def main():\n",
    "    emb = load_item_embeddings()\n",
    "    emb_cols = [c for c in emb.columns if c.startswith(\"emb_\")]\n",
    "    M = emb[emb_cols].to_numpy(np.float32)\n",
    "    ids = emb[\"content_id\"].to_numpy(int)\n",
    "\n",
    "    # contents.csv â†’ domain\n",
    "    contents = read_csv_retry(CONTENTS)\n",
    "    if contents is None or contents.empty:\n",
    "        raise RuntimeError(\"contents.csv í•„ìš”.\")\n",
    "    contents = contents[[\"content_id\", \"domain\"]].copy()\n",
    "    contents[\"content_id\"] = pd.to_numeric(contents[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    contents = contents[contents[\"content_id\"].notna()].copy()\n",
    "    contents[\"content_id\"] = contents[\"content_id\"].astype(int)\n",
    "    emb_meta = emb.merge(contents, on=\"content_id\", how=\"left\")\n",
    "\n",
    "    # domain centroids\n",
    "    domain_vecs = {}\n",
    "    dom_counts = {}\n",
    "    for dom, grp in emb_meta.dropna(subset=[\"domain\"]).groupby(\"domain\"):\n",
    "        mat = grp[emb_cols].to_numpy(np.float32)\n",
    "        if len(mat) == 0: \n",
    "            continue\n",
    "        v = mat.mean(axis=0)\n",
    "        v = v / (np.linalg.norm(v) + 1e-12)\n",
    "        domain_vecs[str(dom)] = v\n",
    "        dom_counts[str(dom)] = len(mat)\n",
    "    print(f\"âœ… ë„ë©”ì¸ ì„¼íŠ¸ë¡œì´ë“œ: { {k: dom_counts[k] for k in sorted(dom_counts)} }\")\n",
    "\n",
    "    # genre centroids (optional)\n",
    "    genre_vecs = {}\n",
    "    if os.path.exists(RAW_GENRES):\n",
    "        rawg = read_csv_retry(RAW_GENRES)\n",
    "        if rawg is not None and not rawg.empty and {\"content_id\",\"raw_genre\"}.issubset(rawg.columns):\n",
    "            rawg = rawg[[\"content_id\", \"raw_genre\"]].dropna().copy()\n",
    "            rawg[\"content_id\"] = pd.to_numeric(rawg[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "            rawg = rawg[rawg[\"content_id\"].notna()].copy()\n",
    "            rawg[\"content_id\"] = rawg[\"content_id\"].astype(int)\n",
    "            rawg[\"g\"] = rawg[\"raw_genre\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "            idx_map = {cid: i for i, cid in enumerate(ids)}\n",
    "            for g, grp in rawg.groupby(\"g\"):\n",
    "                idxs = [idx_map[c] for c in grp[\"content_id\"].tolist() if c in idx_map]\n",
    "                if not idxs:\n",
    "                    continue\n",
    "                v = M[idxs].mean(axis=0)\n",
    "                v = v / (np.linalg.norm(v) + 1e-12)\n",
    "                genre_vecs[g] = v\n",
    "            print(f\"âœ… ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìˆ˜: {len(genre_vecs)}\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ content_raw_genres.csv ë¹„ì–´ìˆìŒ ë˜ëŠ” ì»¬ëŸ¼ ë¶ˆì¶©ë¶„ â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìƒëµ\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ content_raw_genres.csv ì—†ìŒ â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìƒëµ\")\n",
    "\n",
    "    # user prefs\n",
    "    upref = read_csv_retry(UPREF)\n",
    "    uctype = read_csv_retry(UP_CTYPES)\n",
    "    ugen = read_csv_retry(UP_GENRES)\n",
    "    if upref is None or upref.empty:\n",
    "        raise RuntimeError(\"user_preferences.csv í•„ìš”.\")\n",
    "\n",
    "    ctype_map = {}\n",
    "    if uctype is not None and not uctype.empty and {\"preference_id\", \"content_type\"}.issubset(uctype.columns):\n",
    "        for pid, grp in uctype.groupby(\"preference_id\"):\n",
    "            ctype_map[int(pid)] = [str(x).strip() for x in grp[\"content_type\"].tolist()]\n",
    "\n",
    "    genre_map = {}\n",
    "    if ugen is not None and not ugen.empty and {\"preference_id\", \"genre\"}.issubset(ugen.columns):\n",
    "        for pid, grp in ugen.groupby(\"preference_id\"):\n",
    "            genre_map[int(pid)] = [str(x).strip().lower() for x in grp[\"genre\"].tolist()]\n",
    "\n",
    "    def ctypes_to_domains(cts: List[str]) -> List[str]:\n",
    "        outs = set()\n",
    "        for ct in cts:\n",
    "            d = CTYPE2DOMAIN.get(str(ct).lower())\n",
    "            if d:\n",
    "                outs.add(d)\n",
    "        return sorted(outs)\n",
    "\n",
    "    # ê°€ì¤‘ì¹˜\n",
    "    ALPHA_DOM, ALPHA_GEN = 0.6, 0.4\n",
    "\n",
    "    rows = []\n",
    "    skipped = 0\n",
    "    for _, r in upref.iterrows():\n",
    "        if pd.isna(r.get(\"id\")):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        uid = int(r[\"id\"])\n",
    "        uname = r.get(\"username\", \"\")\n",
    "\n",
    "        # domains\n",
    "        doms = ctypes_to_domains(ctype_map.get(uid, []))\n",
    "        dom_vecs = [domain_vecs[d] for d in doms if d in domain_vecs]\n",
    "        v_dom = None\n",
    "        if dom_vecs:\n",
    "            v_dom = np.mean(np.stack(dom_vecs, axis=0), axis=0)\n",
    "            v_dom = v_dom / (np.linalg.norm(v_dom) + 1e-12)\n",
    "\n",
    "        # genres\n",
    "        gnames = [g for g in (genre_map.get(uid, [])) if g in genre_vecs]\n",
    "        g_vecs = [genre_vecs[g] for g in gnames]\n",
    "        v_gen = None\n",
    "        if g_vecs:\n",
    "            v_gen = np.mean(np.stack(g_vecs, axis=0), axis=0)\n",
    "            v_gen = v_gen / (np.linalg.norm(v_gen) + 1e-12)\n",
    "\n",
    "        # user vector\n",
    "        if v_dom is None and v_gen is None:\n",
    "            # í´ë°±: ì „ì²´ ì•„ì´í…œ í‰ê· \n",
    "            v_user = M.mean(axis=0)\n",
    "        elif v_dom is None:\n",
    "            v_user = v_gen\n",
    "        elif v_gen is None:\n",
    "            v_user = v_dom\n",
    "        else:\n",
    "            v_user = ALPHA_DOM * v_dom + ALPHA_GEN * v_gen\n",
    "        v_user = v_user / (np.linalg.norm(v_user) + 1e-12)\n",
    "\n",
    "        rows.append([uid, uname] + list(map(float, v_user)))\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"ìœ ì € ì„ë² ë”© ìƒì„± ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. ì…ë ¥ íŒŒì¼/ì„ í˜¸ ì •ë³´ í™•ì¸ í•„ìš”.\")\n",
    "\n",
    "    dim = M.shape[1]\n",
    "    out = pd.DataFrame(rows, columns=[\"user_id\", \"username\"] + [f\"emb_{i}\" for i in range(dim)])\n",
    "    out.sort_values(\"user_id\").to_csv(OUT_USER, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… user_embeddings.csv ì €ì¥: {OUT_USER} (users={len(out)}, skipped_rows={skipped})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e5fc2",
   "metadata": {},
   "source": [
    "# ìœ ì € ì•„ì´í…œ ì„ë² ë”© top-kì¶”ì²œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91bf4798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: item_embeddings_torch.csv (items=139, dim=64)\n",
      "âœ… recommendations_topK.csv ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\recommendations_topK.csv (rows=50)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#ìœ ì €/ì•„ì´í…œ ì„ë² ë”©ë§Œìœ¼ë¡œ Top-K ì¶”ì²œ (contents.csv ë¶ˆí•„ìš”)\n",
    "#ì…ë ¥:\n",
    "#  clean/user_embeddings.csv                  (user_id, username, emb_0..)\n",
    "#  clean/item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv (content_id, emb_0..)\n",
    "#ì¶œë ¥:\n",
    "#  clean/recommendations_topK.csv             (user_id, username, rank, content_id, score)\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "BASE = r\"C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\"\n",
    "USER_EMB   = os.path.join(BASE, \"user_embeddings.csv\")\n",
    "ITEM_EMB_1 = os.path.join(BASE, \"item_embeddings_torch.csv\")\n",
    "ITEM_EMB_2 = os.path.join(BASE, \"item_embeddings.csv\")\n",
    "OUT_RECS   = os.path.join(BASE, \"recommendations_topK.csv\")\n",
    "\n",
    "TOPK = 5\n",
    "EXCLUDE_DUP_CONTENTS = True   # content_id ì¤‘ë³µí–‰ì´ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©\n",
    "ASSUME_NORMALIZED = True      # ì„ë² ë”©ì´ ì´ë¯¸ L2 ì •ê·œí™”ë˜ì–´ ìˆë‹¤ë©´ True, ì•„ë‹ˆë©´ Falseë¡œ\n",
    "\n",
    "def read_csv_retry(path, encodings=(\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"), **kw) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    last=None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kw)\n",
    "        except Exception as e:\n",
    "            last=e\n",
    "    raise last\n",
    "\n",
    "def l2norm(X: np.ndarray) -> np.ndarray:\n",
    "    return X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def _coerce_content_id_column(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"ì—¬ëŸ¬ ì´ë¦„/í˜•íƒœë¡œ ì €ì¥ëœ content_idë¥¼ ìµœëŒ€í•œ ë³µêµ¬.\"\"\"\n",
    "    df = df.copy()\n",
    "    orig_cols = list(df.columns)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    name_map = {c.lower(): c for c in df.columns}\n",
    "    for key in [\"content_id\", \"contentid\", \"content id\", \"id\", \"unnamed: 0\"]:\n",
    "        if key in name_map:\n",
    "            cand = name_map[key]\n",
    "            s = df[cand]\n",
    "            if s.dtype == object:\n",
    "                s = s.astype(str).str.strip()\n",
    "            s = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "            if s.isna().all():\n",
    "                continue\n",
    "            return s\n",
    "    raise RuntimeError(f\"content_id ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì»¬ëŸ¼: {orig_cols}\")\n",
    "\n",
    "def load_user_embeddings() -> pd.DataFrame:\n",
    "    u = read_csv_retry(USER_EMB)\n",
    "    if u is None or u.empty:\n",
    "        raise RuntimeError(\"user_embeddings.csv í•„ìš”.\")\n",
    "    u = u.copy()\n",
    "    u.columns = [c.strip() for c in u.columns]\n",
    "    if \"user_id\" not in u.columns:\n",
    "        # í˜¹ì‹œ idë¡œ ì €ì¥ë˜ì—ˆì„ ìˆ˜ë„ ìˆìŒ\n",
    "        if \"id\" in u.columns:\n",
    "            u = u.rename(columns={\"id\": \"user_id\"})\n",
    "        else:\n",
    "            raise RuntimeError(\"user_embeddings.csvì— user_id ì»¬ëŸ¼ í•„ìš”.\")\n",
    "    u[\"user_id\"] = pd.to_numeric(u[\"user_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    u = u[u[\"user_id\"].notna()].copy()\n",
    "    u[\"user_id\"] = u[\"user_id\"].astype(int)\n",
    "\n",
    "    emb_cols = [c for c in u.columns if c.lower().startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        raise RuntimeError(\"user_embeddings.csvì— emb_ë¡œ ì‹œì‘í•˜ëŠ” ì„ë² ë”© ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    for c in emb_cols:\n",
    "        u[c] = pd.to_numeric(u[c], errors=\"coerce\")\n",
    "    u.dropna(subset=emb_cols, inplace=True)\n",
    "    if not ASSUME_NORMALIZED:\n",
    "        u.loc[:, emb_cols] = l2norm(u[emb_cols].to_numpy(np.float32))\n",
    "    return u[[\"user_id\", u.columns[u.columns.str.lower()==\"username\"][0] if \"username\" in [x.lower() for x in u.columns] else None] + emb_cols].rename(columns={None:\"username\"})\n",
    "\n",
    "def load_item_embeddings() -> pd.DataFrame:\n",
    "    i1 = read_csv_retry(ITEM_EMB_1)\n",
    "    if i1 is not None and not i1.empty:\n",
    "        df = i1; src = os.path.basename(ITEM_EMB_1)\n",
    "    else:\n",
    "        i2 = read_csv_retry(ITEM_EMB_2)\n",
    "        if i2 is None or i2.empty:\n",
    "            raise RuntimeError(\"item_embeddings CSV í•„ìš”: item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv\")\n",
    "        df = i2; src = os.path.basename(ITEM_EMB_2)\n",
    "\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df[\"content_id\"] = _coerce_content_id_column(df)\n",
    "\n",
    "    emb_cols = [c for c in df.columns if c.lower().startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        # í˜¹ì‹œ ë‹¤ë¥¸ ì ‘ë‘ì–´ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í›„ë³´ ë³´ì—¬ì£¼ê¸°\n",
    "        alt = [c for c in df.columns if re.search(r\"\\d+$\", c)]\n",
    "        raise RuntimeError(f\"ì„ë² ë”© ì»¬ëŸ¼(emb_*) ì—†ìŒ. í™•ì¸ í•„ìš”. í›„ë³´: {alt}\")\n",
    "\n",
    "    for c in emb_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=emb_cols + [\"content_id\"]).copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    if EXCLUDE_DUP_CONTENTS:\n",
    "        df = df.drop_duplicates(subset=[\"content_id\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    if not ASSUME_NORMALIZED:\n",
    "        df.loc[:, emb_cols] = l2norm(df[emb_cols].to_numpy(np.float32))\n",
    "\n",
    "    print(f\"âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: {src} (items={len(df)}, dim={len(emb_cols)})\")\n",
    "    return df[[\"content_id\"] + emb_cols]\n",
    "\n",
    "def main():\n",
    "    u = load_user_embeddings()\n",
    "    i = load_item_embeddings()\n",
    "\n",
    "    ucols = [c for c in u.columns if c.lower().startswith(\"emb_\")]\n",
    "    icols = [c for c in i.columns if c.lower().startswith(\"emb_\")]\n",
    "\n",
    "    U = u[ucols].to_numpy(np.float32)\n",
    "    I = i[icols].to_numpy(np.float32)\n",
    "    # í˜¹ì‹œ ASSUME_NORMALIZED=Trueì¸ë° ì‹¤ì œë¡œ ì •ê·œí™” ì•ˆë˜ì–´ ìˆìœ¼ë©´, ì½”ì‚¬ì¸ = ë‚´ì ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë‹ˆ ë³´ì •\n",
    "    # ì•ˆì „í•˜ê²Œ í•œ ë²ˆ ë” ì •ê·œí™”\n",
    "    U = l2norm(U)\n",
    "    I = l2norm(I)\n",
    "\n",
    "    item_ids = i[\"content_id\"].to_numpy(int)\n",
    "    user_ids = u[\"user_id\"].to_numpy(int)\n",
    "    usernames = u[\"username\"].astype(str).to_numpy() if \"username\" in u.columns else np.array([\"\"]*len(u))\n",
    "\n",
    "    rec_rows = []\n",
    "    for r in range(U.shape[0]):\n",
    "        sims = (I @ U[r:r+1].T).reshape(-1)   # ì½”ì‚¬ì¸(ì •ê·œí™” ê°€ì •)\n",
    "        k = min(TOPK, len(sims))\n",
    "        if k == 0: \n",
    "            continue\n",
    "        part = np.argpartition(-sims, k-1)[:k]\n",
    "        order = part[np.argsort(-sims[part])]\n",
    "        top_ids = item_ids[order]\n",
    "        top_scs = sims[order]\n",
    "        uid = int(user_ids[r]); uname = usernames[r] if r < len(usernames) else \"\"\n",
    "        for rank, (cid, sc) in enumerate(zip(top_ids, top_scs), start=1):\n",
    "            rec_rows.append([uid, uname, rank, int(cid), float(sc)])\n",
    "\n",
    "    rec = pd.DataFrame(rec_rows, columns=[\"user_id\",\"username\",\"rank\",\"content_id\",\"score\"])\n",
    "    rec.sort_values([\"user_id\",\"rank\"]).to_csv(OUT_RECS, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… recommendations_topK.csv ì €ì¥: {OUT_RECS} (rows={len(rec)})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180a1b3",
   "metadata": {},
   "source": [
    "# ë„ë©”ì¸ ë³„ ì¶”ì²œ ì•„ì´í…œ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cc4284b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: item_embeddings_torch.csv (items=139, dim=64)\n",
      "âœ… recommendations_per_domain.csv ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\recommendations_per_domain.csv (rows=90)\n",
      "\n",
      "[User 1] ì´ 9ê°œ\n",
      "  # 1  cid=57  dom=AV  sim=0.3405\n",
      "  # 2  cid=86  dom=AV  sim=0.3375\n",
      "  # 3  cid=49  dom=AV  sim=0.3046\n",
      "  # 4  cid=207  dom=WEBNOVEL  sim=0.2554\n",
      "  # 5  cid=232  dom=WEBNOVEL  sim=0.2549\n",
      "  # 6  cid=122  dom=GAME  sim=0.2396\n",
      "  # 7  cid=131  dom=WEBNOVEL  sim=0.2148\n",
      "  # 8  cid=123  dom=GAME  sim=0.1827\n",
      "  # 9  cid=121  dom=GAME  sim=0.1629\n",
      "\n",
      "[User 2] ì´ 9ê°œ\n",
      "  # 1  cid=232  dom=WEBNOVEL  sim=0.4273\n",
      "  # 2  cid=49  dom=AV  sim=0.4104\n",
      "  # 3  cid=222  dom=WEBNOVEL  sim=0.3949\n",
      "  # 4  cid=65  dom=AV  sim=0.3864\n",
      "  # 5  cid=8  dom=AV  sim=0.3794\n",
      "  # 6  cid=240  dom=WEBNOVEL  sim=0.3548\n",
      "  # 7  cid=113  dom=GAME  sim=0.1971\n",
      "  # 8  cid=123  dom=GAME  sim=0.1736\n",
      "  # 9  cid=122  dom=GAME  sim=0.1426\n",
      "\n",
      "[User 3] ì´ 9ê°œ\n",
      "  # 1  cid=123  dom=GAME  sim=0.4318\n",
      "  # 2  cid=49  dom=AV  sim=0.4039\n",
      "  # 3  cid=207  dom=WEBNOVEL  sim=0.3873\n",
      "  # 4  cid=122  dom=GAME  sim=0.3818\n",
      "  # 5  cid=34  dom=AV  sim=0.3646\n",
      "  # 6  cid=109  dom=GAME  sim=0.3249\n",
      "  # 7  cid=27  dom=AV  sim=0.3249\n",
      "  # 8  cid=226  dom=WEBNOVEL  sim=0.3133\n",
      "  # 9  cid=130  dom=WEBNOVEL  sim=0.2752\n",
      "\n",
      "[User 4] ì´ 9ê°œ\n",
      "  # 1  cid=179  dom=WEBNOVEL  sim=0.4346\n",
      "  # 2  cid=222  dom=WEBNOVEL  sim=0.3852\n",
      "  # 3  cid=144  dom=WEBNOVEL  sim=0.3690\n",
      "  # 4  cid=8  dom=AV  sim=0.3655\n",
      "  # 5  cid=65  dom=AV  sim=0.3321\n",
      "  # 6  cid=49  dom=AV  sim=0.2912\n",
      "  # 7  cid=113  dom=GAME  sim=0.2341\n",
      "  # 8  cid=108  dom=GAME  sim=0.1968\n",
      "  # 9  cid=124  dom=GAME  sim=0.1598\n",
      "\n",
      "[User 5] ì´ 9ê°œ\n",
      "  # 1  cid=57  dom=AV  sim=0.3405\n",
      "  # 2  cid=86  dom=AV  sim=0.3375\n",
      "  # 3  cid=49  dom=AV  sim=0.3046\n",
      "  # 4  cid=207  dom=WEBNOVEL  sim=0.2554\n",
      "  # 5  cid=232  dom=WEBNOVEL  sim=0.2549\n",
      "  # 6  cid=122  dom=GAME  sim=0.2396\n",
      "  # 7  cid=131  dom=WEBNOVEL  sim=0.2148\n",
      "  # 8  cid=123  dom=GAME  sim=0.1827\n",
      "  # 9  cid=121  dom=GAME  sim=0.1629\n",
      "\n",
      "[User 6] ì´ 9ê°œ\n",
      "  # 1  cid=57  dom=AV  sim=0.3405\n",
      "  # 2  cid=86  dom=AV  sim=0.3375\n",
      "  # 3  cid=49  dom=AV  sim=0.3046\n",
      "  # 4  cid=207  dom=WEBNOVEL  sim=0.2554\n",
      "  # 5  cid=232  dom=WEBNOVEL  sim=0.2549\n",
      "  # 6  cid=122  dom=GAME  sim=0.2396\n",
      "  # 7  cid=131  dom=WEBNOVEL  sim=0.2148\n",
      "  # 8  cid=123  dom=GAME  sim=0.1827\n",
      "  # 9  cid=121  dom=GAME  sim=0.1629\n",
      "\n",
      "[User 7] ì´ 9ê°œ\n",
      "  # 1  cid=123  dom=GAME  sim=0.4182\n",
      "  # 2  cid=109  dom=GAME  sim=0.3820\n",
      "  # 3  cid=124  dom=GAME  sim=0.3690\n",
      "  # 4  cid=207  dom=WEBNOVEL  sim=0.2836\n",
      "  # 5  cid=226  dom=WEBNOVEL  sim=0.2607\n",
      "  # 6  cid=49  dom=AV  sim=0.2575\n",
      "  # 7  cid=34  dom=AV  sim=0.2550\n",
      "  # 8  cid=137  dom=WEBNOVEL  sim=0.2547\n",
      "  # 9  cid=27  dom=AV  sim=0.2454\n",
      "\n",
      "[User 8] ì´ 9ê°œ\n",
      "  # 1  cid=232  dom=WEBNOVEL  sim=0.4273\n",
      "  # 2  cid=49  dom=AV  sim=0.4104\n",
      "  # 3  cid=222  dom=WEBNOVEL  sim=0.3949\n",
      "  # 4  cid=65  dom=AV  sim=0.3864\n",
      "  # 5  cid=8  dom=AV  sim=0.3794\n",
      "  # 6  cid=240  dom=WEBNOVEL  sim=0.3548\n",
      "  # 7  cid=113  dom=GAME  sim=0.1971\n",
      "  # 8  cid=123  dom=GAME  sim=0.1736\n",
      "  # 9  cid=122  dom=GAME  sim=0.1426\n",
      "\n",
      "[User 9] ì´ 9ê°œ\n",
      "  # 1  cid=179  dom=WEBNOVEL  sim=0.4346\n",
      "  # 2  cid=222  dom=WEBNOVEL  sim=0.3852\n",
      "  # 3  cid=144  dom=WEBNOVEL  sim=0.3690\n",
      "  # 4  cid=8  dom=AV  sim=0.3655\n",
      "  # 5  cid=65  dom=AV  sim=0.3321\n",
      "  # 6  cid=49  dom=AV  sim=0.2912\n",
      "  # 7  cid=113  dom=GAME  sim=0.2341\n",
      "  # 8  cid=108  dom=GAME  sim=0.1968\n",
      "  # 9  cid=124  dom=GAME  sim=0.1598\n",
      "\n",
      "[User 10] ì´ 9ê°œ\n",
      "  # 1  cid=42  dom=AV  sim=0.4515\n",
      "  # 2  cid=49  dom=AV  sim=0.3955\n",
      "  # 3  cid=121  dom=GAME  sim=0.3621\n",
      "  # 4  cid=34  dom=AV  sim=0.3472\n",
      "  # 5  cid=207  dom=WEBNOVEL  sim=0.2997\n",
      "  # 6  cid=123  dom=GAME  sim=0.2679\n",
      "  # 7  cid=122  dom=GAME  sim=0.2654\n",
      "  # 8  cid=242  dom=WEBNOVEL  sim=0.2528\n",
      "  # 9  cid=240  dom=WEBNOVEL  sim=0.2242\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#ë„ë©”ì¸ë³„ Top-N ì¶”ì²œ (ìœ ì €/ì•„ì´í…œ ì„ë² ë”© + contents.csv)\n",
    "#ì…ë ¥:\n",
    "#  clean/user_embeddings.csv                  (user_id, username, emb_0..)\n",
    "#  clean/item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv (content_id, emb_0..)\n",
    "#  clean/contents.csv                         (content_id, domain)\n",
    "#ì¶œë ¥:\n",
    "#  clean/recommendations_per_domain.csv       (user_id, username, rank, content_id, score, domain)\n",
    "\n",
    "#ì˜µì…˜:\n",
    "#  DOMAIN_QUOTA: ê° ë„ë©”ì¸ë³„ ì¶”ì²œ ê°œìˆ˜ (ì˜ˆ: {'AV':3,'GAME':3,'WEBNOVEL':3})\n",
    "#  FILL_WITH_GLOBAL: ë„ë©”ì¸ ë‚´ í›„ë³´ ë¶€ì¡± ì‹œ ì „ì²´ì—ì„œ ì±„ì›€\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict\n",
    "\n",
    "# ===== ê²½ë¡œ =====\n",
    "BASE = r\"C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\"\n",
    "USER_EMB   = os.path.join(BASE, \"user_embeddings.csv\")\n",
    "ITEM_EMB_1 = os.path.join(BASE, \"item_embeddings_torch.csv\")\n",
    "ITEM_EMB_2 = os.path.join(BASE, \"item_embeddings.csv\")\n",
    "CONTENTS   = os.path.join(BASE, \"contents.csv\")\n",
    "OUT_CSV    = os.path.join(BASE, \"recommendations_per_domain.csv\")\n",
    "\n",
    "# ===== ì„¤ì • =====\n",
    "DOMAIN_QUOTA: Dict[str, int] = {\"AV\": 3, \"GAME\": 3, \"WEBNOVEL\": 3}   # ë„ë©”ì¸ë³„ ê°œìˆ˜\n",
    "FILL_WITH_GLOBAL = True    # ë„ë©”ì¸ í›„ë³´ ë¶€ì¡± ì‹œ ê¸€ë¡œë²Œ Topìœ¼ë¡œ ë³´ì¶©\n",
    "ASSUME_NORMALIZED = True   # ì„ë² ë”©ì´ ì´ë¯¸ L2 ì •ê·œí™”ë¼ë©´ True\n",
    "EXCLUDE_DUP_CONTENTS = True\n",
    "\n",
    "# ===== ìœ í‹¸ =====\n",
    "def read_csv_retry(path, encodings=(\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"), **kw) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    last=None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kw)\n",
    "        except Exception as e:\n",
    "            last=e\n",
    "    raise last\n",
    "\n",
    "def l2norm(X: np.ndarray) -> np.ndarray:\n",
    "    return X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def _coerce_content_id_column(df: pd.DataFrame) -> pd.Series:\n",
    "    df = df.copy()\n",
    "    orig_cols = list(df.columns)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    name_map = {c.lower(): c for c in df.columns}\n",
    "    for key in [\"content_id\", \"contentid\", \"content id\", \"id\", \"unnamed: 0\"]:\n",
    "        if key in name_map:\n",
    "            s = df[name_map[key]]\n",
    "            if s.dtype == object:\n",
    "                s = s.astype(str).str.strip()\n",
    "            s = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "            if not s.isna().all():\n",
    "                return s\n",
    "    raise RuntimeError(f\"content_id ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì»¬ëŸ¼: {orig_cols}\")\n",
    "\n",
    "def load_user_embeddings() -> pd.DataFrame:\n",
    "    u = read_csv_retry(USER_EMB)\n",
    "    if u is None or u.empty:\n",
    "        raise RuntimeError(\"user_embeddings.csv í•„ìš”.\")\n",
    "    u = u.copy()\n",
    "    u.columns = [c.strip() for c in u.columns]\n",
    "    if \"user_id\" not in u.columns:\n",
    "        if \"id\" in u.columns:\n",
    "            u = u.rename(columns={\"id\": \"user_id\"})\n",
    "        else:\n",
    "            raise RuntimeError(\"user_embeddings.csvì— user_id ì»¬ëŸ¼ í•„ìš”.\")\n",
    "    u[\"user_id\"] = pd.to_numeric(u[\"user_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    u = u[u[\"user_id\"].notna()].copy()\n",
    "    u[\"user_id\"] = u[\"user_id\"].astype(int)\n",
    "\n",
    "    emb_cols = [c for c in u.columns if c.lower().startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        raise RuntimeError(\"user_embeddings.csvì— emb_ë¡œ ì‹œì‘í•˜ëŠ” ì„ë² ë”© ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    for c in emb_cols:\n",
    "        u[c] = pd.to_numeric(u[c], errors=\"coerce\")\n",
    "    u.dropna(subset=emb_cols, inplace=True)\n",
    "    if not ASSUME_NORMALIZED:\n",
    "        u.loc[:, emb_cols] = l2norm(u[emb_cols].to_numpy(np.float32))\n",
    "\n",
    "    # username ì»¬ëŸ¼ ìœ ì—° ì²˜ë¦¬\n",
    "    uname_col = None\n",
    "    for c in u.columns:\n",
    "        if c.lower() == \"username\":\n",
    "            uname_col = c; break\n",
    "    if uname_col is None:\n",
    "        u[\"username\"] = \"\"\n",
    "        uname_col = \"username\"\n",
    "\n",
    "    return u[[\"user_id\", uname_col] + emb_cols].rename(columns={uname_col: \"username\"})\n",
    "\n",
    "def load_item_embeddings() -> pd.DataFrame:\n",
    "    i1 = read_csv_retry(ITEM_EMB_1)\n",
    "    if i1 is not None and not i1.empty:\n",
    "        df = i1; src = os.path.basename(ITEM_EMB_1)\n",
    "    else:\n",
    "        i2 = read_csv_retry(ITEM_EMB_2)\n",
    "        if i2 is None or i2.empty:\n",
    "            raise RuntimeError(\"item_embeddings CSV í•„ìš”: item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv\")\n",
    "        df = i2; src = os.path.basename(ITEM_EMB_2)\n",
    "\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df[\"content_id\"] = _coerce_content_id_column(df)\n",
    "\n",
    "    emb_cols = [c for c in df.columns if c.lower().startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        alt = [c for c in df.columns if re.search(r\"\\d+$\", c)]\n",
    "        raise RuntimeError(f\"ì„ë² ë”© ì»¬ëŸ¼(emb_*) ì—†ìŒ. í™•ì¸ í•„ìš”. í›„ë³´: {alt}\")\n",
    "\n",
    "    for c in emb_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=emb_cols + [\"content_id\"]).copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    if EXCLUDE_DUP_CONTENTS:\n",
    "        df = df.drop_duplicates(subset=[\"content_id\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    if not ASSUME_NORMALIZED:\n",
    "        df.loc[:, emb_cols] = l2norm(df[emb_cols].to_numpy(np.float32))\n",
    "\n",
    "    print(f\"âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: {src} (items={len(df)}, dim={len(emb_cols)})\")\n",
    "    return df[[\"content_id\"] + emb_cols]\n",
    "\n",
    "def main():\n",
    "    # --- ë°ì´í„° ë¡œë“œ ---\n",
    "    u = load_user_embeddings()\n",
    "    i = load_item_embeddings()\n",
    "\n",
    "    contents = read_csv_retry(CONTENTS)\n",
    "    if contents is None or contents.empty or {\"content_id\",\"domain\"} - set(contents.columns):\n",
    "        raise RuntimeError(\"contents.csv í•„ìš” (content_id, domain).\")\n",
    "    dom_map = contents[[\"content_id\",\"domain\"]].dropna().copy()\n",
    "    dom_map[\"content_id\"] = pd.to_numeric(dom_map[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    dom_map = dom_map[dom_map[\"content_id\"].notna()].copy()\n",
    "    dom_map[\"content_id\"] = dom_map[\"content_id\"].astype(int)\n",
    "    dom_map = dom_map.set_index(\"content_id\")[\"domain\"].to_dict()\n",
    "\n",
    "    ucols = [c for c in u.columns if c.lower().startswith(\"emb_\")]\n",
    "    icols = [c for c in i.columns if c.lower().startswith(\"emb_\")]\n",
    "\n",
    "    U = u[ucols].to_numpy(np.float32); U = l2norm(U)   # ì•ˆì „í•˜ê²Œ í•œ ë²ˆ ë” ì •ê·œí™”\n",
    "    I = i[icols].to_numpy(np.float32); I = l2norm(I)\n",
    "    item_ids = i[\"content_id\"].to_numpy(int)\n",
    "\n",
    "    # --- ì „ì—­ Top ê³„ì‚°ì— ì‚¬ìš©í•  ì „ì²´ ì ìˆ˜(ìœ ì €ë³„) ì¤€ë¹„ìš© ---\n",
    "    # (ëŒ€ê·œëª¨ë©´ ìœ ì €ë³„ë¡œ on-the-fly ê³„ì‚°í•´ë„ OK)\n",
    "    rec_rows = []\n",
    "    for r in range(U.shape[0]):\n",
    "        uvec = U[r:r+1, :]                   # [1, D]\n",
    "        sims = (I @ uvec.T).reshape(-1)      # [N,]\n",
    "\n",
    "        # 1) ë„ë©”ì¸ë³„ quota ì±„ìš°ê¸°\n",
    "        picked_mask = np.zeros_like(sims, dtype=bool)\n",
    "        per_domain_picks = []\n",
    "        for dom, need in DOMAIN_QUOTA.items():\n",
    "            if need <= 0: \n",
    "                continue\n",
    "            # í•´ë‹¹ ë„ë©”ì¸ ì•„ì´í…œ ë§ˆìŠ¤í¬\n",
    "            dom_mask = np.array([dom_map.get(int(cid)) == dom for cid in item_ids], dtype=bool)\n",
    "            if not dom_mask.any():\n",
    "                continue\n",
    "            # ì•„ì§ ì•ˆ ë½‘íŒ + í•´ë‹¹ ë„ë©”ì¸\n",
    "            cand_mask = dom_mask & (~picked_mask)\n",
    "            if not cand_mask.any():\n",
    "                continue\n",
    "            cand_idx = np.where(cand_mask)[0]\n",
    "            cand_scores = sims[cand_idx]\n",
    "\n",
    "            k = min(need, len(cand_idx))\n",
    "            part = np.argpartition(-cand_scores, k-1)[:k]\n",
    "            order = part[np.argsort(-cand_scores[part])]\n",
    "            chosen = cand_idx[order]\n",
    "\n",
    "            picked_mask[chosen] = True\n",
    "            for j in chosen:\n",
    "                per_domain_picks.append((int(item_ids[j]), float(sims[j]), dom))\n",
    "\n",
    "        # 2) ë¶€ì¡±ë¶„ ê¸€ë¡œë²Œ Topìœ¼ë¡œ ë³´ì¶©(ì˜µì…˜)\n",
    "        total_need = sum(max(0, n) for n in DOMAIN_QUOTA.values())\n",
    "        if FILL_WITH_GLOBAL and picked_mask.sum() < total_need:\n",
    "            remain = total_need - picked_mask.sum()\n",
    "            left_idx = np.where(~picked_mask)[0]\n",
    "            if len(left_idx) > 0:\n",
    "                left_scores = sims[left_idx]\n",
    "                k2 = min(remain, len(left_idx))\n",
    "                part = np.argpartition(-left_scores, k2-1)[:k2]\n",
    "                order = part[np.argsort(-left_scores[part])]\n",
    "                chosen = left_idx[order]\n",
    "                for j in chosen:\n",
    "                    c = int(item_ids[j])\n",
    "                    per_domain_picks.append((c, float(sims[j]), dom_map.get(c, \"\")))\n",
    "                    picked_mask[j] = True\n",
    "\n",
    "        # 3) rank ë§¤ê¸°ê³  ì €ì¥\n",
    "        per_domain_picks.sort(key=lambda x: -x[1])\n",
    "        uid = int(u.loc[r, \"user_id\"]); uname = str(u.loc[r, \"username\"])\n",
    "        for rank, (cid, sc, dom) in enumerate(per_domain_picks, start=1):\n",
    "            rec_rows.append([uid, uname, rank, cid, sc, dom])\n",
    "\n",
    "    rec = pd.DataFrame(rec_rows, columns=[\"user_id\",\"username\",\"rank\",\"content_id\",\"score\",\"domain\"])\n",
    "    rec.sort_values([\"user_id\",\"rank\"]).to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… recommendations_per_domain.csv ì €ì¥: {OUT_CSV} (rows={len(rec)})\")\n",
    "\n",
    "    # ì½˜ì†” ìš”ì•½\n",
    "    try:\n",
    "        for uid in sorted(rec[\"user_id\"].unique()):\n",
    "            sub = rec[rec[\"user_id\"]==uid].sort_values(\"rank\")\n",
    "            print(f\"\\n[User {uid}] ì´ {len(sub)}ê°œ\")\n",
    "            for _,row in sub.iterrows():\n",
    "                print(f\"  #{int(row['rank']):2d}  cid={int(row['content_id'])}  dom={row['domain']}  sim={row['score']:.4f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
