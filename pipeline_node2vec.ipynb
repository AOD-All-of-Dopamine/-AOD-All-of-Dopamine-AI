{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1437a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# 1. ë¶„ë¦¬ ë¬¸ì(Separator)ë¥¼ ì„¸ë¯¸ì½œë¡ (;)ìœ¼ë¡œ ì§€ì •\n",
    "df = pd.read_csv('csv ë°ì´í„°\\clean\\contents.csv', encoding='cp949')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f20f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>master_title</th>\n",
       "      <th>original_title</th>\n",
       "      <th>poster_image_url</th>\n",
       "      <th>release_year</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>199</td>\n",
       "      <td>AV</td>\n",
       "      <td>ë„ì¿„ êµ¬ìš¸</td>\n",
       "      <td>æ±äº¬?ç¨®ãƒˆ?ã‚­ãƒ§?ã‚°?ãƒ«</td>\n",
       "      <td>https://image.tmdb.org/t/p/w500/nkDtqLGG7J7L7Y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>\"êµ°ì§‘ ì†ì— ì„ì—¬ ì¸ê°„ì„ ì‚¬ëƒ¥í•˜ê³  ê·¸ ì£½ì€ ê³ ê¸°ë¥¼ ë¨¹ëŠ” ê´´ì¸.\\n\\nìš°ë¦¬ë“¤ì€ ê·¸ë“¤ì„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>200</td>\n",
       "      <td>AV</td>\n",
       "      <td>ë§ˆì´ ë¦¬í‹€ í¬ë‹ˆ: ìš°ì •ì€ ë§ˆë²•</td>\n",
       "      <td>My Little Pony: Friendship Is Magic</td>\n",
       "      <td>https://image.tmdb.org/t/p/w500/iqpxbJX66AjG7i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>í¬ë‹ˆ íŠ¸ì™€ì¼ë¼ì‡ ìŠ¤íŒŒí´ê³¼ ë‹¤ì„¯ ëª…ì˜ í¬ë‹ˆê°€ ê°€ê¹Œìš´ ì¹œêµ¬ê°€ ë˜ë©´ì„œ ìš°ì •ì— ìƒê¸´ ë¬¸ì œë“¤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>201</td>\n",
       "      <td>GAME</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://shared.akamai.steamstatic.com/store_it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>ì„¸ê³„ ìµœê³ ì˜ ì˜¨ë¼ì¸ ì•¡ì…˜ ê²Œì„ì„ ì¦ê¸°ì„¸ìš”. ì „ ì„¸ê³„ì ìœ¼ë¡œ ìœ ëª…í•œ íŒ€ ê¸°ë°˜ì˜ ê²Œì„ì—ì„œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>202</td>\n",
       "      <td>GAME</td>\n",
       "      <td>Team Fortress Classic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://shared.akamai.steamstatic.com/store_it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>ì „ ì„¸ê³„ì—ì„œ ê°€ì¥ ìœ ëª…í•œ ì˜¨ë¼ì¸ ì•¡ì…˜ ê²Œì„ ì¤‘ í•˜ë‚˜ì¸ Team Fortress Cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>203</td>\n",
       "      <td>GAME</td>\n",
       "      <td>Day of Defeat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://shared.akamai.steamstatic.com/store_it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>ì œ 2ì°¨ ì„¸ê³„ ëŒ€ì „ì˜ ìœ ëŸ½ ì‘ì „ ì§€ì—­ì— ë°°ì¹˜ëœ ìƒìƒí•œ ì¶”ì¶•êµ­ ëŒ€ ì—°í•©êµ° íŒ€í”Œë ˆì´ì— ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>204</td>\n",
       "      <td>GAME</td>\n",
       "      <td>Deathmatch Classic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://shared.akamai.steamstatic.com/store_it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>Deathmatch Classic(DMC)ì—ì„œ ë¹ ë¥¸ í…œí¬ì˜ ë©€í‹°í”Œë ˆì´ì–´ ê²Œì„ì„ ì¦ê¸°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>205</td>\n",
       "      <td>GAME</td>\n",
       "      <td>Half-Life: Opposing Force</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://shared.akamai.steamstatic.com/store_it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>ì™„ì „íˆ ìƒˆë¡œìš´ ê²½í—˜ì˜ ì‹±ê¸€í”Œë ˆì´ì–´ ì•¡ì…˜ì„ ì²´í—˜í•´ë³´ì„¸ìš”. ì‚¬ë‚©ê³  ë§¹ë ¬í•œ ì™¸ê³„ì¸ ì êµ°ì„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>206</td>\n",
       "      <td>GAME</td>\n",
       "      <td>Ricochet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://shared.akamai.steamstatic.com/store_it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>50ê°œ ì´ìƒì˜ ì¶œíŒë¬¼ì—ì„œ ì˜¬í•´ì˜ ê²Œì„ìœ¼ë¡œ ì„ ì •ëœ Valveì˜ ë°ë·” íƒ€ì´í‹€ì€ ì•¡ì…˜ê³¼ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>207</td>\n",
       "      <td>GAME</td>\n",
       "      <td>Half-Life</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://shared.akamai.steamstatic.com/store_it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>50ê°œ ì´ìƒì˜ ì–¸ë¡ ì‚¬ì—ì„œ ì˜¬í•´ì˜ ê²Œì„ìœ¼ë¡œ ì„ ì •ëœ Valveì˜ ë°ë·” íƒ€ì´í‹€ë¡œì„œ, ìˆ˜ìƒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>208</td>\n",
       "      <td>GAME</td>\n",
       "      <td>Counter-Strike: Condition Zero</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://shared.akamai.steamstatic.com/store_it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>Nuclearvision Entertainmentê°€ ì œì‘í•œ Codename Gord...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     content_id domain                    master_title  \\\n",
       "200         199     AV                           ë„ì¿„ êµ¬ìš¸   \n",
       "201         200     AV                ë§ˆì´ ë¦¬í‹€ í¬ë‹ˆ: ìš°ì •ì€ ë§ˆë²•   \n",
       "202         201   GAME                  Counter-Strike   \n",
       "203         202   GAME           Team Fortress Classic   \n",
       "204         203   GAME                   Day of Defeat   \n",
       "205         204   GAME              Deathmatch Classic   \n",
       "206         205   GAME       Half-Life: Opposing Force   \n",
       "207         206   GAME                        Ricochet   \n",
       "208         207   GAME                       Half-Life   \n",
       "209         208   GAME  Counter-Strike: Condition Zero   \n",
       "\n",
       "                          original_title  \\\n",
       "200                         æ±äº¬?ç¨®ãƒˆ?ã‚­ãƒ§?ã‚°?ãƒ«   \n",
       "201  My Little Pony: Friendship Is Magic   \n",
       "202                                  NaN   \n",
       "203                                  NaN   \n",
       "204                                  NaN   \n",
       "205                                  NaN   \n",
       "206                                  NaN   \n",
       "207                                  NaN   \n",
       "208                                  NaN   \n",
       "209                                  NaN   \n",
       "\n",
       "                                      poster_image_url  release_year  \\\n",
       "200  https://image.tmdb.org/t/p/w500/nkDtqLGG7J7L7Y...           NaN   \n",
       "201  https://image.tmdb.org/t/p/w500/iqpxbJX66AjG7i...           NaN   \n",
       "202  https://shared.akamai.steamstatic.com/store_it...           NaN   \n",
       "203  https://shared.akamai.steamstatic.com/store_it...           NaN   \n",
       "204  https://shared.akamai.steamstatic.com/store_it...           NaN   \n",
       "205  https://shared.akamai.steamstatic.com/store_it...           NaN   \n",
       "206  https://shared.akamai.steamstatic.com/store_it...           NaN   \n",
       "207  https://shared.akamai.steamstatic.com/store_it...           NaN   \n",
       "208  https://shared.akamai.steamstatic.com/store_it...           NaN   \n",
       "209  https://shared.akamai.steamstatic.com/store_it...           NaN   \n",
       "\n",
       "     created_at  updated_at                                           synopsis  \n",
       "200  2025-10-29  2025-10-29  \"êµ°ì§‘ ì†ì— ì„ì—¬ ì¸ê°„ì„ ì‚¬ëƒ¥í•˜ê³  ê·¸ ì£½ì€ ê³ ê¸°ë¥¼ ë¨¹ëŠ” ê´´ì¸.\\n\\nìš°ë¦¬ë“¤ì€ ê·¸ë“¤ì„...  \n",
       "201  2025-10-29  2025-10-29  í¬ë‹ˆ íŠ¸ì™€ì¼ë¼ì‡ ìŠ¤íŒŒí´ê³¼ ë‹¤ì„¯ ëª…ì˜ í¬ë‹ˆê°€ ê°€ê¹Œìš´ ì¹œêµ¬ê°€ ë˜ë©´ì„œ ìš°ì •ì— ìƒê¸´ ë¬¸ì œë“¤...  \n",
       "202  2025-10-29  2025-10-29  ì„¸ê³„ ìµœê³ ì˜ ì˜¨ë¼ì¸ ì•¡ì…˜ ê²Œì„ì„ ì¦ê¸°ì„¸ìš”. ì „ ì„¸ê³„ì ìœ¼ë¡œ ìœ ëª…í•œ íŒ€ ê¸°ë°˜ì˜ ê²Œì„ì—ì„œ...  \n",
       "203  2025-10-29  2025-10-29  ì „ ì„¸ê³„ì—ì„œ ê°€ì¥ ìœ ëª…í•œ ì˜¨ë¼ì¸ ì•¡ì…˜ ê²Œì„ ì¤‘ í•˜ë‚˜ì¸ Team Fortress Cl...  \n",
       "204  2025-10-29  2025-10-29  ì œ 2ì°¨ ì„¸ê³„ ëŒ€ì „ì˜ ìœ ëŸ½ ì‘ì „ ì§€ì—­ì— ë°°ì¹˜ëœ ìƒìƒí•œ ì¶”ì¶•êµ­ ëŒ€ ì—°í•©êµ° íŒ€í”Œë ˆì´ì— ...  \n",
       "205  2025-10-29  2025-10-29  Deathmatch Classic(DMC)ì—ì„œ ë¹ ë¥¸ í…œí¬ì˜ ë©€í‹°í”Œë ˆì´ì–´ ê²Œì„ì„ ì¦ê¸°...  \n",
       "206  2025-10-29  2025-10-29  ì™„ì „íˆ ìƒˆë¡œìš´ ê²½í—˜ì˜ ì‹±ê¸€í”Œë ˆì´ì–´ ì•¡ì…˜ì„ ì²´í—˜í•´ë³´ì„¸ìš”. ì‚¬ë‚©ê³  ë§¹ë ¬í•œ ì™¸ê³„ì¸ ì êµ°ì„...  \n",
       "207  2025-10-29  2025-10-29  50ê°œ ì´ìƒì˜ ì¶œíŒë¬¼ì—ì„œ ì˜¬í•´ì˜ ê²Œì„ìœ¼ë¡œ ì„ ì •ëœ Valveì˜ ë°ë·” íƒ€ì´í‹€ì€ ì•¡ì…˜ê³¼ ...  \n",
       "208  2025-10-29  2025-10-29  50ê°œ ì´ìƒì˜ ì–¸ë¡ ì‚¬ì—ì„œ ì˜¬í•´ì˜ ê²Œì„ìœ¼ë¡œ ì„ ì •ëœ Valveì˜ ë°ë·” íƒ€ì´í‹€ë¡œì„œ, ìˆ˜ìƒ...  \n",
       "209  2025-10-29  2025-10-29  Nuclearvision Entertainmentê°€ ì œì‘í•œ Codename Gord...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[200:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89527ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: cp949 -*-\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# ===== íŒŒì¼ ê²½ë¡œ ì§ì ‘ ìˆ˜ì •í•´ì„œ ì‚¬ìš© =====\n",
    "INPUT_TXT = r\"csv ë°ì´í„°\\av_contents_txt.txt\"   # ì›ë³¸ txt\n",
    "OUTPUT_CSV = r\"csv ë°ì´í„°\\clean\\av_contents.csv\"  # ê²°ê³¼ csv\n",
    "\n",
    "# í•œ ì¤„ì— ìµœëŒ€ ëª‡ ê°œì˜ ì¥ë¥´ë¥¼ í¼ì¹ ì§€ (0~3 â†’ ì´ 4ê°œ)\n",
    "MAX_GENRES = 4\n",
    "\n",
    "# ===== ì¥ë¥´ â†’ TMDB ID ë§¤í•‘ =====\n",
    "# í‚¤ëŠ” JSON ì•ˆì— ë“¤ì–´ìˆëŠ” \"í•œê¸€ ì¥ë¥´ëª…\" ê·¸ëŒ€ë¡œ\n",
    "GENRE_MAP = {\n",
    "    u\"ê³µí¬\":       (27,    u\"ê³µí¬\"),\n",
    "    u\"ìŠ¤ë¦´ëŸ¬\":     (53,    u\"ìŠ¤ë¦´ëŸ¬\"),\n",
    "    u\"ì „ìŸ\":       (10752, u\"ì „ìŸ\"),\n",
    "    u\"ì•¡ì…˜\":       (28,    u\"ì•¡ì…˜\"),\n",
    "    u\"ë²”ì£„\":       (80,    u\"ë²”ì£„\"),\n",
    "    u\"íŒíƒ€ì§€\":     (14,    u\"íŒíƒ€ì§€\"),\n",
    "    u\"ìŒì•…\":       (10402, u\"ìŒì•…\"),\n",
    "    u\"ì½”ë¯¸ë””\":     (35,    u\"ì½”ë¯¸ë””\"),\n",
    "    u\"ì• ë‹ˆë©”ì´ì…˜\": (16,    u\"ì• ë‹ˆë©”ì´ì…˜\"),\n",
    "    u\"ë“œë¼ë§ˆ\":     (18,    u\"ë“œë¼ë§ˆ\"),\n",
    "    u\"ê°€ì¡±\":       (10751, u\"ê°€ì¡±\"),\n",
    "    u\"ë¯¸ìŠ¤í„°ë¦¬\":   (9648,  u\"ë¯¸ìŠ¤í„°ë¦¬\"),\n",
    "    u\"ì—­ì‚¬\":       (36,    u\"ì—­ì‚¬\"),\n",
    "    u\"ë¡œë§¨ìŠ¤\":     (10749, u\"ë¡œë§¨ìŠ¤\"),\n",
    "    u\"SF\":        (878,   u\"SF\"),\n",
    "    u\"ë‹¤íë©˜í„°ë¦¬\": (99,    u\"ë‹¤íë©˜í„°ë¦¬\"),\n",
    "    u\"ëª¨í—˜\":       (12,    u\"ëª¨í—˜\"),\n",
    "}\n",
    "\n",
    "\n",
    "def parse_line_to_json(line):\n",
    "    \"\"\"\n",
    "    í•œ ì¤„:\n",
    "    \"{\"\"release_date\"\":\"\"2025-10-22\"\",\"\"tmdb_id\"\":1306525,...}\"\n",
    "    ì´ëŸ° í˜•ì‹ì„ ì‹¤ì œ JSON ë¬¸ìì—´ë¡œ ë³µì›í•´ì„œ dict ë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    # ì²« ì¤„ \"row_to_json\" í—¤ë”ë©´ ê±´ë„ˆë›°ê¸°\n",
    "    if line.lower().startswith('\"row_to_json\"'):\n",
    "        return None\n",
    "\n",
    "    # ë§¨ ì•/ë’¤ í°ë”°ì˜´í‘œ ì œê±°\n",
    "    if line.startswith('\"') and line.endswith('\"'):\n",
    "        line = line[1:-1]\n",
    "\n",
    "    # \"\" -> \" ë¡œ ë³€ê²½ (CSV ìŠ¤íƒ€ì¼ escape)\n",
    "    line = line.replace('\"\"', '\"')\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(line)\n",
    "    except Exception as e:\n",
    "        print(\"JSON íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        print(\"ë¬¸ì œëœ ë¼ì¸:\", line)\n",
    "        return None\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ANSI(cp949) ë¡œ ì½ê³  ì“°ê¸°\n",
    "    with open(INPUT_TXT, \"r\", encoding=\"cp949\") as f_in, \\\n",
    "         open(OUTPUT_CSV, \"w\", encoding=\"cp949\", newline=\"\") as f_out:\n",
    "\n",
    "        writer = csv.writer(f_out)\n",
    "\n",
    "        # ===== í—¤ë” ì‘ì„± =====\n",
    "        header = [\n",
    "            \"av_type\",\n",
    "            \"content_id\",\n",
    "        ]\n",
    "        for i in range(MAX_GENRES):\n",
    "            header.append(\"genres.tmdb_genres.%d.id\" % i)\n",
    "            header.append(\"genres.tmdb_genres.%d.name\" % i)\n",
    "        header.extend([\"release_date\", \"tmdb_id\"])\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # ===== ë³¸ë¬¸ =====\n",
    "        for raw_line in f_in:\n",
    "            obj = parse_line_to_json(raw_line)\n",
    "            if obj is None:\n",
    "                continue\n",
    "\n",
    "            av_type = (obj.get(\"av_type\") or \"\").upper()  # movie -> MOVIE\n",
    "            content_id = obj.get(\"content_id\", \"\")\n",
    "            release_date = obj.get(\"release_date\", \"\")\n",
    "            tmdb_id = obj.get(\"tmdb_id\", \"\")\n",
    "            genres = obj.get(\"genres\") or []\n",
    "\n",
    "            row = [av_type, content_id]\n",
    "\n",
    "            # ì¥ë¥´ ìµœëŒ€ 4ê°œê¹Œì§€ í¼ì¹˜ê¸°\n",
    "            for i in range(MAX_GENRES):\n",
    "                if i < len(genres):\n",
    "                    g = genres[i]\n",
    "                    # ë§¤í•‘ ìˆìœ¼ë©´ id/name, ì—†ìœ¼ë©´ id ê³µë°± + ì›ë³¸ ì´ë¦„\n",
    "                    if g in GENRE_MAP:\n",
    "                        gid, gname = GENRE_MAP[g]\n",
    "                    else:\n",
    "                        gid, gname = \"\", g\n",
    "                    row.extend([gid, gname])\n",
    "                else:\n",
    "                    # ë¶€ì¡±í•œ ì¹¸ì€ ë¹ˆì¹¸\n",
    "                    row.extend([\"\", \"\"])\n",
    "\n",
    "            row.extend([release_date, tmdb_id])\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e2fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ! -> csv ë°ì´í„°\\clean\\contents.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: cp949 -*-\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "INPUT_TXT = r\"csv ë°ì´í„°\\contents_txt.txt\"   # ì›ë³¸ txt\n",
    "OUTPUT_CSV = r\"csv ë°ì´í„°\\clean\\contents.csv\"  # ê²°ê³¼ csv\n",
    "\n",
    "# ===== JSON í•œ ì¤„ íŒŒì‹± ìœ í‹¸ =====\n",
    "def parse_line_to_json(line):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    # ì²« ì¤„ \"row_to_json\" í—¤ë”ë©´ ìŠ¤í‚µ\n",
    "    if line.lower().startswith('\"row_to_json\"'):\n",
    "        return None\n",
    "\n",
    "    # ë§¨ ì•/ë’¤ í°ë”°ì˜´í‘œ ì œê±°\n",
    "    if line.startswith('\"') and line.endswith('\"'):\n",
    "        line = line[1:-1]\n",
    "\n",
    "    # \"\" -> \" (CSV ìŠ¤íƒ€ì¼ escape ì œê±°)\n",
    "    line = line.replace('\"\"', '\"')\n",
    "\n",
    "    try:\n",
    "        return json.loads(line)\n",
    "    except Exception as e:\n",
    "        print(\"JSON íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        print(\"ë¬¸ì œëœ ë¼ì¸:\", line)\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ì…ë ¥ íŒŒì¼ ì²´í¬\n",
    "    if not os.path.exists(INPUT_TXT):\n",
    "        print(\"âŒ INPUT_TXT íŒŒì¼ ì—†ìŒ\")\n",
    "        print(\"INPUT_TXT =\", INPUT_TXT)\n",
    "        return\n",
    "\n",
    "    # ì¶œë ¥ í´ë” ì—†ìœ¼ë©´ ìƒì„±\n",
    "    out_dir = os.path.dirname(OUTPUT_CSV)\n",
    "    if out_dir and not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(INPUT_TXT, \"r\", encoding=\"cp949\") as f_in, \\\n",
    "         open(OUTPUT_CSV, \"w\", encoding=\"cp949\", newline=\"\") as f_out:\n",
    "\n",
    "        writer = csv.writer(f_out)\n",
    "\n",
    "        # ===== í—¤ë” =====\n",
    "        header = [\n",
    "            \"content_id\",\n",
    "            \"domain\",\n",
    "            \"master_title\",\n",
    "            \"original_title\",\n",
    "            \"poster_image_url\",\n",
    "            \"release_year\",\n",
    "            \"created_at\",\n",
    "            \"updated_at\",\n",
    "            \"synopsis\",\n",
    "        ]\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # ===== ë³¸ë¬¸ =====\n",
    "        for raw_line in f_in:\n",
    "            obj = parse_line_to_json(raw_line)\n",
    "            if obj is None:\n",
    "                continue\n",
    "\n",
    "            content_id       = obj.get(\"content_id\", \"\")\n",
    "            domain           = obj.get(\"domain\", \"\")\n",
    "            master_title     = obj.get(\"master_title\", \"\")\n",
    "            original_title   = obj.get(\"original_title\", \"\")\n",
    "            poster_image_url = obj.get(\"poster_image_url\", \"\")\n",
    "            release_date     = obj.get(\"release_date\")  # nullì´ë©´ None\n",
    "            created_at       = obj.get(\"created_at\", \"\")\n",
    "            updated_at       = obj.get(\"updated_at\", \"\")\n",
    "            synopsis         = obj.get(\"synopsis\", \"\")\n",
    "\n",
    "            # release_year: release_dateê°€ \"2025-10-29\" ì´ëŸ° í˜•ì‹ì´ë©´ ì• 4ìë¦¬\n",
    "            if release_date:\n",
    "                release_year = str(release_date)[:4]\n",
    "            else:\n",
    "                release_year = \"\"\n",
    "\n",
    "            # created_at / updated_at ì€ ë‚ ì§œë§Œ ì“°ê³  ì‹¶ìœ¼ë©´ ì´ì²˜ëŸ¼ ìë¥´ê¸°\n",
    "            if created_at:\n",
    "                created_at_out = str(created_at).split(\"T\", 1)[0]\n",
    "            else:\n",
    "                created_at_out = \"\"\n",
    "\n",
    "            if updated_at:\n",
    "                updated_at_out = str(updated_at).split(\"T\", 1)[0]\n",
    "            else:\n",
    "                updated_at_out = \"\"\n",
    "\n",
    "            row = [\n",
    "                content_id,\n",
    "                domain,\n",
    "                master_title,\n",
    "                original_title,\n",
    "                poster_image_url,\n",
    "                release_year,\n",
    "                created_at_out,\n",
    "                updated_at_out,\n",
    "                synopsis,\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"âœ… ì™„ë£Œ! ->\", OUTPUT_CSV)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dabb3c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ! -> csv ë°ì´í„°\\clean\\game_contents.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: cp949 -*-\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ===== ê²½ë¡œë§Œ ë„¤ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • =====\n",
    "INPUT_TXT = r\"csv ë°ì´í„°\\game_contents_txt.txt\"   # ì›ë³¸ txt\n",
    "OUTPUT_CSV = r\"csv ë°ì´í„°\\clean\\game_contents.csv\"  # ê²°ê³¼ csv\n",
    "\n",
    "def parse_line_to_json(line):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    # ì²« ì¤„ \"row_to_json\" í—¤ë” ìŠ¤í‚µ\n",
    "    if line.lower().startswith('\"row_to_json\"'):\n",
    "        return None\n",
    "\n",
    "    # ë§¨ ì•/ë’¤ í°ë”°ì˜´í‘œ ì œê±°\n",
    "    if line.startswith('\"') and line.endswith('\"'):\n",
    "        line = line[1:-1]\n",
    "\n",
    "    # \"\" -> \" (CSV escape ì œê±°)\n",
    "    line = line.replace('\"\"', '\"')\n",
    "\n",
    "    try:\n",
    "        return json.loads(line)\n",
    "    except Exception as e:\n",
    "        print(\"JSON íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        print(\"ë¬¸ì œëœ ë¼ì¸:\", line)\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ì…ë ¥ íŒŒì¼ ì²´í¬\n",
    "    if not os.path.exists(INPUT_TXT):\n",
    "        print(\"âŒ INPUT_TXT íŒŒì¼ ì—†ìŒ\")\n",
    "        print(\"INPUT_TXT =\", INPUT_TXT)\n",
    "        print(\"í˜„ì¬ ì‘ì—… ë””ë ‰í„°ë¦¬ =\", os.getcwd())\n",
    "        return\n",
    "\n",
    "    # ì¶œë ¥ í´ë” ì—†ìœ¼ë©´ ë§Œë“¤ê¸°\n",
    "    out_dir = os.path.dirname(OUTPUT_CSV)\n",
    "    if out_dir and not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(INPUT_TXT, \"r\", encoding=\"cp949\") as f_in, \\\n",
    "         open(OUTPUT_CSV, \"w\", encoding=\"cp949\", newline=\"\") as f_out:\n",
    "\n",
    "        writer = csv.writer(f_out)\n",
    "\n",
    "        # ===== í—¤ë” =====\n",
    "        header = [\n",
    "            \"content_id\",\n",
    "            \"developer\",\n",
    "            \"publisher\",\n",
    "            \"release_date\",\n",
    "            \"platform_mac\",\n",
    "            \"platform_linux\",\n",
    "            \"platform_windows\",\n",
    "            \"genres_str\",\n",
    "        ]\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # ===== ë³¸ë¬¸ =====\n",
    "        for raw_line in f_in:\n",
    "            obj = parse_line_to_json(raw_line)\n",
    "            if obj is None:\n",
    "                continue\n",
    "\n",
    "            content_id   = obj.get(\"content_id\", \"\")\n",
    "            developer    = obj.get(\"developer\", \"\")\n",
    "            publisher    = obj.get(\"publisher\", \"\")\n",
    "            release_date = obj.get(\"release_date\", \"\")  # \"2000-11-01\" ê°™ì€ í˜•ì‹\n",
    "\n",
    "            platforms = obj.get(\"platforms\") or {}\n",
    "            platform_mac    = bool(platforms.get(\"mac\", False))\n",
    "            platform_linux  = bool(platforms.get(\"linux\", False))\n",
    "            platform_window = bool(platforms.get(\"windows\", False))\n",
    "\n",
    "            genres = obj.get(\"genres\") or []\n",
    "            # ì—¬ëŸ¬ ê°œë©´ \"ì•¡ì…˜|RPG\" ì´ëŸ° ì‹ìœ¼ë¡œ ë¶™ì´ê¸°\n",
    "            genres_str = \"|\".join(genres)\n",
    "\n",
    "            row = [\n",
    "                content_id,\n",
    "                developer,\n",
    "                publisher,\n",
    "                release_date,\n",
    "                platform_mac,\n",
    "                platform_linux,\n",
    "                platform_window,\n",
    "                genres_str,\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"âœ… ì™„ë£Œ! ->\", OUTPUT_CSV)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae630de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ! -> csv ë°ì´í„°\\clean\\platform_data.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: cp949 -*-\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re  # âœ… ì¶”ê°€: ì˜ëª»ëœ \\ escape ì²˜ë¦¬ìš©\n",
    "\n",
    "# ===== ê²½ë¡œë§Œ ë„¤ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•´ì„œ ì‚¬ìš© =====\n",
    "INPUT_TXT = r\"csv ë°ì´í„°\\platform_data_txt.txt\"   # ì›ë³¸ txt\n",
    "OUTPUT_CSV = r\"csv ë°ì´í„°\\clean\\platform_data.csv\"  # ê²°ê³¼ csv\n",
    "\n",
    "def parse_line_to_json(line):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    # ì²« ì¤„ \"row_to_json\" í—¤ë” ìŠ¤í‚µ\n",
    "    if line.lower().startswith('\"row_to_json\"'):\n",
    "        return None\n",
    "\n",
    "    # ë§¨ ì•/ë’¤ í°ë”°ì˜´í‘œ ì œê±° (ìˆì„ ë•Œë§Œ)\n",
    "    if line.startswith('\"') and line.endswith('\"'):\n",
    "        line = line[1:-1]\n",
    "        # \"\" -> \" (CSV escape ì œê±°)\n",
    "        line = line.replace('\"\"', '\"')\n",
    "\n",
    "    # âœ… ë¬¸ì œ: \"\\ 5,500\" ê°™ì€ ì˜ëª»ëœ JSON ì´ìŠ¤ì¼€ì´í”„ ìˆ˜ì •\n",
    "    #   \\ ë’¤ì— ê³µë°±ì´ ì˜¤ëŠ” ê²½ìš° â†’ \\\\ + ê³µë°± ìœ¼ë¡œ ë°”ê¿”ì„œ ìœ íš¨í•˜ê²Œ ë§Œë“ ë‹¤.\n",
    "    #   ì˜ˆ: \"\\ 5,500\" -> \"\\\\ 5,500\"\n",
    "    line = re.sub(r'\\\\( )', r'\\\\\\\\\\1', line)\n",
    "\n",
    "    try:\n",
    "        return json.loads(line)\n",
    "    except Exception as e:\n",
    "        print(\"JSON íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        print(\"ë¬¸ì œëœ ë¼ì¸:\", line)\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ì…ë ¥ íŒŒì¼ ì²´í¬\n",
    "    if not os.path.exists(INPUT_TXT):\n",
    "        print(\"âŒ INPUT_TXT íŒŒì¼ ì—†ìŒ\")\n",
    "        print(\"INPUT_TXT =\", INPUT_TXT)\n",
    "        print(\"í˜„ì¬ ì‘ì—… ë””ë ‰í„°ë¦¬ =\", os.getcwd())\n",
    "        return\n",
    "\n",
    "    # ì¶œë ¥ í´ë” ì—†ìœ¼ë©´ ìƒì„±\n",
    "    out_dir = os.path.dirname(OUTPUT_CSV)\n",
    "    if out_dir and not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(INPUT_TXT, \"r\", encoding=\"cp949\") as f_in, \\\n",
    "         open(OUTPUT_CSV, \"w\", encoding=\"cp949\", newline=\"\") as f_out:\n",
    "\n",
    "        writer = csv.writer(f_out)\n",
    "\n",
    "        # ===== í—¤ë” =====\n",
    "        header = [\n",
    "            \"platform_data_id\",\n",
    "            \"content_id\",\n",
    "            \"platform_name\",\n",
    "            \"platform_specific_id\",\n",
    "            \"url\",\n",
    "            \"last_seen_at\",\n",
    "            \"runtime\",\n",
    "            \"season_count\",\n",
    "            \"cast_count\",\n",
    "            \"crew_count\",\n",
    "            \"watch_providers_keys\",\n",
    "        ]\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # ===== ë³¸ë¬¸ =====\n",
    "        for raw_line in f_in:\n",
    "            obj = parse_line_to_json(raw_line)\n",
    "            if obj is None:\n",
    "                continue\n",
    "\n",
    "            platform_data_id      = obj.get(\"platform_data_id\", \"\")\n",
    "            content_id            = obj.get(\"content_id\", \"\")\n",
    "            platform_name_raw     = obj.get(\"platform_name\", \"\")\n",
    "            platform_specific_id  = obj.get(\"platform_specific_id\", \"\")\n",
    "            url                   = obj.get(\"url\", \"\")\n",
    "            last_seen_at          = obj.get(\"last_seen_at\", \"\")\n",
    "            attrs                 = obj.get(\"attributes\") or {}\n",
    "\n",
    "            # platform_name: TMDB_MOVIE â†’ TMDB ì´ëŸ° ì‹ìœ¼ë¡œ ì¤„ì´ê¸°\n",
    "            if isinstance(platform_name_raw, str) and platform_name_raw.startswith(\"TMDB\"):\n",
    "                platform_name = \"TMDB\"\n",
    "            else:\n",
    "                platform_name = platform_name_raw\n",
    "\n",
    "            # last_seen_at: ë‚ ì§œë§Œ ë‚¨ê¸°ê¸° (ì˜ˆ: 2025-10-29T... â†’ 2025-10-29)\n",
    "            if last_seen_at:\n",
    "                last_seen_out = str(last_seen_at).split(\"T\", 1)[0]\n",
    "            else:\n",
    "                last_seen_out = \"\"\n",
    "\n",
    "            # runtime (Steam ë°ì´í„°ëŠ” ì—†ì„ ìˆ˜ë„ ìˆìŒ)\n",
    "            runtime = attrs.get(\"runtime\", \"\")\n",
    "\n",
    "            # season_count: TV ìª½ì—ë§Œ ì“°ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ê¸°ë³¸ 0.0\n",
    "            season_count = attrs.get(\"season_count\", 0.0)\n",
    "\n",
    "            # cast / crew ë¦¬ìŠ¤íŠ¸ (TMDB ìª½ì—ë§Œ ìˆì„ ìˆ˜ ìˆìŒ)\n",
    "            cast_list = attrs.get(\"cast\") or []\n",
    "            crew_list = attrs.get(\"crew\") or []\n",
    "            cast_count = len(cast_list)\n",
    "            crew_count = len(crew_list)\n",
    "\n",
    "            # watch_providers: ë¦¬ìŠ¤íŠ¸ë¥¼ \"|\" ë¡œ í•©ì¹˜ê¸° (Steamì€ ì—†ê³  TMDBë§Œ ìˆì„ ê°€ëŠ¥ì„± í¼)\n",
    "            wp_list = attrs.get(\"watch_providers\") or []\n",
    "            watch_providers_keys = \"|\".join(wp_list) if wp_list else \"\"\n",
    "\n",
    "            row = [\n",
    "                platform_data_id,\n",
    "                content_id,\n",
    "                platform_name,\n",
    "                platform_specific_id,\n",
    "                url,\n",
    "                last_seen_out,\n",
    "                runtime,\n",
    "                season_count,\n",
    "                cast_count,\n",
    "                crew_count,\n",
    "                watch_providers_keys,\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"âœ… ì™„ë£Œ! ->\", OUTPUT_CSV)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "545a8390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ! -> csv ë°ì´í„°\\clean\\raw_item.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re  # ì˜ëª»ëœ \\ escape ì²˜ë¦¬ìš©\n",
    "\n",
    "# ===== ê²½ë¡œë§Œ ë„¤ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•´ì„œ ì‚¬ìš© =====\n",
    "INPUT_TXT  = r\"csv ë°ì´í„°\\raw_item_txt.txt\"           # ì§€ê¸ˆ ê°™ì€ row_to_json í…ìŠ¤íŠ¸\n",
    "OUTPUT_CSV = r\"csv ë°ì´í„°\\clean\\raw_item.csv\"    # ë§Œë“¤ CSV\n",
    "\n",
    "\n",
    "def parse_line_to_json(line):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    # ì²« ì¤„ \"row_to_json\" í—¤ë” ìŠ¤í‚µ\n",
    "    if line.lower().startswith('\"row_to_json\"'):\n",
    "        return None\n",
    "\n",
    "    # CSVì—ì„œ ë”°ì˜´í‘œë¡œ í•œ ë²ˆ ë” ê°ì‹¸ì§„ ê²½ìš° ì²˜ë¦¬\n",
    "    if line.startswith('\"') and line.endswith('\"'):\n",
    "        line = line[1:-1]\n",
    "        # \"\" -> \" (CSV escape ì œê±°)\n",
    "        line = line.replace('\"\"', '\"')\n",
    "\n",
    "    # Steam ìª½ì—ì„œ ì¢…ì¢… ìƒê¸°ëŠ” ì˜ëª»ëœ \"\\ 5,500\" ê°™ì€ íŒ¨í„´ ìˆ˜ì •\n",
    "    # \\ ë’¤ì— ê³µë°±ì´ ì˜¤ëŠ” ê²½ìš° â†’ \\\\ + ê³µë°± ìœ¼ë¡œ ë°”ê¿”ì„œ ìœ íš¨í•œ JSONìœ¼ë¡œ\n",
    "    line = re.sub(r'\\\\( )', r'\\\\\\\\\\1', line)\n",
    "\n",
    "    try:\n",
    "        return json.loads(line)\n",
    "    except Exception as e:\n",
    "        print(\"JSON íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        print(\"ë¬¸ì œëœ ë¼ì¸:\", line[:300] + \"...\" if len(line) > 300 else line)\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ì…ë ¥ íŒŒì¼ ì²´í¬\n",
    "    if not os.path.exists(INPUT_TXT):\n",
    "        print(\"âŒ INPUT_TXT íŒŒì¼ ì—†ìŒ\")\n",
    "        print(\"INPUT_TXT =\", INPUT_TXT)\n",
    "        print(\"í˜„ì¬ ì‘ì—… ë””ë ‰í„°ë¦¬ =\", os.getcwd())\n",
    "        return\n",
    "\n",
    "    # ì¶œë ¥ í´ë” ì—†ìœ¼ë©´ ìƒì„±\n",
    "    out_dir = os.path.dirname(OUTPUT_CSV)\n",
    "    if out_dir and not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # âœ… ì…ì¶œë ¥ ëª¨ë‘ UTF-8 ë¡œ\n",
    "    with open(INPUT_TXT, \"r\", encoding=\"utf-8\") as f_in, \\\n",
    "         open(OUTPUT_CSV, \"w\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
    "\n",
    "        writer = csv.writer(f_out)\n",
    "\n",
    "        # ===== í—¤ë” =====\n",
    "        header = [\n",
    "            \"raw_id\",\n",
    "            \"domain\",\n",
    "            \"platform_name\",\n",
    "            \"platform_specific_id\",\n",
    "            \"name\",\n",
    "            \"content_type\",\n",
    "            \"genres_str\",\n",
    "            \"is_free\",\n",
    "            \"platform_mac\",\n",
    "            \"platform_linux\",\n",
    "            \"platform_windows\",\n",
    "            \"release_date\",\n",
    "            \"fetched_at\",\n",
    "            \"processed_at\",\n",
    "            \"url\",\n",
    "        ]\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # ===== ë³¸ë¬¸ =====\n",
    "        for raw_line in f_in:\n",
    "            obj = parse_line_to_json(raw_line)\n",
    "            if obj is None:\n",
    "                continue\n",
    "\n",
    "            raw_id               = obj.get(\"raw_id\", \"\")\n",
    "            domain               = obj.get(\"domain\", \"\")\n",
    "            platform_name        = obj.get(\"platform_name\", \"\")\n",
    "            platform_specific_id = obj.get(\"platform_specific_id\", \"\")\n",
    "            url                  = obj.get(\"url\", \"\")\n",
    "            fetched_at           = obj.get(\"fetched_at\", \"\")\n",
    "            processed_at         = obj.get(\"processed_at\", \"\")\n",
    "            payload              = obj.get(\"source_payload\") or {}\n",
    "\n",
    "            # ë‚ ì§œë§Œ (YYYY-MM-DD) ë¡œ ìë¥´ê¸°\n",
    "            if fetched_at:\n",
    "                fetched_at_out = str(fetched_at).split(\"T\", 1)[0]\n",
    "            else:\n",
    "                fetched_at_out = \"\"\n",
    "\n",
    "            if processed_at:\n",
    "                processed_at_out = str(processed_at).split(\"T\", 1)[0]\n",
    "            else:\n",
    "                processed_at_out = \"\"\n",
    "\n",
    "            # ì´ë¦„ / íƒ€ì…\n",
    "            name = (\n",
    "                payload.get(\"name\", \"\")\n",
    "                or payload.get(\"original_name\", \"\")\n",
    "                or \"\"\n",
    "            )\n",
    "            content_type = payload.get(\"type\", \"\") or payload.get(\"av_type\", \"\")\n",
    "\n",
    "            # ì¥ë¥´\n",
    "            genres = payload.get(\"genres\") or []\n",
    "            genres_str = \"|\".join(genres)\n",
    "\n",
    "            # ë¬´ë£Œ ì—¬ë¶€\n",
    "            is_free = payload.get(\"is_free\", \"\")\n",
    "\n",
    "            # í”Œë«í¼ (Steam GAME ìª½)\n",
    "            platforms = payload.get(\"platforms\") or {}\n",
    "            platform_mac     = bool(platforms.get(\"mac\", False))\n",
    "            platform_linux   = bool(platforms.get(\"linux\", False))\n",
    "            platform_windows = bool(platforms.get(\"windows\", False))\n",
    "\n",
    "            # ì¶œì‹œì¼:\n",
    "            #  - Steam: source_payload.release_date.date (dict)\n",
    "            #  - TMDB_TV: source_payload.first_air_date (string)\n",
    "            release_date = \"\"\n",
    "            if \"release_date\" in payload:\n",
    "                rd = payload.get(\"release_date\")\n",
    "                if isinstance(rd, dict):\n",
    "                    release_date = rd.get(\"date\", \"\") or \"\"\n",
    "                else:\n",
    "                    release_date = rd or \"\"\n",
    "            elif \"first_air_date\" in payload:\n",
    "                release_date = payload.get(\"first_air_date\", \"\")\n",
    "\n",
    "            row = [\n",
    "                raw_id,\n",
    "                domain,\n",
    "                platform_name,\n",
    "                platform_specific_id,\n",
    "                name,\n",
    "                content_type,\n",
    "                genres_str,\n",
    "                is_free,\n",
    "                platform_mac,\n",
    "                platform_linux,\n",
    "                platform_windows,\n",
    "                release_date,\n",
    "                fetched_at_out,\n",
    "                processed_at_out,\n",
    "                url,\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"âœ… ì™„ë£Œ! ->\", OUTPUT_CSV)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06eb2966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ! -> csv ë°ì´í„°\\clean\\webnovel_contents.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: cp949 -*-\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ===== íŒŒì¼ ê²½ë¡œ ìˆ˜ì • =====\n",
    "INPUT_TXT  = r\"csv ë°ì´í„°\\webnovel_contents_txt.txt\"           # ìœ„ì— ìˆëŠ” row_to_json í…ìŠ¤íŠ¸ íŒŒì¼\n",
    "OUTPUT_CSV = r\"csv ë°ì´í„°\\clean\\webnovel_contents.csv\"    # ê²°ê³¼ CSV\n",
    "\n",
    "def parse_line_to_json(line):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    # ì²« ì¤„ \"row_to_json\" í—¤ë” ìŠ¤í‚µ\n",
    "    if line.lower().startswith('\"row_to_json\"'):\n",
    "        return None\n",
    "\n",
    "    # ë°”ê¹¥ì— í•œë²ˆ ë” \"ë¡œ ê°ì‹¸ì ¸ ìˆì„ ìˆ˜ ìˆì–´ì„œ ì œê±°\n",
    "    if line.startswith('\"') and line.endswith('\"'):\n",
    "        line = line[1:-1]\n",
    "        # \"\" -> \" (CSV escape ì œê±°)\n",
    "        line = line.replace('\"\"', '\"')\n",
    "\n",
    "    try:\n",
    "        return json.loads(line)\n",
    "    except Exception as e:\n",
    "        print(\"JSON íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        print(\"ë¬¸ì œëœ ë¼ì¸:\", line)\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ì…ë ¥ íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "    if not os.path.exists(INPUT_TXT):\n",
    "        print(\"âŒ INPUT_TXT íŒŒì¼ ì—†ìŒ\")\n",
    "        print(\"INPUT_TXT =\", INPUT_TXT)\n",
    "        print(\"í˜„ì¬ ì‘ì—… ë””ë ‰í„°ë¦¬ =\", os.getcwd())\n",
    "        return\n",
    "\n",
    "    # ì¶œë ¥ í´ë” ì—†ìœ¼ë©´ ìƒì„±\n",
    "    out_dir = os.path.dirname(OUTPUT_CSV)\n",
    "    if out_dir and not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(INPUT_TXT, \"r\", encoding=\"cp949\") as f_in, \\\n",
    "         open(OUTPUT_CSV, \"w\", encoding=\"cp949\", newline=\"\") as f_out:\n",
    "\n",
    "        writer = csv.writer(f_out)\n",
    "\n",
    "        # ===== í—¤ë” =====\n",
    "        header = [\n",
    "            \"content_id\",\n",
    "            \"age_rating\",\n",
    "            \"author\",\n",
    "            \"publisher\",\n",
    "            \"genres_str\",\n",
    "            \"started_at\",\n",
    "        ]\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # ===== ë³¸ë¬¸ =====\n",
    "        for raw_line in f_in:\n",
    "            obj = parse_line_to_json(raw_line)\n",
    "            if obj is None:\n",
    "                continue\n",
    "\n",
    "            content_id  = obj.get(\"content_id\", \"\")\n",
    "            age_rating  = obj.get(\"age_rating\", \"\")\n",
    "            author      = obj.get(\"author\", \"\")\n",
    "            publisher   = obj.get(\"publisher\", \"\")\n",
    "            genres      = obj.get(\"genres\") or []\n",
    "            started_at  = obj.get(\"started_at\", \"\")\n",
    "\n",
    "            genres_str = \"|\".join(genres)\n",
    "\n",
    "            row = [\n",
    "                content_id,\n",
    "                age_rating,\n",
    "                author,\n",
    "                publisher,\n",
    "                genres_str,\n",
    "                started_at,\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"âœ… ì™„ë£Œ! ->\", OUTPUT_CSV)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d55f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ì¤‘: av_contents.csv\n",
      "ì²˜ë¦¬ ì¤‘: contents.csv\n",
      "ì²˜ë¦¬ ì¤‘: content_raw_genres.csv\n",
      "ì²˜ë¦¬ ì¤‘: game_contents.csv\n",
      "ì²˜ë¦¬ ì¤‘: graph_edges_bipartite.csv\n",
      "ì²˜ë¦¬ ì¤‘: graph_edges_item_item.csv\n",
      "ì²˜ë¦¬ ì¤‘: graph_nodes.csv\n",
      "ì²˜ë¦¬ ì¤‘: item_embeddings_torch.csv\n",
      "ì²˜ë¦¬ ì¤‘: meta_nodes.csv\n",
      "ì²˜ë¦¬ ì¤‘: platform_data.csv\n",
      "ì²˜ë¦¬ ì¤‘: raw_item.csv\n",
      "ì²˜ë¦¬ ì¤‘: webnovel_contents.csv\n",
      "âœ… ìš”ì•½ íŒŒì¼ ìƒì„± ì™„ë£Œ: csv ë°ì´í„°\\clean\\csv_summary.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. ê¸°ë³¸ ê²½ë¡œ ì„¤ì •\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_dir = r\"csv ë°ì´í„°\\clean\"\n",
    "output_file = os.path.join(base_dir, \"csv_summary.txt\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. CSV ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "csv_files = [f for f in os.listdir(base_dir) if f.lower().endswith(\".csv\")]\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"âŒ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    exit()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. ê²°ê³¼ íŒŒì¼ ì´ˆê¸°í™”\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    f_out.write(\"ğŸ“Š CSV íŒŒì¼ ìš”ì•½ ë³´ê³ ì„œ\\n\")\n",
    "    f_out.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. ê° CSV íŒŒì¼ì— ëŒ€í•œ ì •ë³´ ìˆ˜ì§‘\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    print(f\"ì²˜ë¦¬ ì¤‘: {file_name}\")\n",
    "\n",
    "    # íŒŒì¼ ì½ê¸° (UTF-8 â†’ CP949 ìˆœìœ¼ë¡œ ì‹œë„)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file_path, encoding=\"cp949\")\n",
    "    except Exception as e:\n",
    "        with open(output_file, \"a\", encoding=\"utf-8\") as f_out:\n",
    "            f_out.write(f\"âš ï¸ {file_name} ì½ê¸° ì˜¤ë¥˜: {e}\\n\\n\")\n",
    "        continue\n",
    "\n",
    "    # â”€â”€ ìƒìœ„ 5ê°œ ìƒ˜í”Œë§Œ í‘œì‹œ â”€â”€\n",
    "    sample = df.head(5)\n",
    "\n",
    "    # â”€â”€ ê²°ê³¼ ì‘ì„± â”€â”€\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f_out:\n",
    "        f_out.write(f\"ğŸ“„ íŒŒì¼ëª…: {file_name}\\n\")\n",
    "        f_out.write(f\"ì»¬ëŸ¼ ({len(df.columns)}ê°œ): {list(df.columns)}\\n\\n\")\n",
    "        f_out.write(\"ìƒ˜í”Œ 5í–‰:\\n\")\n",
    "        f_out.write(sample.to_string(index=False))\n",
    "        f_out.write(\"\\n\" + \"-\" * 60 + \"\\n\\n\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. ì™„ë£Œ ë©”ì‹œì§€\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"âœ… ìš”ì•½ íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af077e",
   "metadata": {},
   "source": [
    "# content_raw_genres ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09e05ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë³€í™˜ëœ í–‰ ìˆ˜: 630\n",
      "âœ… ì €ì¥ ì™„ë£Œ: csv ë°ì´í„°\\clean\\content_raw_genres.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import Optional, List\n",
    "\n",
    "BASE = r\"csv ë°ì´í„°\\clean\"\n",
    "OLD_GENRES = os.path.join(BASE, \"content_raw_genres.csv\")  # ê¸°ì¡´ íŒŒì¼\n",
    "RAW_ITEM   = os.path.join(BASE, \"raw_item.csv\")            # raw_id, genres_str ìˆëŠ” íŒŒì¼\n",
    "\n",
    "# âš ï¸ ë®ì–´ì“°ê³  ì‹¶ìœ¼ë©´ OUT_GENRESë¥¼ OLD_GENRESë¡œ ë°”ê¿”ë„ ë¨\n",
    "OUT_GENRES = os.path.join(BASE, \"content_raw_genres.csv\")\n",
    "\n",
    "def read_csv_retry(path, encodings=(\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"), **kw) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    last = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kw)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "def parse_genres(genres_str: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    'ë“œë¼ë§ˆ|ë²”ì£„|ë¯¸ìŠ¤í„°ë¦¬' ì²˜ëŸ¼ ìƒê¸´ ë¬¸ìì—´ì„ ['ë“œë¼ë§ˆ','ë²”ì£„','ë¯¸ìŠ¤í„°ë¦¬']ë¡œ íŒŒì‹±.\n",
    "    ì—†ìœ¼ë©´ [] ë¦¬í„´.\n",
    "    \"\"\"\n",
    "    if not isinstance(genres_str, str):\n",
    "        return []\n",
    "    genres_str = genres_str.strip()\n",
    "    if not genres_str:\n",
    "        return []\n",
    "    parts = re.split(r\"[|,/Â·]\", genres_str)\n",
    "    parts = [g.strip() for g in parts if g.strip()]\n",
    "    return parts\n",
    "\n",
    "def main():\n",
    "    # 1) ê¸°ì¡´ content_raw_genres.csv\n",
    "    cg = read_csv_retry(OLD_GENRES)\n",
    "    if cg is None or cg.empty:\n",
    "        raise RuntimeError(\"content_raw_genres.csv ë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    cg = cg.copy()\n",
    "    cg.columns = [c.strip() for c in cg.columns]\n",
    "\n",
    "    needed_cg = [\"content_id\", \"source\", \"raw_genre\"]\n",
    "    miss_cg = [c for c in needed_cg if c not in cg.columns]\n",
    "    if miss_cg:\n",
    "        raise RuntimeError(\"content_raw_genres.csv ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤: \" + \", \".join(miss_cg))\n",
    "\n",
    "    cg[\"content_id\"] = pd.to_numeric(cg[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    cg = cg[cg[\"content_id\"].notna()].copy()\n",
    "    cg[\"content_id\"] = cg[\"content_id\"].astype(int)\n",
    "\n",
    "    # 2) raw_item.csv\n",
    "    ri = read_csv_retry(RAW_ITEM)\n",
    "    if ri is None or ri.empty:\n",
    "        raise RuntimeError(\"raw_item.csv ë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    ri = ri.copy()\n",
    "    ri.columns = [c.strip() for c in ri.columns]\n",
    "\n",
    "    needed_ri = [\"raw_id\", \"genres_str\"]\n",
    "    miss_ri = [c for c in needed_ri if c not in ri.columns]\n",
    "    if miss_ri:\n",
    "        raise RuntimeError(\"raw_item.csv ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤: \" + \", \".join(miss_ri))\n",
    "\n",
    "    ri[\"raw_id\"] = pd.to_numeric(ri[\"raw_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    ri = ri[ri[\"raw_id\"].notna()].copy()\n",
    "    ri[\"raw_id\"] = ri[\"raw_id\"].astype(int)\n",
    "\n",
    "    # 3) content_id == raw_id ë¡œ join í•´ì„œ genres_str ë¶™ì´ê¸°\n",
    "    merged = cg.merge(\n",
    "        ri[[\"raw_id\", \"genres_str\"]],\n",
    "        left_on=\"content_id\",\n",
    "        right_on=\"raw_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 4) raw_genre_1,2,3 ìƒì„±\n",
    "    new_rows = []\n",
    "    for _, row in merged.iterrows():\n",
    "        cid = int(row[\"content_id\"])\n",
    "        source = str(row[\"source\"]).strip()\n",
    "\n",
    "        base_genre = str(row[\"raw_genre\"]).strip()  # ê¸°ì¡´ í•œ ê°œì§œë¦¬ raw_genre\n",
    "        genres_from_str = parse_genres(row.get(\"genres_str\", \"\"))\n",
    "\n",
    "        # ìš°ì„ ìˆœìœ„:\n",
    "        # 1) genres_strì—ì„œ íŒŒì‹±í•œ ì¥ë¥´ë“¤ì„ ìš°ì„  ì‚¬ìš©\n",
    "        # 2) ê¸°ì¡´ raw_genreê°€ ë¹„ì–´ìˆì§€ ì•Šê³ , ëª©ë¡ì— ì—†ìœ¼ë©´ ë§¨ ì•ì— ì¶”ê°€\n",
    "        genres = list(genres_from_str)\n",
    "        if base_genre and base_genre not in genres:\n",
    "            genres.insert(0, base_genre)\n",
    "\n",
    "        # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ì‚¬ìš©\n",
    "        g1 = genres[0] if len(genres) > 0 else \"\"\n",
    "        g2 = genres[1] if len(genres) > 1 else \"\"\n",
    "        g3 = genres[2] if len(genres) > 2 else \"\"\n",
    "\n",
    "        new_rows.append((cid, source, g1, g2, g3))\n",
    "\n",
    "    out = pd.DataFrame(new_rows, columns=[\"content_id\", \"source\", \"raw_genre_1\", \"raw_genre_2\", \"raw_genre_3\"])\n",
    "\n",
    "    print(f\"âœ… ë³€í™˜ëœ í–‰ ìˆ˜: {len(out)}\")\n",
    "    out.to_csv(OUT_GENRES, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {OUT_GENRES}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "953bf7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ì¤‘: av_contents.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\av_contents.json (records: 13)\n",
      "ì²˜ë¦¬ ì¤‘: contents.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\contents.json (records: 139)\n",
      "ì²˜ë¦¬ ì¤‘: content_ratings.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\content_ratings.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: game_contents.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\game_contents.json (records: 22)\n",
      "ì²˜ë¦¬ ì¤‘: llm_recommendation_requests.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LG\\AppData\\Local\\Temp\\ipykernel_27168\\4233549663.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n",
      "C:\\Users\\LG\\AppData\\Local\\Temp\\ipykernel_27168\\4233549663.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n",
      "C:\\Users\\LG\\AppData\\Local\\Temp\\ipykernel_27168\\4233549663.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\llm_recommendation_requests.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: naver_series_novel.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\naver_series_novel.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: platform_data.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\platform_data.json (records: 123)\n",
      "ì²˜ë¦¬ ì¤‘: raw_items.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LG\\AppData\\Local\\Temp\\ipykernel_27168\\4233549663.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n",
      "C:\\Users\\LG\\AppData\\Local\\Temp\\ipykernel_27168\\4233549663.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\raw_items.json (records: 6)\n",
      "ì²˜ë¦¬ ì¤‘: transform_runs.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\transform_runs.json (records: 250)\n",
      "ì²˜ë¦¬ ì¤‘: users.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\users.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: user_preferences.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\user_preferences.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: user_preferred_content_types.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\user_preferred_content_types.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: user_preferred_genres.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\user_preferred_genres.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: user_roles.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\user_roles.json (records: 0)\n",
      "ì²˜ë¦¬ ì¤‘: webnovel_contents.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\webnovel_contents.json (records: 93)\n",
      "ì²˜ë¦¬ ì¤‘: webtoon_contents.csv\n",
      "  âœ“ ì €ì¥: C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\webtoon_contents.json (records: 0)\n",
      "\n",
      "âœ… ì™„ë£Œ: ëª¨ë“  JSONì€ clean/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ====== ì„¤ì • ======\n",
    "BASE_DIR = r\"C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\"\n",
    "OUT_DIR  = os.path.join(BASE_DIR, \"clean\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ì¸ì½”ë”© ì‹œë„ ìˆœì„œ\n",
    "ENCODINGS = [\"utf-8-sig\", \"utf-8\", \"cp949\", \"euc-kr\", \"latin1\"]\n",
    "\n",
    "# ê´„í˜¸ ë‚´ë¶€ ì½¤ë§ˆ ì¹˜í™˜ìš© (í† í¬ë‚˜ì´ì§• ì˜¤ë¥˜ íšŒí”¼)\n",
    "COMMA_PLACEHOLDER = \"âŸ\"\n",
    "\n",
    "DATE_COL_PAT = re.compile(r\"(date|_at)$\", re.IGNORECASE)  # release_date, created_at ë“±\n",
    "\n",
    "\n",
    "# ========== ìœ í‹¸ ==========\n",
    "\n",
    "def read_text_with_encodings(path: str) -> str:\n",
    "    for enc in ENCODINGS:\n",
    "        try:\n",
    "            with io.open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n",
    "                return f.read()\n",
    "        except Exception:\n",
    "            continue\n",
    "    # ìµœí›„: ì—ëŸ¬ ë¬´ì‹œ ë¼í‹´1\n",
    "    with io.open(path, \"r\", encoding=\"latin1\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def protect_commas_inside_brackets(line: str) -> str:\n",
    "    \"\"\"{},[],() ë‚´ë¶€ì˜ ì½¤ë§ˆë¥¼ ì„ì‹œ ì¹˜í™˜í•´ì„œ í•„ë“œ ìˆ˜ ë¶ˆì¼ì¹˜ ë°©ì§€\"\"\"\n",
    "    res, stack = [], []\n",
    "    for ch in line:\n",
    "        if ch in \"{[(\":\n",
    "            stack.append(ch); res.append(ch)\n",
    "        elif ch in \"}])\":\n",
    "            if stack: stack.pop()\n",
    "            res.append(ch)\n",
    "        else:\n",
    "            if ch == \",\" and stack:\n",
    "                res.append(COMMA_PLACEHOLDER)\n",
    "            else:\n",
    "                res.append(ch)\n",
    "    return \"\".join(res)\n",
    "\n",
    "def make_safe_temp_csv_text(raw_text: str) -> str:\n",
    "    return \"\\n\".join(protect_commas_inside_brackets(ln) for ln in raw_text.splitlines())\n",
    "\n",
    "def try_read_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"ì¼ë°˜â†’ì¹˜í™˜í…ìŠ¤íŠ¸â†’bad_lines skip ìˆœì„œë¡œ ë¡œë“œ ì‹œë„\"\"\"\n",
    "    raw = read_text_with_encodings(path)\n",
    "\n",
    "    # 1) ì¼ë°˜ ì‹œë„\n",
    "    for enc in ENCODINGS + [\"latin1\"]:\n",
    "        try:\n",
    "            return pd.read_csv(io.StringIO(raw), encoding=enc, engine=\"python\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) ê´„í˜¸ ë‚´ë¶€ ì½¤ë§ˆ ì¹˜í™˜ í›„ ì¬ì‹œë„\n",
    "    safe = make_safe_temp_csv_text(raw)\n",
    "    for enc in ENCODINGS + [\"latin1\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(io.StringIO(safe), encoding=enc, engine=\"python\")\n",
    "            # ì¹˜í™˜ ì›ë³µ\n",
    "            df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n",
    "            return df\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) ìµœí›„: bad lines skip\n",
    "    df = pd.read_csv(io.StringIO(safe), engine=\"python\", on_bad_lines=\"skip\")\n",
    "    df = df.applymap(lambda x: x.replace(COMMA_PLACEHOLDER, \",\") if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "def parse_json_cell(val):\n",
    "    \"\"\"CSV ì•ˆ JSON ë¬¸ìì—´(ì´ì¤‘ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ í¬í•¨)ì„ dict/listë¡œ íŒŒì‹±(ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€)\"\"\"\n",
    "    if not isinstance(val, str): return val\n",
    "    txt = val.strip()\n",
    "    if not txt: return val\n",
    "    # \"{\"\"k\"\":1}\" -> {\"k\":1}\n",
    "    if txt.startswith('\"') and txt.endswith('\"'):\n",
    "        txt = txt[1:-1]\n",
    "    txt = txt.replace('\"\"', '\"')\n",
    "    if (txt.startswith(\"{\") and txt.endswith(\"}\")) or (txt.startswith(\"[\") and txt.endswith(\"]\")):\n",
    "        try:\n",
    "            return json.loads(txt)\n",
    "        except Exception:\n",
    "            return val\n",
    "    return val\n",
    "\n",
    "def is_jsonish_series(s: pd.Series) -> bool:\n",
    "    \"\"\"ì‹œë¦¬ì¦ˆì˜ ì•ë¶€ë¶„ì„ ë³´ê³  JSON ë¬¸ìì—´ ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ True\"\"\"\n",
    "    sample = s.dropna().astype(str).head(20).tolist()\n",
    "    if not sample: return False\n",
    "    cnt = 0\n",
    "    for v in sample:\n",
    "        v = v.strip().strip('\"')\n",
    "        if (v.startswith(\"{\") and v.endswith(\"}\")) or (v.startswith(\"[\") and v.endswith(\"]\")):\n",
    "            cnt += 1\n",
    "    return cnt >= max(3, len(sample)//4)\n",
    "\n",
    "def normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # 1) JSON ë¬¸ìì—´ë¡œ ë³´ì´ëŠ” ì»¬ëŸ¼ íŒŒì‹±\n",
    "    for col in out.columns:\n",
    "        try:\n",
    "            if out[col].dtype == \"object\" and is_jsonish_series(out[col]):\n",
    "                out[col] = out[col].apply(parse_json_cell)\n",
    "        except Exception:\n",
    "            # ì»¬ëŸ¼ ë‹¨ìœ„ ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ì§„í–‰\n",
    "            pass\n",
    "\n",
    "    # 2) ë‚ ì§œ ì»¬ëŸ¼ ISO ë¬¸ìì—´í™”\n",
    "    for col in out.columns:\n",
    "        if DATE_COL_PAT.search(col):\n",
    "            try:\n",
    "                out[col] = pd.to_datetime(out[col], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 3) NaN/NaT -> None, numpy íƒ€ì… -> íŒŒì´ì¬ ê¸°ë³¸ íƒ€ì…í™”\n",
    "    out = out.where(pd.notnull(out), None)\n",
    "\n",
    "    return out\n",
    "\n",
    "def df_to_json_records(df: pd.DataFrame):\n",
    "    \"\"\"JSON ì§ë ¬í™” ì¹œí™”ì ì¸ records ìƒì„±\"\"\"\n",
    "    # pandas ê¸°ë³¸ dict ë³€í™˜\n",
    "    recs = df.to_dict(orient=\"records\")\n",
    "\n",
    "    # numpy íƒ€ì…, Timestamp ë“± ì²˜ë¦¬\n",
    "    def py_convert(v):\n",
    "        if isinstance(v, (np.integer,)): return int(v)\n",
    "        if isinstance(v, (np.floating,)) and not np.isnan(v): return float(v)\n",
    "        if isinstance(v, (np.bool_,)): return bool(v)\n",
    "        # Timestamp/Datetime -> ISO ë¬¸ìì—´\n",
    "        if isinstance(v, (pd.Timestamp,)): return v.strftime(\"%Y-%m-%dT%H:%M:%S\") if pd.notna(v) else None\n",
    "        return v\n",
    "\n",
    "    def walk(o):\n",
    "        if isinstance(o, dict):\n",
    "            return {k: walk(py_convert(v)) for k, v in o.items()}\n",
    "        if isinstance(o, list):\n",
    "            return [walk(py_convert(x)) for x in o]\n",
    "        return py_convert(o)\n",
    "\n",
    "    return [walk(r) for r in recs]\n",
    "\n",
    "\n",
    "# ========== ë©”ì¸ ==========\n",
    "\n",
    "def main():\n",
    "    csv_files = [f for f in os.listdir(BASE_DIR) if f.lower().endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        print(\"âŒ CSVê°€ ì—†ìŠµë‹ˆë‹¤.\"); return\n",
    "\n",
    "    for fn in csv_files:\n",
    "        src = os.path.join(BASE_DIR, fn)\n",
    "        stem = os.path.splitext(fn)[0]\n",
    "        dst_json = os.path.join(OUT_DIR, f\"{stem}.json\")\n",
    "\n",
    "        try:\n",
    "            print(f\"ì²˜ë¦¬ ì¤‘: {fn}\")\n",
    "            df = try_read_csv(src)\n",
    "            df_clean = normalize_df(df)\n",
    "            records = df_to_json_records(df_clean)\n",
    "\n",
    "            with open(dst_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"  âœ“ ì €ì¥: {dst_json} (records: {len(records)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ ì‹¤íŒ¨: {fn} -> {e}\")\n",
    "\n",
    "    print(\"\\nâœ… ì™„ë£Œ: ëª¨ë“  JSONì€ clean/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67294444",
   "metadata": {},
   "source": [
    "# ì´ë¶„ ê·¸ë˜í”„ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad23ee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… graph_nodes ì €ì¥: csv ë°ì´í„°\\clean\\graph_nodes.csv (rows=495)\n",
      "âœ… content_raw_genres ì €ì¥: csv ë°ì´í„°\\clean\\content_raw_genres.csv (rows=630)\n",
      "âœ… meta_nodes ì €ì¥: csv ë°ì´í„°\\clean\\meta_nodes.csv (rows=45)\n",
      "âœ… graph_edges_bipartite ì €ì¥: csv ë°ì´í„°\\clean\\graph_edges_bipartite.csv (edges=1125)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "BASE = r\"csv ë°ì´í„°\\clean\"\n",
    "PATH_CONTENTS = os.path.join(BASE, \"contents.csv\")\n",
    "PATH_AV       = os.path.join(BASE, \"av_contents.csv\")\n",
    "PATH_GAME     = os.path.join(BASE, \"game_contents.csv\")\n",
    "PATH_WEBNOVEL = os.path.join(BASE, \"webnovel_contents.csv\")  # ìˆìœ¼ë©´ ì‚¬ìš©\n",
    "\n",
    "OUT_NODES     = os.path.join(BASE, \"graph_nodes.csv\")              # ì½˜í…ì¸  ë…¸ë“œ\n",
    "OUT_RAWGEN    = os.path.join(BASE, \"content_raw_genres.csv\")       # ì›ë³¸ ì¥ë¥´ ì„¸ë¡œí‘œ\n",
    "OUT_META      = os.path.join(BASE, \"meta_nodes.csv\")               # ë©”íƒ€ë…¸ë“œ ëª©ë¡\n",
    "OUT_EDGES_BI  = os.path.join(BASE, \"graph_edges_bipartite.csv\")    # ì½˜í…ì¸ â†’ë©”íƒ€ ì—£ì§€\n",
    "\n",
    "# ì—£ì§€ ê°€ì¤‘ì¹˜(í•„ìš” ì‹œ ì¡°ì •)\n",
    "DOMAIN_EDGE_WEIGHT = 1.0\n",
    "GENRE_EDGE_WEIGHT  = 1.0\n",
    "\n",
    "ENCODINGS = [\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"]\n",
    "def read_csv_retry(path, **kwargs):\n",
    "    last = None\n",
    "    for enc in ENCODINGS:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kwargs)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "def safe_read(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"âš ï¸ ì—†ìŒ: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        return read_csv_retry(path)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì½ê¸° ì‹¤íŒ¨: {path} -> {e}\")\n",
    "        return None\n",
    "\n",
    "def add_prefix_except_key(df, prefix, key=\"content_id\"):\n",
    "    if df is None: return None\n",
    "    return df.rename(columns={c: (c if c==key else f\"{prefix}{c}\") for c in df.columns})\n",
    "\n",
    "def norm_for_id(s: str) -> str:\n",
    "    \"\"\"\n",
    "    ë©”íƒ€ë…¸ë“œ IDìš© ê°„ë‹¨ ì •ê·œí™” (ê³µë°±/êµ¬ë‘ì  ì œê±°, ì†Œë¬¸ì)\n",
    "    ì˜ˆ: 'Science Fiction' -> 'science_fiction'\n",
    "    \"\"\"\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    # í•œ/ì˜/ìˆ«ì ì™¸ëŠ” ê³µë°± ì¹˜í™˜\n",
    "    s = re.sub(r\"[^\\w\\sê°€-í£]\", \" \", s)\n",
    "    s = \"_\".join(s.split())\n",
    "    return s\n",
    "\n",
    "# 1) ì½˜í…ì¸  ë…¸ë“œ ìƒì„±(ê·¸ëŒ€ë¡œ ìœ ì§€)\n",
    "def build_nodes():\n",
    "    contents = safe_read(PATH_CONTENTS)\n",
    "    if contents is None or contents.empty:\n",
    "        raise RuntimeError(\"âŒ contents.csv ë¹„ì–´ìˆìŒ/ë¡œë“œ ì‹¤íŒ¨\")\n",
    "    if \"content_id\" not in contents.columns or \"domain\" not in contents.columns:\n",
    "        raise RuntimeError(\"âŒ contents.csvì— content_id/domain ì»¬ëŸ¼ í•„ìš”\")\n",
    "\n",
    "    av   = add_prefix_except_key(safe_read(PATH_AV), \"av_\")\n",
    "    game = add_prefix_except_key(safe_read(PATH_GAME), \"game_\")\n",
    "    web  = add_prefix_except_key(safe_read(PATH_WEBNOVEL), \"webnovel_\")\n",
    "\n",
    "    def typed_left_merge(left, right, key=\"content_id\"):\n",
    "        if right is None or right.empty:\n",
    "            return left.copy()\n",
    "        L = left.copy(); R = right.copy()\n",
    "        if key in L.columns: L[key] = pd.to_numeric(L[key], errors=\"coerce\").astype(\"Int64\")\n",
    "        if key in R.columns: R[key] = pd.to_numeric(R[key], errors=\"coerce\").astype(\"Int64\")\n",
    "        return L.merge(R, on=key, how=\"left\")\n",
    "\n",
    "    parts = []\n",
    "    doms = contents[\"domain\"].dropna().unique().tolist()\n",
    "    if \"AV\" in doms:\n",
    "        sub = contents[contents[\"domain\"]==\"AV\"].copy()\n",
    "        sub = typed_left_merge(sub, av); parts.append(sub)\n",
    "    if \"GAME\" in doms:\n",
    "        sub = contents[contents[\"domain\"]==\"GAME\"].copy()\n",
    "        sub = typed_left_merge(sub, game); parts.append(sub)\n",
    "    if \"WEBNOVEL\" in doms:\n",
    "        sub = contents[contents[\"domain\"]==\"WEBNOVEL\"].copy()\n",
    "        sub = typed_left_merge(sub, web); parts.append(sub)\n",
    "\n",
    "    known = {\"AV\",\"GAME\",\"WEBNOVEL\"}\n",
    "    nodes = pd.concat(parts, ignore_index=True) if parts else contents[contents[\"domain\"].isin(known)].copy()\n",
    "    nodes = nodes.drop_duplicates(subset=[\"content_id\"], keep=\"first\")\n",
    "\n",
    "    # ë³´ê¸° ì¢‹ê²Œ ì •ë ¬\n",
    "    first = [c for c in [\"content_id\",\"domain\",\"master_title\",\"original_title\",\"release_year\",\n",
    "                         \"poster_image_url\",\"created_at\",\"updated_at\",\"synopsis\"] if c in nodes.columns]\n",
    "    nodes = nodes[first + [c for c in nodes.columns if c not in first]]\n",
    "\n",
    "    nodes.to_csv(OUT_NODES, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… graph_nodes ì €ì¥: {OUT_NODES} (rows={len(nodes)})\")\n",
    "    return nodes\n",
    "\n",
    "# 2) ì›ë³¸ ì¥ë¥´ ì„¸ë¡œí‘œ(content_id, source, raw_genre)\n",
    "def build_raw_genres(nodes):\n",
    "    valid_ids = set(pd.to_numeric(nodes[\"content_id\"], errors=\"coerce\").dropna().astype(int))\n",
    "    rows = []\n",
    "\n",
    "    # AV: TMDB ì¥ë¥´ (genres.tmdb_genres.N.name)\n",
    "    av = safe_read(PATH_AV)\n",
    "    if av is not None and not av.empty:\n",
    "        name_cols = [c for c in av.columns if c.startswith(\"genres.tmdb_genres.\") and c.endswith(\".name\")]\n",
    "        if name_cols:\n",
    "            tmp = av.melt(id_vars=[\"content_id\"], value_vars=name_cols, value_name=\"raw_genre\").dropna(subset=[\"raw_genre\"])\n",
    "            tmp[\"raw_genre\"] = tmp[\"raw_genre\"].astype(str).str.strip()\n",
    "            tmp = tmp[(tmp[\"raw_genre\"]!=\"\") & (tmp[\"content_id\"].isin(valid_ids))]\n",
    "            tmp[\"source\"] = \"tmdb\"\n",
    "            rows.append(tmp[[\"content_id\",\"source\",\"raw_genre\"]])\n",
    "\n",
    "    # GAME: Steam ì¥ë¥´ (genres_str: \"A;B;C\")\n",
    "    game = safe_read(PATH_GAME)\n",
    "    if game is not None and not game.empty and \"genres_str\" in game.columns:\n",
    "        g2 = game.dropna(subset=[\"genres_str\"]).copy()\n",
    "        g2[\"raw_genre\"] = g2[\"genres_str\"].astype(str).str.split(r\"\\s*;\\s*\")\n",
    "        g2 = g2.explode(\"raw_genre\").dropna(subset=[\"raw_genre\"])\n",
    "        g2[\"raw_genre\"] = g2[\"raw_genre\"].astype(str).str.strip()\n",
    "        g2 = g2[(g2[\"raw_genre\"]!=\"\") & (g2[\"content_id\"].isin(valid_ids))]\n",
    "        g2[\"source\"] = \"steam\"\n",
    "        rows.append(g2[[\"content_id\",\"source\",\"raw_genre\"]])\n",
    "\n",
    "    # WEBNOVEL: ì¥ë¥´ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ë¶„í•´ (ì˜ˆ: 'genres'ê°€ \"íŒíƒ€ì§€;ë¡œë§¨ìŠ¤\")\n",
    "    web = safe_read(PATH_WEBNOVEL)\n",
    "    if web is not None and not web.empty:\n",
    "        # í›„ë³´ ì»¬ëŸ¼ë“¤ ì¤‘ ì¡´ì¬í•˜ëŠ” ê²ƒ ì„ íƒ\n",
    "        cand_cols = [c for c in [\"genres\",\"genre\",\"genre_str\"] if c in web.columns]\n",
    "        if cand_cols:\n",
    "            col = cand_cols[0]\n",
    "            w2 = web.dropna(subset=[col]).copy()\n",
    "            # êµ¬ë¶„ì: ; , / | (í˜¼ìš© ë°©ì§€ìš© ì •ê·œì‹)\n",
    "            w2[\"raw_genre\"] = w2[col].astype(str).str.split(r\"\\s*[;|/,]\\s*\")\n",
    "            w2 = w2.explode(\"raw_genre\").dropna(subset=[\"raw_genre\"])\n",
    "            w2[\"raw_genre\"] = w2[\"raw_genre\"].astype(str).str.strip()\n",
    "            if \"content_id\" in w2.columns:\n",
    "                w2 = w2[w2[\"raw_genre\"]!=\"\"]\n",
    "                w2 = w2[w2[\"content_id\"].isin(valid_ids)]\n",
    "                w2[\"source\"] = \"webnovel\"\n",
    "                rows.append(w2[[\"content_id\",\"source\",\"raw_genre\"]])\n",
    "\n",
    "    df = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=[\"content_id\",\"source\",\"raw_genre\"])\n",
    "    if not df.empty:\n",
    "        df[\"content_id\"] = pd.to_numeric(df[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df = df[df[\"content_id\"].notna()]\n",
    "        df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    df.to_csv(OUT_RAWGEN, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… content_raw_genres ì €ì¥: {OUT_RAWGEN} (rows={len(df)})\")\n",
    "    return df\n",
    "\n",
    "# 3) ë©”íƒ€ë…¸ë“œ + ì´ë¶„ ì—£ì§€ ìƒì„± (ë„ë©”ì¸/ì¥ë¥´ ëª¨ë‘ 'ì›ë³¸' ê¸°ì¤€)\n",
    "def build_bipartite(nodes, raw_genres):\n",
    "    # ë©”íƒ€ë…¸ë“œ: ë„ë©”ì¸\n",
    "    domain_nodes = sorted(nodes[\"domain\"].dropna().unique().tolist())\n",
    "\n",
    "    meta_rows = []\n",
    "    for d in domain_nodes:\n",
    "        meta_rows.append({\"meta_id\": f\"DOM:{d}\", \"meta_type\": \"domain\", \"label\": d, \"source\": \"\"})\n",
    "\n",
    "    # ë©”íƒ€ë…¸ë“œ: ì¥ë¥´ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì ìš©)\n",
    "    if raw_genres is not None and not raw_genres.empty:\n",
    "        # (source, raw_genre_norm)ë³„ í•˜ë‚˜ì˜ ë©”íƒ€ë…¸ë“œ\n",
    "        rg = raw_genres.copy()\n",
    "        rg[\"raw_genre_norm\"] = rg[\"raw_genre\"].astype(str).str.strip()\n",
    "        rg[\"raw_genre_norm\"] = rg[\"raw_genre_norm\"].replace({\"\": None})\n",
    "        rg = rg.dropna(subset=[\"raw_genre_norm\"])\n",
    "        uniq = rg[[\"source\",\"raw_genre_norm\"]].drop_duplicates()\n",
    "        for src, g in uniq.itertuples(index=False):\n",
    "            meta_id = f\"GEN:{src}:{norm_for_id(g)}\" if g else None\n",
    "            if meta_id:\n",
    "                meta_rows.append({\"meta_id\": meta_id, \"meta_type\": \"genre\", \"label\": g, \"source\": src})\n",
    "\n",
    "    df_meta = pd.DataFrame(meta_rows, columns=[\"meta_id\",\"meta_type\",\"label\",\"source\"]).drop_duplicates()\n",
    "    df_meta.to_csv(OUT_META, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… meta_nodes ì €ì¥: {OUT_META} (rows={len(df_meta)})\")\n",
    "\n",
    "    # ì´ë¶„ ì—£ì§€: ì½˜í…ì¸  â†’ ë„ë©”ì¸\n",
    "    edges = []\n",
    "    sub = nodes[[\"content_id\",\"domain\"]].dropna()\n",
    "    for cid, dom in sub.itertuples(index=False):\n",
    "        edges.append({\n",
    "            \"src_content_id\": int(cid),\n",
    "            \"dst_meta_id\":    f\"DOM:{dom}\",\n",
    "            \"edge_type\":      \"content-domain\",\n",
    "            \"weight\":         DOMAIN_EDGE_WEIGHT\n",
    "        })\n",
    "\n",
    "    # ì´ë¶„ ì—£ì§€: ì½˜í…ì¸  â†’ (sourceë³„ ì›ë³¸ì¥ë¥´)\n",
    "    if raw_genres is not None and not raw_genres.empty:\n",
    "        for cid, src, g in raw_genres[[\"content_id\",\"source\",\"raw_genre\"]].itertuples(index=False):\n",
    "            g_norm = norm_for_id(g)\n",
    "            if not g_norm:\n",
    "                continue\n",
    "            meta_id = f\"GEN:{src}:{g_norm}\"\n",
    "            edges.append({\n",
    "                \"src_content_id\": int(cid),\n",
    "                \"dst_meta_id\":    meta_id,\n",
    "                \"edge_type\":      f\"content-genre-{src}\",\n",
    "                \"weight\":         GENRE_EDGE_WEIGHT\n",
    "            })\n",
    "\n",
    "    df_edges = pd.DataFrame(edges, columns=[\"src_content_id\",\"dst_meta_id\",\"edge_type\",\"weight\"]).drop_duplicates()\n",
    "    df_edges.to_csv(OUT_EDGES_BI, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… graph_edges_bipartite ì €ì¥: {OUT_EDGES_BI} (edges={len(df_edges)})\")\n",
    "    return df_meta, df_edges\n",
    "\n",
    "def main():\n",
    "    nodes = build_nodes()\n",
    "    rawg  = build_raw_genres(nodes)\n",
    "    build_bipartite(nodes, rawg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232b444",
   "metadata": {},
   "source": [
    "# ì´ë¶„ê·¸ë˜í”„ ìƒì„± 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa6704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… graph_nodes ì €ì¥: csv ë°ì´í„°\\clean\\graph_nodes.csv (rows=495)\n",
      "âœ… content_raw_genres (wide) ì €ì¥: csv ë°ì´í„°\\clean\\content_raw_genres.csv (rows=300)\n",
      "âœ… raw_genre_1~3 â†’ ì„¸ë¡œí˜• ë³€í™˜: rows=646\n",
      "âœ… meta_nodes ì €ì¥: csv ë°ì´í„°\\clean\\meta_nodes.csv (rows=48)\n",
      "âœ… graph_edges_bipartite ì €ì¥: csv ë°ì´í„°\\clean\\graph_edges_bipartite.csv (edges=1141)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import Optional, List\n",
    "\n",
    "BASE = r\"csv ë°ì´í„°\\clean\"\n",
    "PATH_CONTENTS = os.path.join(BASE, \"contents.csv\")\n",
    "PATH_AV       = os.path.join(BASE, \"av_contents.csv\")\n",
    "PATH_GAME     = os.path.join(BASE, \"game_contents.csv\")\n",
    "PATH_WEBNOVEL = os.path.join(BASE, \"webnovel_contents.csv\")  # ìˆìœ¼ë©´ ì‚¬ìš©\n",
    "\n",
    "PATH_RAW_ITEM = os.path.join(BASE, \"raw_item.csv\")           # raw_id, genres_str\n",
    "\n",
    "OUT_NODES     = os.path.join(BASE, \"graph_nodes.csv\")              # ì½˜í…ì¸  ë…¸ë“œ\n",
    "OUT_RAWGEN    = os.path.join(BASE, \"content_raw_genres.csv\")       # (ìµœì¢…) content_id, source, raw_genre_1~3\n",
    "OUT_META      = os.path.join(BASE, \"meta_nodes.csv\")               # ë©”íƒ€ë…¸ë“œ ëª©ë¡\n",
    "OUT_EDGES_BI  = os.path.join(BASE, \"graph_edges_bipartite.csv\")    # ì½˜í…ì¸ â†’ë©”íƒ€ ì—£ì§€\n",
    "\n",
    "# ì—£ì§€ ê°€ì¤‘ì¹˜(í•„ìš” ì‹œ ì¡°ì •)\n",
    "DOMAIN_EDGE_WEIGHT = 1.0\n",
    "GENRE_EDGE_WEIGHT  = 1.0\n",
    "\n",
    "ENCODINGS = [\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"]\n",
    "def read_csv_retry(path, **kwargs):\n",
    "    last = None\n",
    "    for enc in ENCODINGS:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kwargs)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "def safe_read(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"âš ï¸ ì—†ìŒ: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        return read_csv_retry(path)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì½ê¸° ì‹¤íŒ¨: {path} -> {e}\")\n",
    "        return None\n",
    "\n",
    "def add_prefix_except_key(df, prefix, key=\"content_id\"):\n",
    "    if df is None: return None\n",
    "    return df.rename(columns={c: (c if c==key else f\"{prefix}{c}\") for c in df.columns})\n",
    "\n",
    "def norm_for_id(s: str) -> str:\n",
    "    \"\"\"\n",
    "    ë©”íƒ€ë…¸ë“œ IDìš© ê°„ë‹¨ ì •ê·œí™” (ê³µë°±/êµ¬ë‘ì  ì œê±°, ì†Œë¬¸ì)\n",
    "    ì˜ˆ: 'Science Fiction' -> 'science_fiction'\n",
    "    \"\"\"\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    # í•œ/ì˜/ìˆ«ì ì™¸ëŠ” ê³µë°± ì¹˜í™˜\n",
    "    s = re.sub(r\"[^\\w\\sê°€-í£]\", \" \", s)\n",
    "    s = \"_\".join(s.split())\n",
    "    return s\n",
    "\n",
    "def parse_genres(genres_str: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    'ë“œë¼ë§ˆ|ë²”ì£„|ë¯¸ìŠ¤í„°ë¦¬' â†’ ['ë“œë¼ë§ˆ','ë²”ì£„','ë¯¸ìŠ¤í„°ë¦¬']\n",
    "    ê³µë°±/ì½¤ë§ˆ/ìŠ¬ë˜ì‹œ/Â· ê°™ì€ ê²ƒë„ êµ¬ë¶„ìë¡œ ì²˜ë¦¬\n",
    "    \"\"\"\n",
    "    if not isinstance(genres_str, str):\n",
    "        return []\n",
    "    genres_str = genres_str.strip()\n",
    "    if not genres_str:\n",
    "        return []\n",
    "    parts = re.split(r\"[|,/Â·;]\", genres_str)\n",
    "    parts = [g.strip() for g in parts if g.strip()]\n",
    "    return parts\n",
    "\n",
    "# 1) ì½˜í…ì¸  ë…¸ë“œ ìƒì„±(ê·¸ëŒ€ë¡œ ìœ ì§€)\n",
    "def build_nodes():\n",
    "    contents = safe_read(PATH_CONTENTS)\n",
    "    if contents is None or contents.empty:\n",
    "        raise RuntimeError(\"âŒ contents.csv ë¹„ì–´ìˆìŒ/ë¡œë“œ ì‹¤íŒ¨\")\n",
    "    if \"content_id\" not in contents.columns or \"domain\" not in contents.columns:\n",
    "        raise RuntimeError(\"âŒ contents.csvì— content_id/domain ì»¬ëŸ¼ í•„ìš”\")\n",
    "\n",
    "    av   = add_prefix_except_key(safe_read(PATH_AV), \"av_\")\n",
    "    game = add_prefix_except_key(safe_read(PATH_GAME), \"game_\")\n",
    "    web  = add_prefix_except_key(safe_read(PATH_WEBNOVEL), \"webnovel_\")\n",
    "\n",
    "    def typed_left_merge(left, right, key=\"content_id\"):\n",
    "        if right is None or right.empty:\n",
    "            return left.copy()\n",
    "        L = left.copy(); R = right.copy()\n",
    "        if key in L.columns: L[key] = pd.to_numeric(L[key], errors=\"coerce\").astype(\"Int64\")\n",
    "        if key in R.columns: R[key] = pd.to_numeric(R[key], errors=\"coerce\").astype(\"Int64\")\n",
    "        return L.merge(R, on=key, how=\"left\")\n",
    "\n",
    "    parts = []\n",
    "    doms = contents[\"domain\"].dropna().unique().tolist()\n",
    "    if \"AV\" in doms:\n",
    "        sub = contents[contents[\"domain\"]==\"AV\"].copy()\n",
    "        sub = typed_left_merge(sub, av); parts.append(sub)\n",
    "    if \"GAME\" in doms:\n",
    "        sub = contents[contents[\"domain\"]==\"GAME\"].copy()\n",
    "        sub = typed_left_merge(sub, game); parts.append(sub)\n",
    "    if \"WEBNOVEL\" in doms:\n",
    "        sub = contents[contents[\"domain\"]==\"WEBNOVEL\"].copy()\n",
    "        sub = typed_left_merge(sub, web); parts.append(sub)\n",
    "\n",
    "    known = {\"AV\",\"GAME\",\"WEBNOVEL\"}\n",
    "    nodes = pd.concat(parts, ignore_index=True) if parts else contents[contents[\"domain\"].isin(known)].copy()\n",
    "    nodes = nodes.drop_duplicates(subset=[\"content_id\"], keep=\"first\")\n",
    "\n",
    "    # ë³´ê¸° ì¢‹ê²Œ ì •ë ¬\n",
    "    first = [c for c in [\"content_id\",\"domain\",\"master_title\",\"original_title\",\"release_year\",\n",
    "                         \"poster_image_url\",\"created_at\",\"updated_at\",\"synopsis\"] if c in nodes.columns]\n",
    "    nodes = nodes[first + [c for c in nodes.columns if c not in first]]\n",
    "\n",
    "    nodes.to_csv(OUT_NODES, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… graph_nodes ì €ì¥: {OUT_NODES} (rows={len(nodes)})\")\n",
    "    return nodes\n",
    "\n",
    "# 2) ì¥ë¥´ í‘œ ìƒì„±\n",
    "#   2-1) ìš°ì„  ì˜ˆì „ì²˜ëŸ¼ ì„¸ë¡œí˜•(content_id, source, raw_genre)ì„ ë§Œë“¤ê³ \n",
    "#   2-2) raw_item.csvì˜ genres_strë¥¼ ì´ìš©í•´ ìµœëŒ€ 3ê°œ ì¥ë¥´ë¡œ ê°€ë¡œí˜•(raw_genre_1~3)ìœ¼ë¡œ ë³€í™˜\n",
    "def build_raw_genres(nodes):\n",
    "    valid_ids = set(pd.to_numeric(nodes[\"content_id\"], errors=\"coerce\").dropna().astype(int))\n",
    "    rows = []\n",
    "\n",
    "    # ----- AV: TMDB ì¥ë¥´ (genres.tmdb_genres.N.name) -----\n",
    "    av = safe_read(PATH_AV)\n",
    "    if av is not None and not av.empty:\n",
    "        av = av.copy()\n",
    "        av.columns = [c.strip() for c in av.columns]\n",
    "        name_cols = [c for c in av.columns if c.startswith(\"genres.tmdb_genres.\") and c.endswith(\".name\")]\n",
    "        if name_cols and \"content_id\" in av.columns:\n",
    "            tmp = av.melt(id_vars=[\"content_id\"], value_vars=name_cols, value_name=\"raw_genre\").dropna(subset=[\"raw_genre\"])\n",
    "            tmp[\"raw_genre\"] = tmp[\"raw_genre\"].astype(str).str.strip()\n",
    "            tmp = tmp[(tmp[\"raw_genre\"]!=\"\") & (tmp[\"content_id\"].isin(valid_ids))]\n",
    "            tmp[\"source\"] = \"tmdb\"\n",
    "            rows.append(tmp[[\"content_id\",\"source\",\"raw_genre\"]])\n",
    "\n",
    "    # ----- GAME: Steam ì¥ë¥´ (genres_str: \"A;B;C\") -----\n",
    "    game = safe_read(PATH_GAME)\n",
    "    if game is not None and not game.empty:\n",
    "        game = game.copy()\n",
    "        game.columns = [c.strip() for c in game.columns]\n",
    "        if \"genres_str\" in game.columns and \"content_id\" in game.columns:\n",
    "            g2 = game.dropna(subset=[\"genres_str\"]).copy()\n",
    "            g2[\"raw_genre\"] = g2[\"genres_str\"].astype(str).str.split(r\"\\s*;\\s*\")\n",
    "            g2 = g2.explode(\"raw_genre\").dropna(subset=[\"raw_genre\"])\n",
    "            g2[\"raw_genre\"] = g2[\"raw_genre\"].astype(str).str.strip()\n",
    "            g2 = g2[(g2[\"raw_genre\"]!=\"\") & (g2[\"content_id\"].isin(valid_ids))]\n",
    "            g2[\"source\"] = \"steam\"\n",
    "            rows.append(g2[[\"content_id\",\"source\",\"raw_genre\"]])\n",
    "\n",
    "    # ----- WEBNOVEL: ì¥ë¥´ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ë¶„í•´ (ì˜ˆ: 'genres'ê°€ \"íŒíƒ€ì§€;ë¡œë§¨ìŠ¤\") -----\n",
    "    web = safe_read(PATH_WEBNOVEL)\n",
    "    if web is not None and not web.empty:\n",
    "        web = web.copy()\n",
    "        web.columns = [c.strip() for c in web.columns]\n",
    "        cand_cols = [c for c in [\"genres\",\"genre\",\"genre_str\"] if c in web.columns]\n",
    "        if cand_cols:\n",
    "            col = cand_cols[0]\n",
    "            w2 = web.dropna(subset=[col]).copy()\n",
    "            w2[\"raw_genre\"] = w2[col].astype(str).str.split(r\"\\s*[;|/,]\\s*\")\n",
    "            w2 = w2.explode(\"raw_genre\").dropna(subset=[\"raw_genre\"])\n",
    "            w2[\"raw_genre\"] = w2[\"raw_genre\"].astype(str).str.strip()\n",
    "            if \"content_id\" in w2.columns:\n",
    "                w2 = w2[w2[\"raw_genre\"]!=\"\"]\n",
    "                w2 = w2[w2[\"content_id\"].isin(valid_ids)]\n",
    "                w2[\"source\"] = \"webnovel\"\n",
    "                rows.append(w2[[\"content_id\",\"source\",\"raw_genre\"]])\n",
    "\n",
    "    # ----- 2-1) tall: content_id, source, raw_genre -----\n",
    "    if rows:\n",
    "        cg = pd.concat(rows, ignore_index=True)\n",
    "        cg[\"content_id\"] = pd.to_numeric(cg[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        cg = cg[cg[\"content_id\"].notna()]\n",
    "        cg[\"content_id\"] = cg[\"content_id\"].astype(int)\n",
    "        cg[\"raw_genre\"] = cg[\"raw_genre\"].astype(str).str.strip()\n",
    "        cg = cg[cg[\"raw_genre\"]!=\"\"]\n",
    "        cg = cg.drop_duplicates().reset_index(drop=True)\n",
    "    else:\n",
    "        cg = pd.DataFrame(columns=[\"content_id\",\"source\",\"raw_genre\"])\n",
    "\n",
    "    # ----- 2-2) raw_item.csv ì˜ genres_str ë¥¼ ì´ìš©í•´ raw_genre_1~3 ìƒì„± -----\n",
    "    ri = safe_read(PATH_RAW_ITEM)\n",
    "    if ri is not None and not ri.empty:\n",
    "        ri = ri.copy()\n",
    "        ri.columns = [c.strip() for c in ri.columns]\n",
    "\n",
    "        needed_ri = [\"raw_id\", \"genres_str\"]\n",
    "        miss_ri = [c for c in needed_ri if c not in ri.columns]\n",
    "        if miss_ri:\n",
    "            print(\"âš ï¸ raw_item.csv ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ ì—†ì–´ genres_strë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\", \", \".join(miss_ri))\n",
    "            ri = None\n",
    "        else:\n",
    "            ri[\"raw_id\"] = pd.to_numeric(ri[\"raw_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "            ri = ri[ri[\"raw_id\"].notna()].copy()\n",
    "            ri[\"raw_id\"] = ri[\"raw_id\"].astype(int)\n",
    "    else:\n",
    "        ri = None\n",
    "\n",
    "    if ri is not None and not cg.empty:\n",
    "        merged = cg.merge(\n",
    "            ri[[\"raw_id\", \"genres_str\"]],\n",
    "            left_on=\"content_id\",\n",
    "            right_on=\"raw_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    else:\n",
    "        merged = cg.copy()\n",
    "        merged[\"genres_str\"] = \"\"\n",
    "\n",
    "    # raw_genre_1,2,3 ìƒì„±\n",
    "    new_rows = []\n",
    "    for _, row in merged.iterrows():\n",
    "        cid    = int(row[\"content_id\"])\n",
    "        source = str(row[\"source\"]).strip()\n",
    "        base_genre = str(row[\"raw_genre\"]).strip()\n",
    "\n",
    "        genres_from_str = parse_genres(row.get(\"genres_str\", \"\"))\n",
    "\n",
    "        genres = list(genres_from_str)\n",
    "        # ê¸°ì¡´ raw_genreê°€ ìˆê³  ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ë§¨ ì•ì— ì¶”ê°€\n",
    "        if base_genre and base_genre not in genres:\n",
    "            genres.insert(0, base_genre)\n",
    "\n",
    "        g1 = genres[0] if len(genres) > 0 else \"\"\n",
    "        g2 = genres[1] if len(genres) > 1 else \"\"\n",
    "        g3 = genres[2] if len(genres) > 2 else \"\"\n",
    "\n",
    "        new_rows.append((cid, source, g1, g2, g3))\n",
    "\n",
    "    wide = pd.DataFrame(new_rows, columns=[\"content_id\",\"source\",\"raw_genre_1\",\"raw_genre_2\",\"raw_genre_3\"])\n",
    "    wide = wide.drop_duplicates(subset=[\"content_id\",\"source\",\"raw_genre_1\",\"raw_genre_2\",\"raw_genre_3\"])\n",
    "\n",
    "    wide.to_csv(OUT_RAWGEN, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… content_raw_genres (wide) ì €ì¥: {OUT_RAWGEN} (rows={len(wide)})\")\n",
    "    return wide\n",
    "\n",
    "# wide(raw_genre_1~3) â†’ tall(raw_genre) ë³€í™˜ (ê·¸ë˜í”„ ë‚´ë¶€ìš©)\n",
    "def to_tall_raw_genres(raw_genres: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = raw_genres.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # ì´ë¯¸ raw_genre ë‹¨ì¼ ì»¬ëŸ¼ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "    if \"raw_genre\" in df.columns and not any(\n",
    "        c in df.columns for c in [\"raw_genre_1\",\"raw_genre_2\",\"raw_genre_3\"]\n",
    "    ):\n",
    "        df[\"content_id\"] = pd.to_numeric(df[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df = df[df[\"content_id\"].notna()].copy()\n",
    "        df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "        df[\"raw_genre\"] = df[\"raw_genre\"].astype(str).str.strip()\n",
    "        df = df[df[\"raw_genre\"]!=\"\"]\n",
    "        return df[[\"content_id\",\"source\",\"raw_genre\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # ê°€ë¡œí˜• raw_genre_1~3 â†’ ì„¸ë¡œí˜•\n",
    "    genre_cols = [c for c in [\"raw_genre_1\",\"raw_genre_2\",\"raw_genre_3\"] if c in df.columns]\n",
    "    if not genre_cols:\n",
    "        raise RuntimeError(\"raw_genresì— raw_genre ë˜ëŠ” raw_genre_1/2/3 ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    df[\"content_id\"] = pd.to_numeric(df[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df[df[\"content_id\"].notna()].copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    tall = df.melt(\n",
    "        id_vars=[\"content_id\",\"source\"],\n",
    "        value_vars=genre_cols,\n",
    "        value_name=\"raw_genre\"\n",
    "    )\n",
    "    tall[\"raw_genre\"] = tall[\"raw_genre\"].astype(str).str.strip()\n",
    "    tall = tall[tall[\"raw_genre\"].notna() & (tall[\"raw_genre\"]!=\"\")]\n",
    "    tall = tall[[\"content_id\",\"source\",\"raw_genre\"]].drop_duplicates()\n",
    "\n",
    "    print(f\"âœ… raw_genre_1~3 â†’ ì„¸ë¡œí˜• ë³€í™˜: rows={len(tall)}\")\n",
    "    return tall.reset_index(drop=True)\n",
    "\n",
    "# 3) ë©”íƒ€ë…¸ë“œ + ì´ë¶„ ì—£ì§€ ìƒì„±\n",
    "def build_bipartite(nodes, raw_genres):\n",
    "    # ë¨¼ì € wide(raw_genre_1~3)ë¥¼ tall(raw_genre)ë¡œ ë³€í™˜\n",
    "    tall = to_tall_raw_genres(raw_genres) if raw_genres is not None and not raw_genres.empty else None\n",
    "\n",
    "    # ë©”íƒ€ë…¸ë“œ: ë„ë©”ì¸\n",
    "    domain_nodes = sorted(nodes[\"domain\"].dropna().unique().tolist())\n",
    "\n",
    "    meta_rows = []\n",
    "    for d in domain_nodes:\n",
    "        meta_rows.append({\"meta_id\": f\"DOM:{d}\", \"meta_type\": \"domain\", \"label\": d, \"source\": \"\"})\n",
    "\n",
    "    # ë©”íƒ€ë…¸ë“œ: ì¥ë¥´ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì ìš©)\n",
    "    if tall is not None and not tall.empty:\n",
    "        rg = tall.copy()\n",
    "        rg[\"raw_genre_norm\"] = rg[\"raw_genre\"].astype(str).str.strip()\n",
    "        rg[\"raw_genre_norm\"] = rg[\"raw_genre_norm\"].replace({\"\": None})\n",
    "        rg = rg.dropna(subset=[\"raw_genre_norm\"])\n",
    "        uniq = rg[[\"source\",\"raw_genre_norm\"]].drop_duplicates()\n",
    "        for src, g in uniq.itertuples(index=False):\n",
    "            meta_id = f\"GEN:{src}:{norm_for_id(g)}\" if g else None\n",
    "            if meta_id:\n",
    "                meta_rows.append({\"meta_id\": meta_id, \"meta_type\": \"genre\", \"label\": g, \"source\": src})\n",
    "\n",
    "    df_meta = pd.DataFrame(meta_rows, columns=[\"meta_id\",\"meta_type\",\"label\",\"source\"]).drop_duplicates()\n",
    "    df_meta.to_csv(OUT_META, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… meta_nodes ì €ì¥: {OUT_META} (rows={len(df_meta)})\")\n",
    "\n",
    "    # ì´ë¶„ ì—£ì§€: ì½˜í…ì¸  â†’ ë„ë©”ì¸\n",
    "    edges = []\n",
    "    sub = nodes[[\"content_id\",\"domain\"]].dropna()\n",
    "    for cid, dom in sub.itertuples(index=False):\n",
    "        edges.append({\n",
    "            \"src_content_id\": int(cid),\n",
    "            \"dst_meta_id\":    f\"DOM:{dom}\",\n",
    "            \"edge_type\":      \"content-domain\",\n",
    "            \"weight\":         DOMAIN_EDGE_WEIGHT\n",
    "        })\n",
    "\n",
    "    # ì´ë¶„ ì—£ì§€: ì½˜í…ì¸  â†’ ì¥ë¥´ (í•œ ì½˜í…ì¸ ê°€ ì—¬ëŸ¬ ì¥ë¥´ì™€ ì—°ê²°ë¨)\n",
    "    if tall is not None and not tall.empty:\n",
    "        for cid, src, g in tall[[\"content_id\",\"source\",\"raw_genre\"]].itertuples(index=False):\n",
    "            g_norm = norm_for_id(g)\n",
    "            if not g_norm:\n",
    "                continue\n",
    "            meta_id = f\"GEN:{src}:{g_norm}\"\n",
    "            edges.append({\n",
    "                \"src_content_id\": int(cid),\n",
    "                \"dst_meta_id\":    meta_id,\n",
    "                \"edge_type\":      f\"content-genre-{src}\",\n",
    "                \"weight\":         GENRE_EDGE_WEIGHT\n",
    "            })\n",
    "\n",
    "    df_edges = pd.DataFrame(edges, columns=[\"src_content_id\",\"dst_meta_id\",\"edge_type\",\"weight\"]).drop_duplicates()\n",
    "    df_edges.to_csv(OUT_EDGES_BI, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… graph_edges_bipartite ì €ì¥: {OUT_EDGES_BI} (edges={len(df_edges)})\")\n",
    "    return df_meta, df_edges\n",
    "\n",
    "def main():\n",
    "    nodes = build_nodes()\n",
    "    rawg  = build_raw_genres(nodes)  # ì—¬ê¸°ì„œ content_raw_genres.csv (raw_genre_1~3) ìƒì„±\n",
    "    build_bipartite(nodes, rawg)    # ì—¬ê¸°ì„œ í•œ ì½˜í…ì¸ ê°€ ì—¬ëŸ¬ ì¥ë¥´ ë…¸ë“œì™€ ì—£ì§€ë¡œ ì—°ê²°ë¨\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b35add",
   "metadata": {},
   "source": [
    "# ì•„ì´í…œ-ì•„ì´í…œ ê·¸ë˜í”„ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04945d44",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['raw_genre']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34696\\2327218247.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[0mdf_edges\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOUT_EDGES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8-sig\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"âœ… ì €ì¥: {OUT_EDGES} (edges={len(df_edges):,}, items={len(all_items):,})\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34696\\2327218247.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mmembers_of_meta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"DOM:{dom}\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# (b) 'ë„ë©”ì¸ë³„ ì›ë³¸ ì¥ë¥´' ë©”íƒ€ë…¸ë“œ: GEN:<source>:<raw_genre>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrawg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mrawg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrawg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content_id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"source\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"raw_genre\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mrawg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content_id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"coerce\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Int64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mrawg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrawg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrawg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mrawg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content_id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrawg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\LG\\anaconda3\\envs\\anaconda\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6666\u001b[0m             \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6667\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6668\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6669\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6670\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6671\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6673\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ['raw_genre']"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE = r\"csv ë°ì´í„°\\clean\"\n",
    "PATH_NODES   = os.path.join(BASE, \"graph_nodes.csv\")\n",
    "PATH_RAWGEN  = os.path.join(BASE, \"content_raw_genres.csv\")\n",
    "OUT_EDGES    = os.path.join(BASE, \"graph_edges_item_item.csv\")\n",
    "\n",
    "# ========= í•˜ì´í¼íŒŒë¼ë¯¸í„°(ì¡°ì ˆ í¬ì¸íŠ¸) =========\n",
    "DOMAIN_BASE_WEIGHT    = 1.0   # ê°™ì€ ë„ë©”ì¸ ê³µìœ  ê¸°ì—¬ì¹˜\n",
    "GENRE_BASE_WEIGHT     = 1.0   # ê°™ì€ 'ë„ë©”ì¸ë³„ ì›ë³¸ì¥ë¥´' ê³µìœ  ê¸°ì—¬ì¹˜\n",
    "ALPHA                 = 0.7   # í—ˆë¸Œ ì™„í™” ì§€ìˆ˜(0=ì™„í™” ì—†ìŒ, 1=ê°•í•œ ì™„í™”). ë³´í†µ 0.5~0.8\n",
    "USE_IDF               = True  # ë©”íƒ€ë…¸ë“œ ì •ë³´ëŸ‰ ê°€ì¤‘ì¹˜(log(1+N/deg)) ì‚¬ìš©\n",
    "TOPK_PER_ITEM         = 100   # ì•„ì´í…œë‹¹ ìƒìœ„ K ì´ì›ƒë§Œ ìœ ì§€ (í¬ì†Œí™”)\n",
    "MAX_MEMBERS_PER_META  = 2000  # ë©”íƒ€ë…¸ë“œ(ë„ë©”ì¸/ì¥ë¥´) ë©¤ë²„ê°€ ë„ˆë¬´ í´ ë•Œ ìƒ˜í”Œë§ ìƒí•œ (ë©”ëª¨ë¦¬/ì‹œê°„ ë³´í˜¸)\n",
    "\n",
    "ENCODINGS = [\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"]\n",
    "def read_csv_retry(path, **kwargs):\n",
    "    last = None\n",
    "    for enc in ENCODINGS:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kwargs)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "def main():\n",
    "    # 1) ë¡œë“œ\n",
    "    nodes = read_csv_retry(PATH_NODES)\n",
    "    rawg  = read_csv_retry(PATH_RAWGEN) if os.path.exists(PATH_RAWGEN) else pd.DataFrame(columns=[\"content_id\",\"source\",\"raw_genre\"])\n",
    "\n",
    "    if nodes.empty:\n",
    "        raise RuntimeError(\"graph_nodes.csv ë¹„ì–´ìˆìŒ\")\n",
    "    if \"content_id\" not in nodes.columns or \"domain\" not in nodes.columns:\n",
    "        raise RuntimeError(\"graph_nodes.csvì— content_id/domain í•„ìš”\")\n",
    "\n",
    "    nodes = nodes.dropna(subset=[\"content_id\",\"domain\"]).copy()\n",
    "    nodes[\"content_id\"] = pd.to_numeric(nodes[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    nodes = nodes[nodes[\"content_id\"].notna()]\n",
    "    nodes[\"content_id\"] = nodes[\"content_id\"].astype(int)\n",
    "\n",
    "    # ë„ë©”ì¸ ë§µ\n",
    "    domain_of = dict(nodes[[\"content_id\",\"domain\"]].values)\n",
    "    all_items = set(domain_of.keys())\n",
    "    N_items   = max(1, len(all_items))\n",
    "\n",
    "    # 2) ë©”íƒ€ë…¸ë“œ ë©¤ë²„ êµ¬ì„±\n",
    "    # (a) ë„ë©”ì¸ ë©”íƒ€ë…¸ë“œ: DOM:<domain>\n",
    "    members_of_meta = defaultdict(list)\n",
    "    for cid, dom in domain_of.items():\n",
    "        members_of_meta[f\"DOM:{dom}\"].append(cid)\n",
    "\n",
    "    # (b) 'ë„ë©”ì¸ë³„ ì›ë³¸ ì¥ë¥´' ë©”íƒ€ë…¸ë“œ: GEN:<source>:<raw_genre>\n",
    "    if not rawg.empty:\n",
    "        rawg = rawg.dropna(subset=[\"content_id\",\"source\",\"raw_genre\"]).copy()\n",
    "        rawg[\"content_id\"] = pd.to_numeric(rawg[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        rawg = rawg[rawg[\"content_id\"].notna()]\n",
    "        rawg[\"content_id\"] = rawg[\"content_id\"].astype(int)\n",
    "        # ê°™ì€ ë„ë©”ì¸ì—ì„œë§Œ ì˜ë¯¸ë¥¼ ì£¼ë ¤ë©´, ì•„ì´í…œì˜ ë„ë©”ì¸ì„ í™•ì¸í•´ì„œ ë¬¶ê¸°ë§Œ í•˜ë©´ ë¨.\n",
    "        # ì—¬ê¸°ì„  ë©”íƒ€ë…¸ë“œ IDì— ë„ë©”ì¸ê¹Œì§€ ë„£ì–´ 'ë„ë©”ì¸ë³„ ì¥ë¥´'ë¡œ ë¶„ë¦¬í•œë‹¤.\n",
    "        # ì˜ˆ) GEN:tmdb:ê³µí¬@AV, GEN:steam:Action@GAME\n",
    "        for cid, src, g in rawg[[\"content_id\",\"source\",\"raw_genre\"]].drop_duplicates().itertuples(index=False):\n",
    "            dom = domain_of.get(cid)\n",
    "            if not dom: \n",
    "                continue\n",
    "            meta_id = f\"GEN:{src}:{g}@{dom}\"\n",
    "            members_of_meta[meta_id].append(cid)\n",
    "\n",
    "    # 3) íˆ¬ì˜(ì•„ì´í…œâ†”ì•„ì´í…œ): ë©”íƒ€ë…¸ë“œ ê³µë™ì†Œì† ê¸°ë°˜ ê°€ì¤‘ì¹˜\n",
    "    def meta_contrib(meta_id, deg):\n",
    "        # deg = ë©”íƒ€ë…¸ë“œ ë©¤ë²„ ìˆ˜\n",
    "        base = DOMAIN_BASE_WEIGHT if meta_id.startswith(\"DOM:\") else GENRE_BASE_WEIGHT\n",
    "        if deg <= 1:\n",
    "            return 0.0\n",
    "        # í—ˆë¸Œ ì™„í™”: 1 / (deg^ALPHA)\n",
    "        w = base / (deg ** ALPHA)\n",
    "        # ì •ë³´ëŸ‰(IDF) ë³´ì •: log(1 + N / deg)\n",
    "        if USE_IDF:\n",
    "            w *= math.log(1.0 + (N_items / float(deg)))\n",
    "        return w\n",
    "\n",
    "    edges = defaultdict(float)\n",
    "\n",
    "    for meta_id, members in members_of_meta.items():\n",
    "        members = list(set(members))\n",
    "        deg = len(members)\n",
    "        if deg <= 1:\n",
    "            continue\n",
    "\n",
    "        # ë„ˆë¬´ í° ë©”íƒ€ë…¸ë“œ(ì˜ˆ: íŠ¹ì • ë„ë©”ì¸ ì „ì²´)ê°€ í­ë°œí•˜ì§€ ì•Šê²Œ ìƒ˜í”Œë§/ìƒí•œ\n",
    "        if deg > MAX_MEMBERS_PER_META:\n",
    "            # ê· ì¼ ìƒ˜í”Œë§ (ì¬í˜„ì„±ì€ ë³´ì¥X. í•„ìš”ì‹œ random.seed() ì¶”ê°€)\n",
    "            members = members[:MAX_MEMBERS_PER_META]\n",
    "            deg = len(members)\n",
    "\n",
    "        contrib = meta_contrib(meta_id, deg)\n",
    "        if contrib == 0.0:\n",
    "            continue\n",
    "\n",
    "        # ê·¼ë¦° ì œí•œ ë°©ì‹: ê° ë©¤ë²„ë§ˆë‹¤ ì•ìª½ Kê°œì™€ë§Œ ì—°ê²° (ì „ìŒ O(n^2) ë°©ì§€)\n",
    "        K = min(TOPK_PER_ITEM, deg - 1)\n",
    "        members.sort()\n",
    "        for i, a in enumerate(members):\n",
    "            # ë„ë©”ì¸ ë‹¤ë¥¸ ìŒì€ ì œì™¸ (ë„ë©”ì¸ ì¼ì¹˜ ê°•ì œ) â€” ë„ë©”ì¸ë³„ ì¥ë¥´ ë©”íƒ€ëŠ” ì´ë¯¸ ë¶„ë¦¬ë˜ì–´ ìˆì§€ë§Œ,\n",
    "            # ë„ë©”ì¸ ë©”íƒ€ì—ì„œë„ ì•ˆì „í•˜ê²Œ ë™ì¼ ë„ë©”ì¸ë§Œ ìœ ì§€\n",
    "            for b in members[i+1 : i+1+K]:\n",
    "                if domain_of.get(a) != domain_of.get(b):\n",
    "                    continue\n",
    "                if a > b:\n",
    "                    a, b = b, a\n",
    "                edges[(a,b)] += contrib\n",
    "\n",
    "    # 4) ì•„ì´í…œë‹¹ TOPK ì´ì›ƒë§Œ ìœ ì§€(ì–‘ë°©í–¥ ê¸°ì¤€ ìµœëŒ€ ê°€ì¤‘ì¹˜ ìœ ì§€)\n",
    "    nbrs = defaultdict(list)\n",
    "    for (a,b), w in edges.items():\n",
    "        nbrs[a].append((b,w))\n",
    "        nbrs[b].append((a,w))\n",
    "\n",
    "    pruned = {}\n",
    "    for a, lst in nbrs.items():\n",
    "        lst.sort(key=lambda x: x[1], reverse=True)\n",
    "        for b, w in lst[:TOPK_PER_ITEM]:\n",
    "            key = (a,b) if a < b else (b,a)\n",
    "            if key not in pruned or pruned[key] < w:\n",
    "                pruned[key] = w\n",
    "\n",
    "    df_edges = pd.DataFrame(\n",
    "        [(a,b,w) for (a,b), w in pruned.items()],\n",
    "        columns=[\"src_content_id\",\"dst_content_id\",\"weight\"]\n",
    "    )\n",
    "    df_edges.to_csv(OUT_EDGES, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì €ì¥: {OUT_EDGES} (edges={len(df_edges):,}, items={len(all_items):,})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65717ea",
   "metadata": {},
   "source": [
    "# ì•„ì´í…œ ê·¸ë˜í”„ ìƒì„±1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1957aaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… raw_genre_1~3 â†’ ì„¸ë¡œí˜• ë³€í™˜: rows=801\n",
      "âœ… ì €ì¥: csv ë°ì´í„°\\clean\\graph_edges_item_item.csv (edges=31,148, items=495)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE = r\"csv ë°ì´í„°\\clean\"\n",
    "PATH_NODES   = os.path.join(BASE, \"graph_nodes.csv\")\n",
    "PATH_RAWGEN  = os.path.join(BASE, \"content_raw_genres.csv\")\n",
    "OUT_EDGES    = os.path.join(BASE, \"graph_edges_item_item.csv\")\n",
    "\n",
    "# ========= í•˜ì´í¼íŒŒë¼ë¯¸í„°(ì¡°ì ˆ í¬ì¸íŠ¸) =========\n",
    "DOMAIN_BASE_WEIGHT    = 1.0   # ê°™ì€ ë„ë©”ì¸ ê³µìœ  ê¸°ì—¬ì¹˜\n",
    "GENRE_BASE_WEIGHT     = 1.0   # ê°™ì€ 'ë„ë©”ì¸ë³„ ì›ë³¸ì¥ë¥´' ê³µìœ  ê¸°ì—¬ì¹˜\n",
    "ALPHA                 = 0.7   # í—ˆë¸Œ ì™„í™” ì§€ìˆ˜(0=ì™„í™” ì—†ìŒ, 1=ê°•í•œ ì™„í™”). ë³´í†µ 0.5~0.8\n",
    "USE_IDF               = True  # ë©”íƒ€ë…¸ë“œ ì •ë³´ëŸ‰ ê°€ì¤‘ì¹˜(log(1+N/deg)) ì‚¬ìš©\n",
    "TOPK_PER_ITEM         = 100   # ì•„ì´í…œë‹¹ ìƒìœ„ K ì´ì›ƒë§Œ ìœ ì§€ (í¬ì†Œí™”)\n",
    "MAX_MEMBERS_PER_META  = 2000  # ë©”íƒ€ë…¸ë“œ ë©¤ë²„ê°€ ë„ˆë¬´ í´ ë•Œ ìƒ˜í”Œë§ ìƒí•œ (ë©”ëª¨ë¦¬/ì‹œê°„ ë³´í˜¸)\n",
    "\n",
    "ENCODINGS = [\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"]\n",
    "\n",
    "def read_csv_retry(path, **kwargs):\n",
    "    last = None\n",
    "    for enc in ENCODINGS:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kwargs)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "def to_tall_raw_genres(rawg: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    content_raw_genres.csvê°€\n",
    "      - (A) ì„¸ë¡œí˜•: content_id, source, raw_genre\n",
    "      - (B) ê°€ë¡œí˜•: content_id, source, raw_genre_1~3\n",
    "    ë‘˜ ì¤‘ ì–´ëŠ í¬ë§·ì´ë“  ë°›ì„ ìˆ˜ ìˆê²Œ í•˜ê³ ,\n",
    "    ìµœì¢…ì ìœ¼ë¡œ í•­ìƒ ì„¸ë¡œí˜• (content_id, source, raw_genre) ë¡œ ë³€í™˜.\n",
    "    \"\"\"\n",
    "    df = rawg.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # (A) ì´ë¯¸ raw_genre ë‹¨ì¼ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì •ë¦¬í•´ì„œ ë°˜í™˜\n",
    "    if \"raw_genre\" in df.columns:\n",
    "        if \"content_id\" not in df.columns or \"source\" not in df.columns:\n",
    "            raise RuntimeError(\"raw_genresì— content_id/source/raw_genre ì»¬ëŸ¼ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        df[\"content_id\"] = pd.to_numeric(df[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df = df[df[\"content_id\"].notna()].copy()\n",
    "        df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "        df[\"raw_genre\"] = df[\"raw_genre\"].astype(str).str.strip()\n",
    "        df = df[df[\"raw_genre\"] != \"\"]\n",
    "        return df[[\"content_id\", \"source\", \"raw_genre\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # (B) ê°€ë¡œí˜•(raw_genre_1~3)ì„ ì„¸ë¡œí˜•ìœ¼ë¡œ ë³€í™˜\n",
    "    genre_cols = [c for c in [\"raw_genre_1\", \"raw_genre_2\", \"raw_genre_3\"] if c in df.columns]\n",
    "    if not genre_cols:\n",
    "        raise RuntimeError(\"content_raw_genres.csvì— raw_genre ë˜ëŠ” raw_genre_1/2/3 ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    if \"content_id\" not in df.columns or \"source\" not in df.columns:\n",
    "        raise RuntimeError(\"content_raw_genres.csvì— content_id/source ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    df[\"content_id\"] = pd.to_numeric(df[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df[df[\"content_id\"].notna()].copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    tall = df.melt(\n",
    "        id_vars=[\"content_id\", \"source\"],\n",
    "        value_vars=genre_cols,\n",
    "        value_name=\"raw_genre\"\n",
    "    )\n",
    "    tall[\"raw_genre\"] = tall[\"raw_genre\"].astype(str).str.strip()\n",
    "    tall = tall[tall[\"raw_genre\"].notna() & (tall[\"raw_genre\"] != \"\")]\n",
    "    tall = tall[[\"content_id\", \"source\", \"raw_genre\"]].drop_duplicates()\n",
    "\n",
    "    print(f\"âœ… raw_genre_1~3 â†’ ì„¸ë¡œí˜• ë³€í™˜: rows={len(tall)}\")\n",
    "    return tall.reset_index(drop=True)\n",
    "\n",
    "def main():\n",
    "    # 1) ë…¸ë“œ ë¡œë“œ\n",
    "    nodes = read_csv_retry(PATH_NODES)\n",
    "    if nodes is None or nodes.empty:\n",
    "        raise RuntimeError(\"graph_nodes.csv ë¹„ì–´ìˆìŒ/ë¡œë“œ ì‹¤íŒ¨\")\n",
    "    if \"content_id\" not in nodes.columns or \"domain\" not in nodes.columns:\n",
    "        raise RuntimeError(\"graph_nodes.csvì— content_id/domain ì»¬ëŸ¼ í•„ìš”\")\n",
    "\n",
    "    nodes = nodes.dropna(subset=[\"content_id\", \"domain\"]).copy()\n",
    "    nodes[\"content_id\"] = pd.to_numeric(nodes[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    nodes = nodes[nodes[\"content_id\"].notna()]\n",
    "    nodes[\"content_id\"] = nodes[\"content_id\"].astype(int)\n",
    "\n",
    "    # ë„ë©”ì¸ ë§µ\n",
    "    domain_of = dict(nodes[[\"content_id\", \"domain\"]].values)\n",
    "    all_items = set(domain_of.keys())\n",
    "    N_items   = max(1, len(all_items))\n",
    "\n",
    "    # 2) content_raw_genres ë¡œë“œ (wide â†’ tall ë³€í™˜ í¬í•¨)\n",
    "    if os.path.exists(PATH_RAWGEN):\n",
    "        rawg_raw = read_csv_retry(PATH_RAWGEN)\n",
    "        if rawg_raw is None or rawg_raw.empty:\n",
    "            rawg = pd.DataFrame(columns=[\"content_id\", \"source\", \"raw_genre\"])\n",
    "        else:\n",
    "            rawg = to_tall_raw_genres(rawg_raw)\n",
    "    else:\n",
    "        print(\"âš ï¸ content_raw_genres.csv ì—†ìŒ â†’ ì¥ë¥´ ê¸°ë°˜ ê¸°ì—¬ ì—†ì´ ë„ë©”ì¸ë§Œ ì‚¬ìš©\")\n",
    "        rawg = pd.DataFrame(columns=[\"content_id\", \"source\", \"raw_genre\"])\n",
    "\n",
    "    # 3) ë©”íƒ€ë…¸ë“œ ë©¤ë²„ êµ¬ì„±\n",
    "    members_of_meta = defaultdict(list)\n",
    "\n",
    "    # (a) ë„ë©”ì¸ ë©”íƒ€ë…¸ë“œ: DOM:<domain>\n",
    "    for cid, dom in domain_of.items():\n",
    "        members_of_meta[f\"DOM:{dom}\"].append(cid)\n",
    "\n",
    "    # (b) ë„ë©”ì¸ë³„ ì¥ë¥´ ë©”íƒ€ë…¸ë“œ: GEN:<source>:<raw_genre>@<domain>\n",
    "    if not rawg.empty:\n",
    "        rawg = rawg.dropna(subset=[\"content_id\", \"source\", \"raw_genre\"]).copy()\n",
    "        rawg[\"content_id\"] = pd.to_numeric(rawg[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        rawg = rawg[rawg[\"content_id\"].notna()]\n",
    "        rawg[\"content_id\"] = rawg[\"content_id\"].astype(int)\n",
    "\n",
    "        for cid, src, g in rawg[[\"content_id\", \"source\", \"raw_genre\"]].drop_duplicates().itertuples(index=False):\n",
    "            dom = domain_of.get(cid)\n",
    "            if not dom:\n",
    "                continue\n",
    "            meta_id = f\"GEN:{src}:{g}@{dom}\"\n",
    "            members_of_meta[meta_id].append(cid)\n",
    "\n",
    "    # 4) ë©”íƒ€ë…¸ë“œ ê³µë™ì†Œì† ê¸°ë°˜ ì•„ì´í…œâ†”ì•„ì´í…œ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "    def meta_contrib(meta_id, deg):\n",
    "        # deg = ë©”íƒ€ë…¸ë“œ ë©¤ë²„ ìˆ˜\n",
    "        base = DOMAIN_BASE_WEIGHT if meta_id.startswith(\"DOM:\") else GENRE_BASE_WEIGHT\n",
    "        if deg <= 1:\n",
    "            return 0.0\n",
    "        # í—ˆë¸Œ ì™„í™”: 1 / (deg^ALPHA)\n",
    "        w = base / (deg ** ALPHA)\n",
    "        # ì •ë³´ëŸ‰(IDF) ë³´ì •: log(1 + N / deg)\n",
    "        if USE_IDF:\n",
    "            w *= math.log(1.0 + (N_items / float(deg)))\n",
    "        return w\n",
    "\n",
    "    edges = defaultdict(float)\n",
    "\n",
    "    for meta_id, members in members_of_meta.items():\n",
    "        members = list(set(members))\n",
    "        deg = len(members)\n",
    "        if deg <= 1:\n",
    "            continue\n",
    "\n",
    "        # ë„ˆë¬´ í° ë©”íƒ€ë…¸ë“œ(ì˜ˆ: íŠ¹ì • ë„ë©”ì¸ ì „ì²´)ê°€ í­ë°œí•˜ì§€ ì•Šê²Œ ìƒ˜í”Œë§/ìƒí•œ\n",
    "        if deg > MAX_MEMBERS_PER_META:\n",
    "            members = members[:MAX_MEMBERS_PER_META]\n",
    "            deg = len(members)\n",
    "\n",
    "        contrib = meta_contrib(meta_id, deg)\n",
    "        if contrib == 0.0:\n",
    "            continue\n",
    "\n",
    "        # ê·¼ë¦° ì œí•œ: ê° ë©¤ë²„ë§ˆë‹¤ ì•ìª½ Kê°œì™€ë§Œ ì—°ê²° (ì „ìŒ O(n^2) ë°©ì§€)\n",
    "        K = min(TOPK_PER_ITEM, deg - 1)\n",
    "        members.sort()\n",
    "        for i, a in enumerate(members):\n",
    "            for b in members[i+1 : i+1+K]:\n",
    "                # ì„œë¡œ ë„ë©”ì¸ì´ ë‹¤ë¥´ë©´ ìŠ¤í‚µ (ì•ˆì „ì¥ì¹˜)\n",
    "                if domain_of.get(a) != domain_of.get(b):\n",
    "                    continue\n",
    "                if a > b:\n",
    "                    a, b = b, a\n",
    "                edges[(a, b)] += contrib\n",
    "\n",
    "    # 5) ì•„ì´í…œë‹¹ TOPK ì´ì›ƒë§Œ ìœ ì§€ (ì–‘ë°©í–¥ ê¸°ì¤€ ìµœëŒ€ ê°€ì¤‘ì¹˜ ìœ ì§€)\n",
    "    nbrs = defaultdict(list)\n",
    "    for (a, b), w in edges.items():\n",
    "        nbrs[a].append((b, w))\n",
    "        nbrs[b].append((a, w))\n",
    "\n",
    "    pruned = {}\n",
    "    for a, lst in nbrs.items():\n",
    "        lst.sort(key=lambda x: x[1], reverse=True)\n",
    "        for b, w in lst[:TOPK_PER_ITEM]:\n",
    "            key = (a, b) if a < b else (b, a)\n",
    "            if key not in pruned or pruned[key] < w:\n",
    "                pruned[key] = w\n",
    "\n",
    "    df_edges = pd.DataFrame(\n",
    "        [(a, b, w) for (a, b), w in pruned.items()],\n",
    "        columns=[\"src_content_id\", \"dst_content_id\", \"weight\"]\n",
    "    )\n",
    "    df_edges.to_csv(OUT_EDGES, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì €ì¥: {OUT_EDGES} (edges={len(df_edges):,}, items={len(all_items):,})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ffdb17",
   "metadata": {},
   "source": [
    "# Noce2vec ì•„ì´í…œ ì„ë² ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23d8616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ ê·¸ë˜í”„ ë¡œë”©...\n",
      "nodes=495, edges(undirected)â‰ˆ31,148\n",
      "ğŸš¶ ëœë¤ì›Œí¬ ìƒì„±...\n",
      "walks=4,950, avg_lenâ‰ˆ40.0\n",
      "ğŸ§© í•™ìŠµ ìŒ ìƒì„±(Skip-gram window)...\n",
      "pairs=1,831,500\n",
      "ğŸ§  Skip-gram(NS) í•™ìŠµ ì‹œì‘...\n",
      "[Epoch 1/3] loss=1.3863\n",
      "[Epoch 2/3] loss=1.3863\n",
      "[Epoch 3/3] loss=1.3863\n",
      "âœ… ì„ë² ë”© ì €ì¥: csv ë°ì´í„°\\clean\\item_embeddings_torch.csv (nodes=495, dim=64)\n"
     ]
    }
   ],
   "source": [
    "# learn_item_embeddings.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#ì•„ì´í…œ-ì•„ì´í…œ ê°€ì¤‘ ê·¸ë˜í”„ -> Node2Vec ìŠ¤íƒ€ì¼ ì„ë² ë”© (Pure PyTorch, gensim/scipy ë¶ˆí•„ìš”)\n",
    "#ì…ë ¥ : C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\graph_edges_item_item.csv\n",
    "#ì¶œë ¥ : C:\\Users\\LG\\Desktop\\2025-2\\A.O.D\\db_export\\clean\\item_embeddings_torch.csv\n",
    "#ì‚¬ìš© : python learn_item_embeddings.py\n",
    "#í•„ìš” : pip install torch pandas numpy\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===== ê²½ë¡œ =====\n",
    "BASE = r\"csv ë°ì´í„°\\clean\"\n",
    "EDGES_PATH = os.path.join(BASE, \"graph_edges_item_item.csv\")\n",
    "OUT_EMB_CSV = os.path.join(BASE, \"item_embeddings_torch.csv\")\n",
    "\n",
    "# ===== í•˜ì´í¼íŒŒë¼ë¯¸í„° =====\n",
    "DIM            = 64       # ì„ë² ë”© ì°¨ì›\n",
    "WALK_LENGTH    = 40       # ëœë¤ì›Œí¬ ê¸¸ì´\n",
    "NUM_WALKS      = 10       # ë…¸ë“œë‹¹ ì›Œí¬ ìˆ˜\n",
    "P_RETURN       = 1.0      # node2vec p (ë˜ëŒì•„ê°€ê¸° ì„ í˜¸ pâ†“)\n",
    "Q_INOUT        = 1.0      # node2vec q (ì›ê±°ë¦¬ íƒìƒ‰ ì„ í˜¸ qâ†‘)\n",
    "WINDOW         = 5        # Skip-gram ìœˆë„ìš°\n",
    "NEGATIVE_K     = 5        # ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ ìˆ˜\n",
    "EPOCHS         = 3\n",
    "BATCH_SIZE     = 8192\n",
    "LR             = 0.025\n",
    "SEED           = 42\n",
    "DEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ---------- ê·¸ë˜í”„ ë¡œë“œ & ê°€ì¤‘ ë£°ë › ì¤€ë¹„ ----------\n",
    "def load_graph(edges_csv):\n",
    "    df = pd.read_csv(edges_csv, encoding=\"utf-8-sig\")\n",
    "    need = {\"src_content_id\",\"dst_content_id\",\"weight\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise ValueError(f\"{edges_csv}ì— {need} ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    adj = defaultdict(list)   # node -> [(nbr, w), ...]\n",
    "    nodes = set()\n",
    "    for a, b, w in df[[\"src_content_id\",\"dst_content_id\",\"weight\"]].itertuples(index=False):\n",
    "        if a == b: \n",
    "            continue\n",
    "        w = float(w) if pd.notna(w) else 1.0\n",
    "        adj[a].append((b, w))\n",
    "        adj[b].append((a, w))\n",
    "        nodes.add(a); nodes.add(b)\n",
    "\n",
    "    # ëˆ„ì í™•ë¥ (ë£°ë ›íœ ) ì‚¬ì „\n",
    "    cumsums = {}\n",
    "    for u, lst in adj.items():\n",
    "        if not lst:\n",
    "            continue\n",
    "        nbrs, ws = zip(*lst)\n",
    "        ws = np.maximum(np.array(ws, dtype=float), 1e-12)\n",
    "        csum = np.cumsum(ws); cumsums[u] = (np.array(nbrs, dtype=int), csum / csum[-1])\n",
    "    return adj, cumsums, sorted(nodes)\n",
    "\n",
    "def weighted_choice(nbrs, cprobs):\n",
    "    r = random.random()\n",
    "    lo, hi = 0, len(cprobs)-1\n",
    "    while lo < hi:\n",
    "        mid = (lo + hi) // 2\n",
    "        if cprobs[mid] < r: lo = mid + 1\n",
    "        else: hi = mid\n",
    "    return int(nbrs[lo])\n",
    "\n",
    "# ---------- Node2Vec ê°€ì¤‘ ëœë¤ì›Œí¬ ----------\n",
    "def node2vec_walks(adj, cumsums, nodes, walk_length, num_walks, p, q):\n",
    "    walks = []\n",
    "    base_nodes = list(nodes)\n",
    "    for _ in range(num_walks):\n",
    "        random.shuffle(base_nodes)\n",
    "        for start in base_nodes:\n",
    "            if start not in cumsums:\n",
    "                continue\n",
    "            walk = [start]\n",
    "            if walk_length == 1:\n",
    "                walks.append(walk); continue\n",
    "\n",
    "            # ì²« ìŠ¤í…: ê°€ì¤‘ì¹˜ ë¹„ë¡€ ì„ íƒ\n",
    "            nbrs, cprobs = cumsums[start]\n",
    "            if len(nbrs) == 0:\n",
    "                walks.append(walk); continue\n",
    "            curr = weighted_choice(nbrs, cprobs)\n",
    "            walk.append(curr); prev = start\n",
    "\n",
    "            for _ in range(2, walk_length):\n",
    "                cand = adj.get(curr, [])\n",
    "                if not cand: break\n",
    "                cand_nodes = [n for n,_ in cand]\n",
    "                cand_w = []\n",
    "                prev_nbrs = {n for n,_ in adj.get(prev, [])}\n",
    "                for nxt, w in cand:\n",
    "                    # p/q ë°”ì´ì–´ìŠ¤(ê·¼ì‚¬)\n",
    "                    if nxt == prev: bias = 1.0 / p\n",
    "                    elif nxt in prev_nbrs: bias = 1.0\n",
    "                    else: bias = 1.0 / q\n",
    "                    cand_w.append(max(w,1e-12) * bias)\n",
    "                cw = np.array(cand_w, dtype=float)\n",
    "                cs = np.cumsum(cw); cs /= cs[-1]\n",
    "                idx = np.searchsorted(cs, random.random())\n",
    "                nxt = cand_nodes[min(idx, len(cand_nodes)-1)]\n",
    "                walk.append(nxt); prev, curr = curr, nxt\n",
    "            walks.append(walk)\n",
    "    return walks\n",
    "\n",
    "# ---------- Skip-gram(ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§) ----------\n",
    "def generate_pairs(walks, window):\n",
    "    pairs = []\n",
    "    for walk in walks:\n",
    "        L = len(walk)\n",
    "        for i in range(L):\n",
    "            c = walk[i]\n",
    "            l = max(0, i-window); r = min(L, i+window+1)\n",
    "            for j in range(l, r):\n",
    "                if j == i: continue\n",
    "                pairs.append((c, walk[j]))\n",
    "    return pairs\n",
    "\n",
    "class SkipGramNS(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.in_emb  = torch.nn.Embedding(vocab_size, dim)\n",
    "        self.out_emb = torch.nn.Embedding(vocab_size, dim)\n",
    "        torch.nn.init.uniform_(self.in_emb.weight,  -0.5/dim, 0.5/dim)\n",
    "        torch.nn.init.uniform_(self.out_emb.weight, -0.5/dim, 0.5/dim)\n",
    "\n",
    "    def forward(self, center, pos, neg):\n",
    "        # center:[B], pos:[B], neg:[B,K]\n",
    "        v  = self.in_emb(center)           # [B, D]\n",
    "        u  = self.out_emb(pos)             # [B, D]\n",
    "        uv = (v * u).sum(dim=1)            # [B]\n",
    "        pos_loss = torch.nn.functional.logsigmoid(uv).mean()\n",
    "\n",
    "        neg_u = self.out_emb(neg)          # [B, K, D]\n",
    "        neg_uv = torch.bmm(neg_u, v.unsqueeze(2)).squeeze(2)  # [B, K]\n",
    "        neg_loss = torch.nn.functional.logsigmoid(-neg_uv).mean()\n",
    "        return -(pos_loss + neg_loss)      # minimize\n",
    "\n",
    "def train_skipgram_ns(pairs, id2idx, epochs=3, batch_size=8192, dim=64, neg_k=5, lr=0.025):\n",
    "    # ë…¸ë“œ ì¸ë±ì‹±\n",
    "    vocab = sorted(id2idx.keys())\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # ë¹ˆë„ ê¸°ë°˜ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ë¶„í¬(0.75 ìŠ¹)\n",
    "    counts = defaultdict(int)\n",
    "    for a,b in pairs:\n",
    "        counts[a] += 1; counts[b] += 1\n",
    "    idx_counts = np.zeros(vocab_size, dtype=np.float64)\n",
    "    for nid, cnt in counts.items():\n",
    "        idx_counts[id2idx[nid]] = cnt\n",
    "    prob = idx_counts ** 0.75\n",
    "    prob = prob / prob.sum()\n",
    "    alias_table = np.cumsum(prob)\n",
    "\n",
    "    def sample_neg(B, K):\n",
    "        # ëˆ„ì ë¶„í¬ ê¸°ë°˜ ë²¡í„°í™” ìƒ˜í”Œë§\n",
    "        r = np.random.rand(B, K)\n",
    "        idx = np.searchsorted(alias_table, r, side=\"right\")\n",
    "        return torch.from_numpy(idx.astype(np.int64))\n",
    "\n",
    "    # í•™ìŠµ ë°ì´í„° í…ì„œí™”(ì¸ë±ìŠ¤ ë³€í™˜)\n",
    "    centers = torch.tensor([id2idx[a] for a,_ in pairs], dtype=torch.long)\n",
    "    contexts= torch.tensor([id2idx[b] for _,b in pairs], dtype=torch.long)\n",
    "\n",
    "    ds_size = len(pairs)\n",
    "    model = SkipGramNS(vocab_size, dim).to(DEVICE)\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        perm = torch.randperm(ds_size)\n",
    "        centers = centers[perm]; contexts = contexts[perm]\n",
    "        total_loss = 0.0; steps = 0\n",
    "\n",
    "        for i in range(0, ds_size, batch_size):\n",
    "            c_batch = centers[i:i+batch_size].to(DEVICE)\n",
    "            p_batch = contexts[i:i+batch_size].to(DEVICE)\n",
    "            B = c_batch.size(0)\n",
    "            n_batch = sample_neg(B, neg_k).to(DEVICE)\n",
    "\n",
    "            loss = model(c_batch, p_batch, n_batch)\n",
    "            optim.zero_grad(); loss.backward(); optim.step()\n",
    "\n",
    "            total_loss += loss.item(); steps += 1\n",
    "\n",
    "        avg = total_loss / max(1, steps)\n",
    "        print(f\"[Epoch {epoch}/{epochs}] loss={avg:.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = model.in_emb.weight.detach().cpu().numpy()\n",
    "    return emb  # shape [vocab_size, dim]\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸ“¥ ê·¸ë˜í”„ ë¡œë”©...\")\n",
    "    adj, cumsums, nodes = load_graph(EDGES_PATH)\n",
    "    print(f\"nodes={len(nodes):,}, edges(undirected)â‰ˆ{sum(len(v) for v in adj.values())//2:,}\")\n",
    "\n",
    "    print(\"ğŸš¶ ëœë¤ì›Œí¬ ìƒì„±...\")\n",
    "    walks = node2vec_walks(\n",
    "        adj, cumsums, nodes,\n",
    "        walk_length=WALK_LENGTH,\n",
    "        num_walks=NUM_WALKS,\n",
    "        p=P_RETURN, q=Q_INOUT\n",
    "    )\n",
    "    avg_len = np.mean([len(w) for w in walks]) if walks else 0\n",
    "    print(f\"walks={len(walks):,}, avg_lenâ‰ˆ{avg_len:.1f}\")\n",
    "\n",
    "    print(\"ğŸ§© í•™ìŠµ ìŒ ìƒì„±(Skip-gram window)...\")\n",
    "    pairs = generate_pairs(walks, WINDOW)\n",
    "    print(f\"pairs={len(pairs):,}\")\n",
    "\n",
    "    # ë…¸ë“œ id â†’ ì—°ì† index ë§¤í•‘\n",
    "    id2idx = {nid:i for i, nid in enumerate(sorted(nodes))}\n",
    "    print(\"ğŸ§  Skip-gram(NS) í•™ìŠµ ì‹œì‘...\")\n",
    "    emb = train_skipgram_ns(\n",
    "        pairs, id2idx,\n",
    "        epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        dim=DIM, neg_k=NEGATIVE_K, lr=LR\n",
    "    )\n",
    "\n",
    "    # CSV ì €ì¥\n",
    "    idx2id = {i:nid for nid,i in id2idx.items()}\n",
    "    rows = [[idx2id[i]] + list(map(float, emb[i])) for i in range(len(idx2id))]\n",
    "    cols = [\"content_id\"] + [f\"emb_{i}\" for i in range(DIM)]\n",
    "    out = pd.DataFrame(rows, columns=cols).sort_values(\"content_id\")\n",
    "    out.to_csv(OUT_EMB_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì„ë² ë”© ì €ì¥: {OUT_EMB_CSV} (nodes={len(out)}, dim={DIM})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7dbca",
   "metadata": {},
   "source": [
    "# ìœ ì € ì„ë² ë”© ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c73c2079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: item_embeddings_torch.csv (items=495, dim=64)\n",
      "âœ… ë„ë©”ì¸ ì„¼íŠ¸ë¡œì´ë“œ: {'AV': 200, 'GAME': 100, 'WEBNOVEL': 195}\n",
      "â„¹ï¸ content_raw_genres.csv ë¹„ì–´ìˆìŒ ë˜ëŠ” ì»¬ëŸ¼ ë¶ˆì¶©ë¶„ â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìƒëµ\n",
      "âœ… user_embeddings.csv ì €ì¥: csv ë°ì´í„°\\clean\\user_embeddings.csv (users=10, skipped_rows=0)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ìœ ì € ì„ë² ë”© ìƒì„± (ì•„ì´í…œ ì„ë² ë”© + ì„ í˜¸ ë„ë©”ì¸/ì¥ë¥´ centroid)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, List\n",
    "\n",
    "BASE = r\"csv ë°ì´í„°\\clean\"\n",
    "ITEM_EMB_1 = os.path.join(BASE, \"item_embeddings_torch.csv\")\n",
    "ITEM_EMB_2 = os.path.join(BASE, \"item_embeddings.csv\")\n",
    "CONTENTS   = os.path.join(BASE, \"contents.csv\")\n",
    "RAW_GENRES = os.path.join(BASE, \"content_raw_genres.csv\")\n",
    "UPREF      = os.path.join(BASE, \"user_preferences.csv\")\n",
    "UP_CTYPES  = os.path.join(BASE, \"user_preferred_content_types.csv\")\n",
    "UP_GENRES  = os.path.join(BASE, \"user_preferred_genres.csv\")\n",
    "OUT_USER   = os.path.join(BASE, \"user_embeddings.csv\")\n",
    "\n",
    "def read_csv_retry(path, encodings=(\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"), **kw) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    last = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kw)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "def load_item_embeddings() -> pd.DataFrame:\n",
    "    # ëª…ì‹œì  ì„ íƒ (DataFrameì— or ê¸ˆì§€)\n",
    "    df1 = read_csv_retry(ITEM_EMB_1)\n",
    "    if df1 is not None and not df1.empty:\n",
    "        df = df1\n",
    "        src = os.path.basename(ITEM_EMB_1)\n",
    "    else:\n",
    "        df2 = read_csv_retry(ITEM_EMB_2)\n",
    "        if df2 is not None and not df2.empty:\n",
    "            df = df2\n",
    "            src = os.path.basename(ITEM_EMB_2)\n",
    "        else:\n",
    "            raise RuntimeError(\"ì•„ì´í…œ ì„ë² ë”© CSVê°€ ì—†ìŠµë‹ˆë‹¤: item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv\")\n",
    "\n",
    "    if \"content_id\" not in df.columns:\n",
    "        raise RuntimeError(\"ì•„ì´í…œ ì„ë² ë”© CSVì— content_id ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    emb_cols = [c for c in df.columns if c.startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        raise RuntimeError(\"ì•„ì´í…œ ì„ë² ë”© CSVì— emb_ë¡œ ì‹œì‘í•˜ëŠ” ì„ë² ë”© ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    df[\"content_id\"] = pd.to_numeric(df[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df[df[\"content_id\"].notna()].copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    for c in emb_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df.dropna(subset=emb_cols, inplace=True)\n",
    "\n",
    "    # L2 normalize\n",
    "    M = df[emb_cols].to_numpy(np.float32)\n",
    "    M = M / (np.linalg.norm(M, axis=1, keepdims=True) + 1e-12)\n",
    "    df.loc[:, emb_cols] = M\n",
    "\n",
    "    print(f\"âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: {src} (items={len(df)}, dim={len(emb_cols)})\")\n",
    "    return df[[\"content_id\"] + emb_cols]\n",
    "\n",
    "# content_type â†’ domain ë§¤í•‘\n",
    "CTYPE2DOMAIN = {\n",
    "    \"movie\": \"AV\", \"tv\": \"AV\",\n",
    "    \"game\": \"GAME\",\n",
    "    \"webnovel\": \"WEBNOVEL\", \"webtoon\": \"WEBNOVEL\"\n",
    "}\n",
    "\n",
    "def main():\n",
    "    emb = load_item_embeddings()\n",
    "    emb_cols = [c for c in emb.columns if c.startswith(\"emb_\")]\n",
    "    M = emb[emb_cols].to_numpy(np.float32)\n",
    "    ids = emb[\"content_id\"].to_numpy(int)\n",
    "\n",
    "    # contents.csv â†’ domain\n",
    "    contents = read_csv_retry(CONTENTS)\n",
    "    if contents is None or contents.empty:\n",
    "        raise RuntimeError(\"contents.csv í•„ìš”.\")\n",
    "    contents = contents[[\"content_id\", \"domain\"]].copy()\n",
    "    contents[\"content_id\"] = pd.to_numeric(contents[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    contents = contents[contents[\"content_id\"].notna()].copy()\n",
    "    contents[\"content_id\"] = contents[\"content_id\"].astype(int)\n",
    "    emb_meta = emb.merge(contents, on=\"content_id\", how=\"left\")\n",
    "\n",
    "    # domain centroids\n",
    "    domain_vecs = {}\n",
    "    dom_counts = {}\n",
    "    for dom, grp in emb_meta.dropna(subset=[\"domain\"]).groupby(\"domain\"):\n",
    "        mat = grp[emb_cols].to_numpy(np.float32)\n",
    "        if len(mat) == 0: \n",
    "            continue\n",
    "        v = mat.mean(axis=0)\n",
    "        v = v / (np.linalg.norm(v) + 1e-12)\n",
    "        domain_vecs[str(dom)] = v\n",
    "        dom_counts[str(dom)] = len(mat)\n",
    "    print(f\"âœ… ë„ë©”ì¸ ì„¼íŠ¸ë¡œì´ë“œ: { {k: dom_counts[k] for k in sorted(dom_counts)} }\")\n",
    "\n",
    "    # genre centroids (optional)\n",
    "    genre_vecs = {}\n",
    "    if os.path.exists(RAW_GENRES):\n",
    "        rawg = read_csv_retry(RAW_GENRES)\n",
    "        if rawg is not None and not rawg.empty and {\"content_id\",\"raw_genre\"}.issubset(rawg.columns):\n",
    "            rawg = rawg[[\"content_id\", \"raw_genre\"]].dropna().copy()\n",
    "            rawg[\"content_id\"] = pd.to_numeric(rawg[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "            rawg = rawg[rawg[\"content_id\"].notna()].copy()\n",
    "            rawg[\"content_id\"] = rawg[\"content_id\"].astype(int)\n",
    "            rawg[\"g\"] = rawg[\"raw_genre\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "            idx_map = {cid: i for i, cid in enumerate(ids)}\n",
    "            for g, grp in rawg.groupby(\"g\"):\n",
    "                idxs = [idx_map[c] for c in grp[\"content_id\"].tolist() if c in idx_map]\n",
    "                if not idxs:\n",
    "                    continue\n",
    "                v = M[idxs].mean(axis=0)\n",
    "                v = v / (np.linalg.norm(v) + 1e-12)\n",
    "                genre_vecs[g] = v\n",
    "            print(f\"âœ… ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìˆ˜: {len(genre_vecs)}\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ content_raw_genres.csv ë¹„ì–´ìˆìŒ ë˜ëŠ” ì»¬ëŸ¼ ë¶ˆì¶©ë¶„ â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìƒëµ\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ content_raw_genres.csv ì—†ìŒ â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìƒëµ\")\n",
    "\n",
    "    # user prefs\n",
    "    upref = read_csv_retry(UPREF)\n",
    "    uctype = read_csv_retry(UP_CTYPES)\n",
    "    ugen = read_csv_retry(UP_GENRES)\n",
    "    if upref is None or upref.empty:\n",
    "        raise RuntimeError(\"user_preferences.csv í•„ìš”.\")\n",
    "\n",
    "    ctype_map = {}\n",
    "    if uctype is not None and not uctype.empty and {\"preference_id\", \"content_type\"}.issubset(uctype.columns):\n",
    "        for pid, grp in uctype.groupby(\"preference_id\"):\n",
    "            ctype_map[int(pid)] = [str(x).strip() for x in grp[\"content_type\"].tolist()]\n",
    "\n",
    "    genre_map = {}\n",
    "    if ugen is not None and not ugen.empty and {\"preference_id\", \"genre\"}.issubset(ugen.columns):\n",
    "        for pid, grp in ugen.groupby(\"preference_id\"):\n",
    "            genre_map[int(pid)] = [str(x).strip().lower() for x in grp[\"genre\"].tolist()]\n",
    "\n",
    "    def ctypes_to_domains(cts: List[str]) -> List[str]:\n",
    "        outs = set()\n",
    "        for ct in cts:\n",
    "            d = CTYPE2DOMAIN.get(str(ct).lower())\n",
    "            if d:\n",
    "                outs.add(d)\n",
    "        return sorted(outs)\n",
    "\n",
    "    # ê°€ì¤‘ì¹˜\n",
    "    ALPHA_DOM, ALPHA_GEN = 0.6, 0.4\n",
    "\n",
    "    rows = []\n",
    "    skipped = 0\n",
    "    for _, r in upref.iterrows():\n",
    "        if pd.isna(r.get(\"id\")):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        uid = int(r[\"id\"])\n",
    "        uname = r.get(\"username\", \"\")\n",
    "\n",
    "        # domains\n",
    "        doms = ctypes_to_domains(ctype_map.get(uid, []))\n",
    "        dom_vecs = [domain_vecs[d] for d in doms if d in domain_vecs]\n",
    "        v_dom = None\n",
    "        if dom_vecs:\n",
    "            v_dom = np.mean(np.stack(dom_vecs, axis=0), axis=0)\n",
    "            v_dom = v_dom / (np.linalg.norm(v_dom) + 1e-12)\n",
    "\n",
    "        # genres\n",
    "        gnames = [g for g in (genre_map.get(uid, [])) if g in genre_vecs]\n",
    "        g_vecs = [genre_vecs[g] for g in gnames]\n",
    "        v_gen = None\n",
    "        if g_vecs:\n",
    "            v_gen = np.mean(np.stack(g_vecs, axis=0), axis=0)\n",
    "            v_gen = v_gen / (np.linalg.norm(v_gen) + 1e-12)\n",
    "\n",
    "        # user vector\n",
    "        if v_dom is None and v_gen is None:\n",
    "            # í´ë°±: ì „ì²´ ì•„ì´í…œ í‰ê· \n",
    "            v_user = M.mean(axis=0)\n",
    "        elif v_dom is None:\n",
    "            v_user = v_gen\n",
    "        elif v_gen is None:\n",
    "            v_user = v_dom\n",
    "        else:\n",
    "            v_user = ALPHA_DOM * v_dom + ALPHA_GEN * v_gen\n",
    "        v_user = v_user / (np.linalg.norm(v_user) + 1e-12)\n",
    "\n",
    "        rows.append([uid, uname] + list(map(float, v_user)))\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"ìœ ì € ì„ë² ë”© ìƒì„± ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. ì…ë ¥ íŒŒì¼/ì„ í˜¸ ì •ë³´ í™•ì¸ í•„ìš”.\")\n",
    "\n",
    "    dim = M.shape[1]\n",
    "    out = pd.DataFrame(rows, columns=[\"user_id\", \"username\"] + [f\"emb_{i}\" for i in range(dim)])\n",
    "    out.sort_values(\"user_id\").to_csv(OUT_USER, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… user_embeddings.csv ì €ì¥: {OUT_USER} (users={len(out)}, skipped_rows={skipped})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab68a11",
   "metadata": {},
   "source": [
    "# ìœ ì € ì„ë² ë”© ìƒì„±(ì¥ë¥´ë§Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c05db78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: item_embeddings_torch.csv (items=495, dim=64)\n",
      "â„¹ï¸ ë„ë©”ì¸ë³„ ì•„ì´í…œ ìˆ˜ (ì°¸ê³ ìš©): {'AV': 200, 'GAME': 100, 'WEBNOVEL': 195}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "content_raw_genres.csv ë¹„ì–´ìˆê±°ë‚˜ ì»¬ëŸ¼ ë¶€ì¡±(raw_genre, content_id) â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° ë¶ˆê°€",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 177\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… user_embeddings.csv ì €ì¥: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_USER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (users=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(out)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, skipped_users_without_valid_genres=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mskipped\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 177\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 112\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìˆ˜: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(genre_vecs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_raw_genres.csv ë¹„ì–´ìˆê±°ë‚˜ ì»¬ëŸ¼ ë¶€ì¡±(raw_genre, content_id) â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° ë¶ˆê°€\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_raw_genres.csv ì—†ìŒ â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° ë¶ˆê°€\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: content_raw_genres.csv ë¹„ì–´ìˆê±°ë‚˜ ì»¬ëŸ¼ ë¶€ì¡±(raw_genre, content_id) â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° ë¶ˆê°€"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ìœ ì € ì„ë² ë”© ìƒì„± (ìœ ì € ì„ í˜¸ ì¥ë¥´ centroidë§Œ ì‚¬ìš©)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, List\n",
    "\n",
    "BASE = r\"csv ë°ì´í„°\\clean\"\n",
    "ITEM_EMB_1 = os.path.join(BASE, \"item_embeddings_torch.csv\")\n",
    "ITEM_EMB_2 = os.path.join(BASE, \"item_embeddings.csv\")\n",
    "CONTENTS   = os.path.join(BASE, \"contents.csv\")\n",
    "RAW_GENRES = os.path.join(BASE, \"content_raw_genres.csv\")\n",
    "UP_GENRES  = os.path.join(BASE, \"user_preferred_genres.csv\")\n",
    "OUT_USER   = os.path.join(BASE, \"user_embeddings.csv\")\n",
    "\n",
    "\n",
    "def read_csv_retry(path, encodings=(\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"), **kw) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    last = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kw)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "\n",
    "def load_item_embeddings() -> pd.DataFrame:\n",
    "    # ëª…ì‹œì  ì„ íƒ (DataFrameì— or ê¸ˆì§€)\n",
    "    df1 = read_csv_retry(ITEM_EMB_1)\n",
    "    if df1 is not None and not df1.empty:\n",
    "        df = df1\n",
    "        src = os.path.basename(ITEM_EMB_1)\n",
    "    else:\n",
    "        df2 = read_csv_retry(ITEM_EMB_2)\n",
    "        if df2 is not None and not df2.empty:\n",
    "            df = df2\n",
    "            src = os.path.basename(ITEM_EMB_2)\n",
    "        else:\n",
    "            raise RuntimeError(\"ì•„ì´í…œ ì„ë² ë”© CSVê°€ ì—†ìŠµë‹ˆë‹¤: item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv\")\n",
    "\n",
    "    if \"content_id\" not in df.columns:\n",
    "        raise RuntimeError(\"ì•„ì´í…œ ì„ë² ë”© CSVì— content_id ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    emb_cols = [c for c in df.columns if c.startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        raise RuntimeError(\"ì•„ì´í…œ ì„ë² ë”© CSVì— emb_ë¡œ ì‹œì‘í•˜ëŠ” ì„ë² ë”© ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    df[\"content_id\"] = pd.to_numeric(df[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df[df[\"content_id\"].notna()].copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    for c in emb_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df.dropna(subset=emb_cols, inplace=True)\n",
    "\n",
    "    # L2 normalize\n",
    "    M = df[emb_cols].to_numpy(np.float32)\n",
    "    M = M / (np.linalg.norm(M, axis=1, keepdims=True) + 1e-12)\n",
    "    df.loc[:, emb_cols] = M\n",
    "\n",
    "    print(f\"âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: {src} (items={len(df)}, dim={len(emb_cols)})\")\n",
    "    return df[[\"content_id\"] + emb_cols]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) ì•„ì´í…œ ì„ë² ë”© + contents ë¡œë“œ\n",
    "    emb = load_item_embeddings()\n",
    "    emb_cols = [c for c in emb.columns if c.startswith(\"emb_\")]\n",
    "    M = emb[emb_cols].to_numpy(np.float32)\n",
    "    ids = emb[\"content_id\"].to_numpy(int)\n",
    "\n",
    "    contents = read_csv_retry(CONTENTS)\n",
    "    if contents is None or contents.empty:\n",
    "        raise RuntimeError(\"contents.csv í•„ìš”.\")\n",
    "    contents = contents[[\"content_id\", \"domain\"]].copy()\n",
    "    contents[\"content_id\"] = pd.to_numeric(contents[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    contents = contents[contents[\"content_id\"].notna()].copy()\n",
    "    contents[\"content_id\"] = contents[\"content_id\"].astype(int)\n",
    "    emb_meta = emb.merge(contents, on=\"content_id\", how=\"left\")\n",
    "\n",
    "    # 2) (ì˜µì…˜) domain ì •ë³´ëŠ” ë” ì´ìƒ ì•ˆ ì“°ì§€ë§Œ, í•„ìš”í•˜ë©´ ì°¸ê³ ìš©ìœ¼ë¡œ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚°\n",
    "    dom_counts = {}\n",
    "    for dom, grp in emb_meta.dropna(subset=[\"domain\"]).groupby(\"domain\"):\n",
    "        dom_counts[str(dom)] = len(grp)\n",
    "    if dom_counts:\n",
    "        print(f\"â„¹ï¸ ë„ë©”ì¸ë³„ ì•„ì´í…œ ìˆ˜ (ì°¸ê³ ìš©): { {k: dom_counts[k] for k in sorted(dom_counts)} }\")\n",
    "\n",
    "    # 3) ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° (content_raw_genres.csv + item ì„ë² ë”©)\n",
    "    genre_vecs = {}\n",
    "    if os.path.exists(RAW_GENRES):\n",
    "        rawg = read_csv_retry(RAW_GENRES)\n",
    "        if rawg is not None and not rawg.empty and {\"content_id\",\"raw_genre\"}.issubset(rawg.columns):\n",
    "            rawg = rawg[[\"content_id\", \"raw_genre\"]].dropna().copy()\n",
    "            rawg[\"content_id\"] = pd.to_numeric(rawg[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "            rawg = rawg[rawg[\"content_id\"].notna()].copy()\n",
    "            rawg[\"content_id\"] = rawg[\"content_id\"].astype(int)\n",
    "            rawg[\"g\"] = rawg[\"raw_genre\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "            idx_map = {cid: i for i, cid in enumerate(ids)}\n",
    "            for g, grp in rawg.groupby(\"g\"):\n",
    "                idxs = [idx_map[c] for c in grp[\"content_id\"].tolist() if c in idx_map]\n",
    "                if not idxs:\n",
    "                    continue\n",
    "                v = M[idxs].mean(axis=0)\n",
    "                v = v / (np.linalg.norm(v) + 1e-12)\n",
    "                genre_vecs[g] = v\n",
    "            print(f\"âœ… ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìˆ˜: {len(genre_vecs)}\")\n",
    "        else:\n",
    "            raise RuntimeError(\"content_raw_genres.csv ë¹„ì–´ìˆê±°ë‚˜ ì»¬ëŸ¼ ë¶€ì¡±(raw_genre, content_id) â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° ë¶ˆê°€\")\n",
    "    else:\n",
    "        raise RuntimeError(\"content_raw_genres.csv ì—†ìŒ â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° ë¶ˆê°€\")\n",
    "\n",
    "    # 4) user_preferred_genres.csv ë¡œë¶€í„° ìœ ì €ë³„ ì„ í˜¸ ì¥ë¥´ ê°€ì ¸ì˜¤ê¸°\n",
    "    ugen = read_csv_retry(UP_GENRES)\n",
    "    if ugen is None or ugen.empty:\n",
    "        raise RuntimeError(\"user_preferred_genres.csv í•„ìš”.\")\n",
    "\n",
    "    # ì–´ë–¤ ì»¬ëŸ¼ì´ ìœ ì € ì‹ë³„ìì¸ì§€ ì¶”ë¡  (user_id / id / preference_id ì¤‘ í•˜ë‚˜)\n",
    "    col_user_id = None\n",
    "    for cand in [\"user_id\", \"id\", \"preference_id\"]:\n",
    "        if cand in ugen.columns:\n",
    "            col_user_id = cand\n",
    "            break\n",
    "    if col_user_id is None:\n",
    "        raise RuntimeError(\"user_preferred_genres.csv ì— user_id / id / preference_id ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ìœ ì € ì‹ë³„ ì»¬ëŸ¼ í•˜ë‚˜ í•„ìš”.\")\n",
    "\n",
    "    if \"genre\" not in ugen.columns:\n",
    "        raise RuntimeError(\"user_preferred_genres.csv ì— genre ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    has_username = \"username\" in ugen.columns\n",
    "\n",
    "    keep_cols = [col_user_id, \"genre\"]\n",
    "    if has_username:\n",
    "        keep_cols.append(\"username\")\n",
    "\n",
    "    ugen = ugen[keep_cols].dropna(subset=[col_user_id, \"genre\"]).copy()\n",
    "    ugen[col_user_id] = pd.to_numeric(ugen[col_user_id], errors=\"coerce\").astype(\"Int64\")\n",
    "    ugen = ugen[ugen[col_user_id].notna()].copy()\n",
    "    ugen[col_user_id] = ugen[col_user_id].astype(int)\n",
    "    ugen[\"g\"] = ugen[\"genre\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # 5) ìœ ì €ë³„ë¡œ ì„ í˜¸ ì¥ë¥´ â†’ ì¥ë¥´ ë²¡í„° í‰ê·  â†’ ìœ ì € ì„ë² ë”©\n",
    "    rows = []\n",
    "    skipped = 0\n",
    "    for uid, grp in ugen.groupby(col_user_id):\n",
    "        # ì´ ìœ ì €ê°€ ì„ í˜¸í•œë‹¤ê³  í‘œì‹œí•œ ì¥ë¥´ë“¤ ì¤‘, ì‹¤ì œ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œê°€ ìˆëŠ” ê²ƒë§Œ ì‚¬ìš©\n",
    "        genres = sorted(set([g for g in grp[\"g\"].tolist() if g in genre_vecs]))\n",
    "        if not genres:\n",
    "            # ì¥ë¥´ ë¬¸ìì—´ì€ ìˆì§€ë§Œ content_raw_genres ê¸°ë°˜ ì„¼íŠ¸ë¡œì´ë“œê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        g_vecs = [genre_vecs[g] for g in genres]\n",
    "        v_user = np.mean(np.stack(g_vecs, axis=0), axis=0)\n",
    "        v_user = v_user / (np.linalg.norm(v_user) + 1e-12)\n",
    "\n",
    "        uname = \"\"\n",
    "        if has_username:\n",
    "            # ê°™ì€ ìœ ì €ì— username ì—¬ëŸ¬ ê°œ ìˆì–´ë„ ì²« ë²ˆì§¸ ê²ƒ ì‚¬ìš©\n",
    "            uname = str(grp[\"username\"].iloc[0])\n",
    "\n",
    "        rows.append([int(uid), uname] + list(map(float, v_user)))\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"ìœ ì € ì„ë² ë”© ìƒì„± ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. user_preferred_genres.csv / ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "    dim = len(emb_cols)\n",
    "    out = pd.DataFrame(rows, columns=[\"user_id\", \"username\"] + [f\"emb_{i}\" for i in range(dim)])\n",
    "    out.sort_values(\"user_id\").to_csv(OUT_USER, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… user_embeddings.csv ì €ì¥: {OUT_USER} (users={len(out)}, skipped_users_without_valid_genres={skipped})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da8244aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: item_embeddings_torch.csv (items=495, dim=64)\n",
      "â„¹ï¸ ë„ë©”ì¸ë³„ ì•„ì´í…œ ìˆ˜ (ì°¸ê³ ìš©): {'AV': 200, 'GAME': 100, 'WEBNOVEL': 195}\n",
      "âœ… content_raw_genres wide â†’ tall ë³€í™˜: rows=801\n",
      "âœ… ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìˆ˜: 45\n",
      "âœ… user_embeddings.csv ì €ì¥: csv ë°ì´í„°\\clean\\user_embeddings.csv (users=10, skipped_users_without_valid_genres=0)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ìœ ì € ì„ë² ë”© ìƒì„± (ìœ ì € ì„ í˜¸ ì¥ë¥´ centroidë§Œ ì‚¬ìš©)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, List\n",
    "\n",
    "BASE = r\"csv ë°ì´í„°\\clean\"\n",
    "ITEM_EMB_1 = os.path.join(BASE, \"item_embeddings_torch.csv\")\n",
    "ITEM_EMB_2 = os.path.join(BASE, \"item_embeddings.csv\")\n",
    "CONTENTS   = os.path.join(BASE, \"contents.csv\")\n",
    "RAW_GENRES = os.path.join(BASE, \"content_raw_genres.csv\")  # ì§€ê¸ˆ: content_id, source, raw_genre_1~3\n",
    "UP_GENRES  = os.path.join(BASE, \"user_preferred_genres.csv\")\n",
    "OUT_USER   = os.path.join(BASE, \"user_embeddings.csv\")\n",
    "\n",
    "\n",
    "def read_csv_retry(path, encodings=(\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"), **kw) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    last = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kw)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "\n",
    "def load_item_embeddings() -> pd.DataFrame:\n",
    "    # ëª…ì‹œì  ì„ íƒ (DataFrameì— or ê¸ˆì§€)\n",
    "    df1 = read_csv_retry(ITEM_EMB_1)\n",
    "    if df1 is not None and not df1.empty:\n",
    "        df = df1\n",
    "        src = os.path.basename(ITEM_EMB_1)\n",
    "    else:\n",
    "        df2 = read_csv_retry(ITEM_EMB_2)\n",
    "        if df2 is not None and not df2.empty:\n",
    "            df = df2\n",
    "            src = os.path.basename(ITEM_EMB_2)\n",
    "        else:\n",
    "            raise RuntimeError(\"ì•„ì´í…œ ì„ë² ë”© CSVê°€ ì—†ìŠµë‹ˆë‹¤: item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv\")\n",
    "\n",
    "    if \"content_id\" not in df.columns:\n",
    "        raise RuntimeError(\"ì•„ì´í…œ ì„ë² ë”© CSVì— content_id ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    emb_cols = [c for c in df.columns if c.startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        raise RuntimeError(\"ì•„ì´í…œ ì„ë² ë”© CSVì— emb_ë¡œ ì‹œì‘í•˜ëŠ” ì„ë² ë”© ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    df[\"content_id\"] = pd.to_numeric(df[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df[df[\"content_id\"].notna()].copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    for c in emb_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df.dropna(subset=emb_cols, inplace=True)\n",
    "\n",
    "    # L2 normalize\n",
    "    M = df[emb_cols].to_numpy(np.float32)\n",
    "    M = M / (np.linalg.norm(M, axis=1, keepdims=True) + 1e-12)\n",
    "    df.loc[:, emb_cols] = M\n",
    "\n",
    "    print(f\"âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: {src} (items={len(df)}, dim={len(emb_cols)})\")\n",
    "    return df[[\"content_id\"] + emb_cols]\n",
    "\n",
    "\n",
    "def load_content_genres_tall() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    content_raw_genres.csv ê°€\n",
    "      - ì„¸ë¡œí˜•: content_id, source, raw_genre\n",
    "      - ë˜ëŠ” ê°€ë¡œí˜•: content_id, source, raw_genre_1~3\n",
    "    ë‘˜ ì¤‘ ì–´ë–¤ í¬ë§·ì´ë“  ë°›ì•„ì„œ\n",
    "    í•­ìƒ (content_id, raw_genre) ì„¸ë¡œí˜•ìœ¼ë¡œ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    g = read_csv_retry(RAW_GENRES)\n",
    "    if g is None or g.empty:\n",
    "        raise RuntimeError(\"content_raw_genres.csv ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    g = g.copy()\n",
    "    g.columns = [c.strip() for c in g.columns]\n",
    "\n",
    "    if \"content_id\" not in g.columns:\n",
    "        raise RuntimeError(\"content_raw_genres.csv ì— content_id ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # ì´ë¯¸ raw_genre ë‹¨ì¼ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "    if \"raw_genre\" in g.columns:\n",
    "        g[\"content_id\"] = pd.to_numeric(g[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        g = g[g[\"content_id\"].notna()].copy()\n",
    "        g[\"content_id\"] = g[\"content_id\"].astype(int)\n",
    "        g[\"raw_genre\"]  = g[\"raw_genre\"].astype(str).str.strip()\n",
    "        g = g[g[\"raw_genre\"] != \"\"]\n",
    "        return g[[\"content_id\", \"raw_genre\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # ê°€ë¡œí˜•: raw_genre_1~3 ì„ ì„¸ë¡œí˜•ìœ¼ë¡œ í´ê¸°\n",
    "    genre_cols = [c for c in [\"raw_genre_1\", \"raw_genre_2\", \"raw_genre_3\"] if c in g.columns]\n",
    "    if not genre_cols:\n",
    "        raise RuntimeError(\"content_raw_genres.csv ì— raw_genre ë˜ëŠ” raw_genre_1/2/3 ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    g[\"content_id\"] = pd.to_numeric(g[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    g = g[g[\"content_id\"].notna()].copy()\n",
    "    g[\"content_id\"] = g[\"content_id\"].astype(int)\n",
    "\n",
    "    tall = g.melt(\n",
    "        id_vars=[\"content_id\"],\n",
    "        value_vars=genre_cols,\n",
    "        value_name=\"raw_genre\"\n",
    "    )\n",
    "    tall[\"raw_genre\"] = tall[\"raw_genre\"].astype(str).str.strip()\n",
    "    tall = tall[tall[\"raw_genre\"].notna() & (tall[\"raw_genre\"] != \"\")]\n",
    "    tall = tall[[\"content_id\", \"raw_genre\"]].drop_duplicates()\n",
    "\n",
    "    print(f\"âœ… content_raw_genres wide â†’ tall ë³€í™˜: rows={len(tall)}\")\n",
    "    return tall.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) ì•„ì´í…œ ì„ë² ë”© + contents ë¡œë“œ\n",
    "    emb = load_item_embeddings()\n",
    "    emb_cols = [c for c in emb.columns if c.startswith(\"emb_\")]\n",
    "    M = emb[emb_cols].to_numpy(np.float32)\n",
    "    ids = emb[\"content_id\"].to_numpy(int)\n",
    "\n",
    "    contents = read_csv_retry(CONTENTS)\n",
    "    if contents is None or contents.empty:\n",
    "        raise RuntimeError(\"contents.csv í•„ìš”.\")\n",
    "    contents = contents[[\"content_id\", \"domain\"]].copy()\n",
    "    contents[\"content_id\"] = pd.to_numeric(contents[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    contents = contents[contents[\"content_id\"].notna()].copy()\n",
    "    contents[\"content_id\"] = contents[\"content_id\"].astype(int)\n",
    "    emb_meta = emb.merge(contents, on=\"content_id\", how=\"left\")\n",
    "\n",
    "    # (ì˜µì…˜) ë„ë©”ì¸ ì •ë³´ ì¶œë ¥ë§Œ\n",
    "    dom_counts = {}\n",
    "    for dom, grp in emb_meta.dropna(subset=[\"domain\"]).groupby(\"domain\"):\n",
    "        dom_counts[str(dom)] = len(grp)\n",
    "    if dom_counts:\n",
    "        print(f\"â„¹ï¸ ë„ë©”ì¸ë³„ ì•„ì´í…œ ìˆ˜ (ì°¸ê³ ìš©): { {k: dom_counts[k] for k in sorted(dom_counts)} }\")\n",
    "\n",
    "    # 3) ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° (content_raw_genres.csv + item ì„ë² ë”©)\n",
    "    if not os.path.exists(RAW_GENRES):\n",
    "        raise RuntimeError(\"content_raw_genres.csv ì—†ìŒ â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° ë¶ˆê°€\")\n",
    "\n",
    "    rawg_tall = load_content_genres_tall()  # í•­ìƒ content_id, raw_genre ì„¸ë¡œí˜•ìœ¼ë¡œ ë°˜í™˜\n",
    "\n",
    "    if rawg_tall is None or rawg_tall.empty:\n",
    "        raise RuntimeError(\"content_raw_genres.csv ì— ìœ íš¨í•œ ì¥ë¥´ê°€ ì—†ìŠµë‹ˆë‹¤. â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° ë¶ˆê°€\")\n",
    "\n",
    "    rawg = rawg_tall.copy()\n",
    "    rawg[\"g\"] = rawg[\"raw_genre\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    idx_map = {cid: i for i, cid in enumerate(ids)}\n",
    "    genre_vecs = {}\n",
    "    for g, grp in rawg.groupby(\"g\"):\n",
    "        idxs = [idx_map[c] for c in grp[\"content_id\"].tolist() if c in idx_map]\n",
    "        if not idxs:\n",
    "            continue\n",
    "        v = M[idxs].mean(axis=0)\n",
    "        v = v / (np.linalg.norm(v) + 1e-12)\n",
    "        genre_vecs[g] = v\n",
    "\n",
    "    if not genre_vecs:\n",
    "        raise RuntimeError(\"ì¥ë¥´ë³„ë¡œ ë§¤ì¹­ë˜ëŠ” ì•„ì´í…œ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. genre_vecs ë¹„ì–´ìˆìŒ.\")\n",
    "\n",
    "    print(f\"âœ… ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìˆ˜: {len(genre_vecs)}\")\n",
    "\n",
    "    # 4) user_preferred_genres.csv ë¡œë¶€í„° ìœ ì €ë³„ ì„ í˜¸ ì¥ë¥´ ê°€ì ¸ì˜¤ê¸°\n",
    "    ugen = read_csv_retry(UP_GENRES)\n",
    "    if ugen is None or ugen.empty:\n",
    "        raise RuntimeError(\"user_preferred_genres.csv í•„ìš”.\")\n",
    "\n",
    "    # ì–´ë–¤ ì»¬ëŸ¼ì´ ìœ ì € ì‹ë³„ìì¸ì§€ ì¶”ë¡  (user_id / id / preference_id ì¤‘ í•˜ë‚˜)\n",
    "    col_user_id = None\n",
    "    for cand in [\"user_id\", \"id\", \"preference_id\"]:\n",
    "        if cand in ugen.columns:\n",
    "            col_user_id = cand\n",
    "            break\n",
    "    if col_user_id is None:\n",
    "        raise RuntimeError(\"user_preferred_genres.csv ì— user_id / id / preference_id ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ìœ ì € ì‹ë³„ ì»¬ëŸ¼ í•˜ë‚˜ í•„ìš”.\")\n",
    "\n",
    "    if \"genre\" not in ugen.columns:\n",
    "        raise RuntimeError(\"user_preferred_genres.csv ì— genre ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    has_username = \"username\" in ugen.columns\n",
    "\n",
    "    keep_cols = [col_user_id, \"genre\"]\n",
    "    if has_username:\n",
    "        keep_cols.append(\"username\")\n",
    "\n",
    "    ugen = ugen[keep_cols].dropna(subset=[col_user_id, \"genre\"]).copy()\n",
    "    ugen[col_user_id] = pd.to_numeric(ugen[col_user_id], errors=\"coerce\").astype(\"Int64\")\n",
    "    ugen = ugen[ugen[col_user_id].notna()].copy()\n",
    "    ugen[col_user_id] = ugen[col_user_id].astype(int)\n",
    "    ugen[\"g\"] = ugen[\"genre\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # 5) ìœ ì €ë³„ë¡œ ì„ í˜¸ ì¥ë¥´ â†’ ì¥ë¥´ ë²¡í„° í‰ê·  â†’ ìœ ì € ì„ë² ë”©\n",
    "    rows = []\n",
    "    skipped = 0\n",
    "    for uid, grp in ugen.groupby(col_user_id):\n",
    "        genres = sorted(set([g for g in grp[\"g\"].tolist() if g in genre_vecs]))\n",
    "        if not genres:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        g_vecs = [genre_vecs[g] for g in genres]\n",
    "        v_user = np.mean(np.stack(g_vecs, axis=0), axis=0)\n",
    "        v_user = v_user / (np.linalg.norm(v_user) + 1e-12)\n",
    "\n",
    "        uname = \"\"\n",
    "        if has_username:\n",
    "            uname = str(grp[\"username\"].iloc[0])\n",
    "\n",
    "        rows.append([int(uid), uname] + list(map(float, v_user)))\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"ìœ ì € ì„ë² ë”© ìƒì„± ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. user_preferred_genres.csv / ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "    dim = len(emb_cols)\n",
    "    out = pd.DataFrame(rows, columns=[\"user_id\", \"username\"] + [f\"emb_{i}\" for i in range(dim)])\n",
    "    out.sort_values(\"user_id\").to_csv(OUT_USER, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… user_embeddings.csv ì €ì¥: {OUT_USER} (users={len(out)}, skipped_users_without_valid_genres={skipped})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e5fc2",
   "metadata": {},
   "source": [
    "# ìœ ì € ì•„ì´í…œ ì„ë² ë”© top-kì¶”ì²œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91bf4798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: item_embeddings_torch.csv (items=495, dim=64)\n",
      "âœ… contents ë©”íƒ€ ë¡œë“œ: 601 rows (content_id, master_title, domain)\n",
      "âœ… recommendations_topK.csv ì €ì¥: csv ë°ì´í„°\\clean\\recommendations_topK.csv (rows=50)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#ìœ ì €/ì•„ì´í…œ ì„ë² ë”©ë§Œìœ¼ë¡œ Top-K ì¶”ì²œ (contents.csvì—ì„œ ì´ë¦„/ë„ë©”ì¸ê¹Œì§€ ë¶™ì´ê¸°)\n",
    "#ì…ë ¥:\n",
    "#  clean/user_embeddings.csv                  (user_id, username, emb_0..)\n",
    "#  clean/item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv (content_id, emb_0..)\n",
    "#  clean/contents.csv                         (content_id, master_title, domain)\n",
    "#  clean/user_preferences.csv                 (ì„ íƒ) id, username  â† ì¶”ê°€ë¡œ ì‚¬ìš©\n",
    "#ì¶œë ¥:\n",
    "#  clean/recommendations_topK.csv             (user_id, username, rank, content_id, master_title, domain, score)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "BASE = r\"csv ë°ì´í„°\\clean\"\n",
    "USER_EMB   = os.path.join(BASE, \"user_embeddings.csv\")\n",
    "ITEM_EMB_1 = os.path.join(BASE, \"item_embeddings_torch.csv\")\n",
    "ITEM_EMB_2 = os.path.join(BASE, \"item_embeddings.csv\")\n",
    "CONTENTS   = os.path.join(BASE, \"contents.csv\")\n",
    "UPREF      = os.path.join(BASE, \"user_preferences.csv\")   # â˜… ì¶”ê°€\n",
    "OUT_RECS   = os.path.join(BASE, \"recommendations_topK.csv\")\n",
    "\n",
    "TOPK = 5\n",
    "EXCLUDE_DUP_CONTENTS = True   # content_id ì¤‘ë³µí–‰ì´ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©\n",
    "ASSUME_NORMALIZED = True      # ì„ë² ë”©ì´ ì´ë¯¸ L2 ì •ê·œí™”ë˜ì–´ ìˆë‹¤ë©´ True, ì•„ë‹ˆë©´ Falseë¡œ\n",
    "\n",
    "def read_csv_retry(path, encodings=(\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"), **kw) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    last=None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kw)\n",
    "        except Exception as e:\n",
    "            last=e\n",
    "    raise last\n",
    "\n",
    "def l2norm(X: np.ndarray) -> np.ndarray:\n",
    "    return X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def _coerce_content_id_column(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"ì—¬ëŸ¬ ì´ë¦„/í˜•íƒœë¡œ ì €ì¥ëœ content_idë¥¼ ìµœëŒ€í•œ ë³µêµ¬.\"\"\"\n",
    "    df = df.copy()\n",
    "    orig_cols = list(df.columns)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    name_map = {c.lower(): c for c in df.columns}\n",
    "    for key in [\"content_id\", \"contentid\", \"content id\", \"id\", \"unnamed: 0\"]:\n",
    "        if key in name_map:\n",
    "            cand = name_map[key]\n",
    "            s = df[cand]\n",
    "            if s.dtype == object:\n",
    "                s = s.astype(str).str.strip()\n",
    "            s = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "            if s.isna().all():\n",
    "                continue\n",
    "            return s\n",
    "    raise RuntimeError(f\"content_id ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì»¬ëŸ¼: {orig_cols}\")\n",
    "\n",
    "def load_user_embeddings() -> pd.DataFrame:\n",
    "    u = read_csv_retry(USER_EMB)\n",
    "    if u is None or u.empty:\n",
    "        raise RuntimeError(\"user_embeddings.csv í•„ìš”.\")\n",
    "    u = u.copy()\n",
    "    u.columns = [c.strip() for c in u.columns]\n",
    "\n",
    "    # user_id ì •ë¦¬\n",
    "    if \"user_id\" not in u.columns:\n",
    "        if \"id\" in u.columns:\n",
    "            u = u.rename(columns={\"id\": \"user_id\"})\n",
    "        else:\n",
    "            raise RuntimeError(\"user_embeddings.csvì— user_id ì»¬ëŸ¼ í•„ìš”.\")\n",
    "    u[\"user_id\"] = pd.to_numeric(u[\"user_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    u = u[u[\"user_id\"].notna()].copy()\n",
    "    u[\"user_id\"] = u[\"user_id\"].astype(int)\n",
    "\n",
    "    # ì„ë² ë”© ì»¬ëŸ¼\n",
    "    emb_cols = [c for c in u.columns if c.lower().startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        raise RuntimeError(\"user_embeddings.csvì— emb_ë¡œ ì‹œì‘í•˜ëŠ” ì„ë² ë”© ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    for c in emb_cols:\n",
    "        u[c] = pd.to_numeric(u[c], errors=\"coerce\")\n",
    "    u.dropna(subset=emb_cols, inplace=True)\n",
    "    if not ASSUME_NORMALIZED:\n",
    "        u.loc[:, emb_cols] = l2norm(u[emb_cols].to_numpy(np.float32))\n",
    "\n",
    "    # username ì»¬ëŸ¼ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)\n",
    "    lower_cols = [x.lower() for x in u.columns]\n",
    "    if \"username\" in lower_cols:\n",
    "        uname_col = u.columns[lower_cols.index(\"username\")]\n",
    "        cols = [\"user_id\", uname_col] + emb_cols\n",
    "        out = u[cols].copy()\n",
    "        if uname_col != \"username\":\n",
    "            out = out.rename(columns={uname_col: \"username\"})\n",
    "    else:\n",
    "        cols = [\"user_id\"] + emb_cols\n",
    "        out = u[cols].copy()\n",
    "        out[\"username\"] = \"\"\n",
    "\n",
    "    # ğŸ”¹ 1ë‹¨ê³„: user_preferences.csvì—ì„œ username ë³´ê°•\n",
    "    pref = read_csv_retry(UPREF)\n",
    "    if pref is not None and not pref.empty:\n",
    "        pref = pref.copy()\n",
    "        pref.columns = [c.strip() for c in pref.columns]\n",
    "        lower = {c.lower(): c for c in pref.columns}\n",
    "\n",
    "        id_col    = lower.get(\"id\") or lower.get(\"user_id\")\n",
    "        uname_col = lower.get(\"username\") or lower.get(\"user_name\")\n",
    "\n",
    "        if id_col is not None and uname_col is not None:\n",
    "            pref = pref[[id_col, uname_col]].dropna(subset=[id_col])\n",
    "            pref[id_col] = pd.to_numeric(pref[id_col], errors=\"coerce\").astype(\"Int64\")\n",
    "            pref = pref[pref[id_col].notna()].copy()\n",
    "            pref[id_col] = pref[id_col].astype(int)\n",
    "            pref = pref.rename(columns={id_col: \"user_id\", uname_col: \"username_pref\"})\n",
    "\n",
    "            out = out.merge(pref, on=\"user_id\", how=\"left\")\n",
    "            # user_embeddingsì˜ usernameì´ ë¹„ì–´ìˆìœ¼ë©´ preferencesì˜ username_prefë¡œ ì±„ìš°ê¸°\n",
    "            out[\"username\"] = out[\"username\"].astype(str)\n",
    "            out[\"username_pref\"] = out[\"username_pref\"].astype(str)\n",
    "\n",
    "            # embeddings ìª½ usernameì´ ì—†ê±°ë‚˜ \"nan\"/\"None\" ê°™ì€ ê°’ì´ë©´ pref ê²ƒìœ¼ë¡œ ë®ì–´ì“°ê¸°\n",
    "            mask_missing = out[\"username\"].str.strip().eq(\"\") | \\\n",
    "                           out[\"username\"].str.strip().str.lower().isin([\"nan\", \"none\"])\n",
    "            out.loc[mask_missing, \"username\"] = out.loc[mask_missing, \"username_pref\"]\n",
    "            out = out.drop(columns=[\"username_pref\"])\n",
    "\n",
    "    # ğŸ”¹ 2ë‹¨ê³„: ìµœì¢…ì ìœ¼ë¡œ ë‚¨ì€ ì´ìƒê°’ ì •ë¦¬\n",
    "    out[\"username\"] = out[\"username\"].replace({np.nan: \"\"})\n",
    "    out[\"username\"] = out[\"username\"].astype(str)\n",
    "    bad = out[\"username\"].str.strip().str.lower().isin([\"nan\", \"none\"])\n",
    "    out.loc[bad, \"username\"] = \"\"\n",
    "\n",
    "    return out[[\"user_id\", \"username\"] + emb_cols]\n",
    "\n",
    "def load_item_embeddings() -> pd.DataFrame:\n",
    "    i1 = read_csv_retry(ITEM_EMB_1)\n",
    "    if i1 is not None and not i1.empty:\n",
    "        df = i1; src = os.path.basename(ITEM_EMB_1)\n",
    "    else:\n",
    "        i2 = read_csv_retry(ITEM_EMB_2)\n",
    "        if i2 is None or i2.empty:\n",
    "            raise RuntimeError(\"item_embeddings CSV í•„ìš”: item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv\")\n",
    "        df = i2; src = os.path.basename(ITEM_EMB_2)\n",
    "\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df[\"content_id\"] = _coerce_content_id_column(df)\n",
    "\n",
    "    emb_cols = [c for c in df.columns if c.lower().startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        alt = [c for c in df.columns if re.search(r\"\\d+$\", c)]\n",
    "        raise RuntimeError(f\"ì„ë² ë”© ì»¬ëŸ¼(emb_*) ì—†ìŒ. í™•ì¸ í•„ìš”. í›„ë³´: {alt}\")\n",
    "\n",
    "    for c in emb_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=emb_cols + [\"content_id\"]).copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    if EXCLUDE_DUP_CONTENTS:\n",
    "        df = df.drop_duplicates(subset=[\"content_id\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    if not ASSUME_NORMALIZED:\n",
    "        df.loc[:, emb_cols] = l2norm(df[emb_cols].to_numpy(np.float32))\n",
    "\n",
    "    print(f\"âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: {src} (items={len(df)}, dim={len(emb_cols)})\")\n",
    "    return df[[\"content_id\"] + emb_cols]\n",
    "\n",
    "def load_contents_meta() -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    contents.csvì—ì„œ content_id, master_title, domainë§Œ ë½‘ì•„ì„œ ë¦¬í„´.\n",
    "    content_idëŠ” ìœ„ì˜ _coerce_content_id_column ë¡œ í†µì¼.\n",
    "    \"\"\"\n",
    "    c = read_csv_retry(CONTENTS)\n",
    "    if c is None or c.empty:\n",
    "        print(\"âš ï¸ contents.csvë¥¼ ì°¾ì§€ ëª»í•´ ì´ë¦„/ë„ë©”ì¸ ë§¤í•‘ ì—†ì´ ì €ì¥í•©ë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "    c = c.copy()\n",
    "    c.columns = [col.strip() for col in c.columns]\n",
    "\n",
    "    # content_id ê°•ì œ ì¶”ì¶œ\n",
    "    c[\"content_id\"] = _coerce_content_id_column(c)\n",
    "    c = c[c[\"content_id\"].notna()].copy()\n",
    "    c[\"content_id\"] = c[\"content_id\"].astype(int)\n",
    "\n",
    "    # master_title / domain ì»¬ëŸ¼ ì´ë¦„(ëŒ€ì†Œë¬¸ì ë¬´ì‹œ) ì°¾ê¸°\n",
    "    lower_map = {col.lower(): col for col in c.columns}\n",
    "    title_col = lower_map.get(\"master_title\")\n",
    "    domain_col = lower_map.get(\"domain\")\n",
    "\n",
    "    if title_col is None:\n",
    "        raise RuntimeError(\"contents.csvì—ì„œ master_title ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    if domain_col is None:\n",
    "        raise RuntimeError(\"contents.csvì—ì„œ domain ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    meta = c[[\"content_id\", title_col, domain_col]].copy()\n",
    "    # ì»¬ëŸ¼ ì´ë¦„ í†µì¼\n",
    "    if title_col != \"master_title\":\n",
    "        meta = meta.rename(columns={title_col: \"master_title\"})\n",
    "    if domain_col != \"domain\":\n",
    "        meta = meta.rename(columns={domain_col: \"domain\"})\n",
    "\n",
    "    meta = meta.drop_duplicates(subset=[\"content_id\"], keep=\"first\").reset_index(drop=True)\n",
    "    print(f\"âœ… contents ë©”íƒ€ ë¡œë“œ: {len(meta)} rows (content_id, master_title, domain)\")\n",
    "    return meta\n",
    "\n",
    "def main():\n",
    "    u = load_user_embeddings()\n",
    "    i = load_item_embeddings()\n",
    "    meta = load_contents_meta()  # ì´ë¦„/ë„ë©”ì¸ ë§¤í•‘\n",
    "\n",
    "    ucols = [c for c in u.columns if c.lower().startswith(\"emb_\")]\n",
    "    icols = [c for c in i.columns if c.lower().startswith(\"emb_\")]\n",
    "\n",
    "    U = u[ucols].to_numpy(np.float32)\n",
    "    I = i[icols].to_numpy(np.float32)\n",
    "\n",
    "    # í˜¹ì‹œ ASSUME_NORMALIZED=Trueì¸ë° ì‹¤ì œë¡œ ì •ê·œí™” ì•ˆë˜ì–´ ìˆìœ¼ë©´, ì½”ì‚¬ì¸ = ë‚´ì ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë‹ˆ ë³´ì •\n",
    "    U = l2norm(U)\n",
    "    I = l2norm(I)\n",
    "\n",
    "    item_ids = i[\"content_id\"].to_numpy(int)\n",
    "    user_ids = u[\"user_id\"].to_numpy(int)\n",
    "    usernames = u[\"username\"].astype(str).to_numpy() if \"username\" in u.columns else np.array([\"\"]*len(u))\n",
    "\n",
    "    rec_rows = []\n",
    "    for r in range(U.shape[0]):\n",
    "        sims = (I @ U[r:r+1].T).reshape(-1)   # ì½”ì‚¬ì¸(ì •ê·œí™” ê°€ì •)\n",
    "        k = min(TOPK, len(sims))\n",
    "        if k == 0:\n",
    "            continue\n",
    "        part = np.argpartition(-sims, k-1)[:k]\n",
    "        order = part[np.argsort(-sims[part])]\n",
    "        top_ids = item_ids[order]\n",
    "        top_scs = sims[order]\n",
    "        uid = int(user_ids[r]); uname = usernames[r] if r < len(usernames) else \"\"\n",
    "        for rank, (cid, sc) in enumerate(zip(top_ids, top_scs), start=1):\n",
    "            rec_rows.append([uid, uname, rank, int(cid), float(sc)])\n",
    "\n",
    "    rec = pd.DataFrame(rec_rows, columns=[\"user_id\",\"username\",\"rank\",\"content_id\",\"score\"])\n",
    "\n",
    "    # contents ë©”íƒ€ì™€ merge í•´ì„œ master_title, domain ì¶”ê°€\n",
    "    if meta is not None:\n",
    "        rec = rec.merge(meta, on=\"content_id\", how=\"left\")\n",
    "        cols = [\"user_id\", \"username\", \"rank\", \"content_id\", \"master_title\", \"domain\", \"score\"]\n",
    "        cols = [c for c in cols if c in rec.columns]\n",
    "        rec = rec[cols]\n",
    "\n",
    "    rec.sort_values([\"user_id\",\"rank\"]).to_csv(OUT_RECS, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… recommendations_topK.csv ì €ì¥: {OUT_RECS} (rows={len(rec)})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976a1f1",
   "metadata": {},
   "source": [
    "# ì•„ì´í…œ ì¶”ì²œ(ì¥ë¥´ì„ë² ë”© í•„í„°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "737a16b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: item_embeddings_torch.csv (items=495, dim=64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "content_raw_genres.csv ì— content_id/raw_genre ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 421\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… recommendations_topK_genre.csv ì €ì¥: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_RECS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (rows=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 421\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[67], line 335\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    332\u001b[0m item_df, M, emb_cols \u001b[38;5;241m=\u001b[39m load_item_embeddings()\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# 2) ì½˜í…ì¸  ì¥ë¥´ raw (í•„ìˆ˜) + 'ì¥ë¥´ê°€ ìˆëŠ” ì•„ì´í…œë§Œ' í•„í„°ë§\u001b[39;00m\n\u001b[1;32m--> 335\u001b[0m gdf_raw \u001b[38;5;241m=\u001b[39m \u001b[43mload_content_genres_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ì—¬ê¸°ì„œ ì—†ìœ¼ë©´ ë°”ë¡œ ì—ëŸ¬\u001b[39;00m\n\u001b[0;32m    336\u001b[0m has_genre_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(gdf_raw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# â–¶ ì¥ë¥´ ì—†ëŠ” ì½˜í…ì¸ ëŠ” ì—¬ê¸°ì„œ ì œê±°\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[67], line 182\u001b[0m, in \u001b[0;36mload_content_genres_raw\u001b[1;34m()\u001b[0m\n\u001b[0;32m    179\u001b[0m gdf\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m gdf\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_genre\u001b[39m\u001b[38;5;124m\"\u001b[39m}\u001b[38;5;241m.\u001b[39missubset(\u001b[38;5;28mset\u001b[39m(gdf\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_raw_genres.csv ì— content_id/raw_genre ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    184\u001b[0m gdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(gdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInt64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    185\u001b[0m gdf \u001b[38;5;241m=\u001b[39m gdf[gdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: content_raw_genres.csv ì— content_id/raw_genre ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ì¥ë¥´ ê¸°ë°˜ ìœ ì €-ì•„ì´í…œ Top-K ì¶”ì²œ (user_embeddings.csv ì‚¬ìš© X)\n",
    "# â–¶ ì¥ë¥´ê°€ ì—†ëŠ” ì½˜í…ì¸ ëŠ” ì¶”ì²œ í›„ë³´ì—ì„œ ì œì™¸\n",
    "#\n",
    "# ì…ë ¥:\n",
    "#   clean/item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv (content_id, emb_0..)\n",
    "#   clean/contents.csv                        (ì„ íƒ) content_id + ì œëª©/ë„ë©”ì¸ ì»¬ëŸ¼\n",
    "#   clean/content_raw_genres.csv              (í•„ìˆ˜) content_id, raw_genre\n",
    "#   clean/user_preferred_genres.csv           (ìœ ì €ë³„ ì„ í˜¸ ì¥ë¥´)\n",
    "#   clean/user_preferences.csv                (ì„ íƒ) id, username ë“± (ìˆìœ¼ë©´ username ë§¤í•‘)\n",
    "#\n",
    "# ì¶œë ¥:\n",
    "#   clean/recommendations_topK_genre.csv\n",
    "#   (user_id, username, rank, content_id, content_name, domain, genres_str, score)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "\n",
    "BASE        = r\"csv ë°ì´í„°\\clean\"\n",
    "ITEM_EMB_1  = os.path.join(BASE, \"item_embeddings_torch.csv\")\n",
    "ITEM_EMB_2  = os.path.join(BASE, \"item_embeddings.csv\")\n",
    "CONTENTS    = os.path.join(BASE, \"contents.csv\")             # ì´ë¦„/ë„ë©”ì¸ ë©”íƒ€\n",
    "RAW_GENRES  = os.path.join(BASE, \"content_raw_genres.csv\")   # ì¥ë¥´ìš© ë©”íƒ€\n",
    "UPREF       = os.path.join(BASE, \"user_preferences.csv\")     # (ì„ íƒ) user_id, username\n",
    "UP_GENRES   = os.path.join(BASE, \"user_preferred_genres.csv\")# ìœ ì € ì„ í˜¸ ì¥ë¥´\n",
    "OUT_RECS    = os.path.join(BASE, \"recommendations_topK_genre.csv\")\n",
    "\n",
    "TOPK = 5\n",
    "EXCLUDE_DUP_CONTENTS = True   # content_id ì¤‘ë³µí–‰ì´ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©\n",
    "ASSUME_NORMALIZED = True      # ì„ë² ë”©ì´ ì´ë¯¸ L2 ì •ê·œí™”ë˜ì–´ ìˆë‹¤ë©´ True, ì•„ë‹ˆë©´ Falseë¡œ\n",
    "\n",
    "def read_csv_retry(path, encodings=(\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"), **kw) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    last = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kw)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "def l2norm(X: np.ndarray) -> np.ndarray:\n",
    "    return X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def _coerce_content_id_column(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"ì—¬ëŸ¬ ì´ë¦„/í˜•íƒœë¡œ ì €ì¥ëœ content_idë¥¼ ìµœëŒ€í•œ ë³µêµ¬.\"\"\"\n",
    "    df = df.copy()\n",
    "    orig_cols = list(df.columns)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    name_map = {c.lower(): c for c in df.columns}\n",
    "    for key in [\"content_id\", \"contentid\", \"content id\", \"id\", \"unnamed: 0\"]:\n",
    "        if key in name_map:\n",
    "            cand = name_map[key]\n",
    "            s = df[cand]\n",
    "            if s.dtype == object:\n",
    "                s = s.astype(str).str.strip()\n",
    "            s = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "            if s.isna().all():\n",
    "                continue\n",
    "            return s\n",
    "    raise RuntimeError(f\"content_id ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì»¬ëŸ¼: {orig_cols}\")\n",
    "\n",
    "def load_item_embeddings() -> Tuple[pd.DataFrame, np.ndarray, List[str]]:\n",
    "    \"\"\"ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ (content_id + emb_*), numpy í–‰ë ¬, emb ì»¬ëŸ¼ ì´ë¦„ ë¦¬í„´\"\"\"\n",
    "    i1 = read_csv_retry(ITEM_EMB_1)\n",
    "    if i1 is not None and not i1.empty:\n",
    "        df = i1; src = os.path.basename(ITEM_EMB_1)\n",
    "    else:\n",
    "        i2 = read_csv_retry(ITEM_EMB_2)\n",
    "        if i2 is None or i2.empty:\n",
    "            raise RuntimeError(\"item_embeddings CSV í•„ìš”: item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv\")\n",
    "        df = i2; src = os.path.basename(ITEM_EMB_2)\n",
    "\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df[\"content_id\"] = _coerce_content_id_column(df)\n",
    "\n",
    "    emb_cols = [c for c in df.columns if c.lower().startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        alt = [c for c in df.columns if re.search(r\"\\d+$\", c)]\n",
    "        raise RuntimeError(f\"ì„ë² ë”© ì»¬ëŸ¼(emb_*) ì—†ìŒ. í™•ì¸ í•„ìš”. í›„ë³´: {alt}\")\n",
    "\n",
    "    for c in emb_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=emb_cols + [\"content_id\"]).copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    if EXCLUDE_DUP_CONTENTS:\n",
    "        df = df.drop_duplicates(subset=[\"content_id\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    if not ASSUME_NORMALIZED:\n",
    "        df.loc[:, emb_cols] = l2norm(df[emb_cols].to_numpy(np.float32))\n",
    "\n",
    "    M = df[emb_cols].to_numpy(np.float32)\n",
    "    M = l2norm(M)  # ì•ˆì „í•˜ê²Œ í•œ ë²ˆ ë” ì •ê·œí™”\n",
    "    df.loc[:, emb_cols] = M\n",
    "\n",
    "    print(f\"âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: {src} (items={len(df)}, dim={len(emb_cols)})\")\n",
    "    return df[[\"content_id\"] + emb_cols], M, emb_cols\n",
    "\n",
    "def load_content_names() -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    contents.csv ì—ì„œ content_id, content_name, domain ì„ ë½‘ì•„ì„œ ë¦¬í„´.\n",
    "    content_name í›„ë³´ ì»¬ëŸ¼:\n",
    "      content_name > master_title > original_title > title > name\n",
    "    domain ì»¬ëŸ¼ì€ 'domain'(ëŒ€ì†Œë¬¸ì ë¬´ì‹œ) ì‚¬ìš©.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(CONTENTS):\n",
    "        print(\"â„¹ï¸ contents.csv ì—†ìŒ â†’ ì´ë¦„/ë„ë©”ì¸ ì—†ì´ ì§„í–‰\")\n",
    "        return None\n",
    "\n",
    "    meta = read_csv_retry(CONTENTS)\n",
    "    if meta is None or meta.empty:\n",
    "        print(\"â„¹ï¸ contents.csv ë¹„ì–´ìˆìŒ â†’ ì´ë¦„/ë„ë©”ì¸ ì—†ì´ ì§„í–‰\")\n",
    "        return None\n",
    "\n",
    "    meta = meta.copy()\n",
    "    meta.columns = [c.strip() for c in meta.columns]\n",
    "\n",
    "    if \"content_id\" not in meta.columns:\n",
    "        print(\"â„¹ï¸ contents.csv ì— content_id ì—†ìŒ â†’ ì´ë¦„/ë„ë©”ì¸ ë¶™ì´ê¸° ìƒëµ\")\n",
    "        return None\n",
    "    meta[\"content_id\"] = pd.to_numeric(meta[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    meta = meta[meta[\"content_id\"].notna()].copy()\n",
    "    meta[\"content_id\"] = meta[\"content_id\"].astype(int)\n",
    "\n",
    "    lower_map = {c.lower(): c for c in meta.columns}\n",
    "\n",
    "    # ì´ë¦„ ì»¬ëŸ¼\n",
    "    name_key = None\n",
    "    for key in [\"content_name\", \"master_title\", \"original_title\", \"title\", \"name\"]:\n",
    "        if key in lower_map:\n",
    "            name_key = lower_map[key]\n",
    "            break\n",
    "\n",
    "    # ë„ë©”ì¸ ì»¬ëŸ¼\n",
    "    domain_key = lower_map.get(\"domain\")\n",
    "\n",
    "    if name_key is None and domain_key is None:\n",
    "        print(\"â„¹ï¸ contents.csv ì—ì„œ ì´ë¦„/ë„ë©”ì¸ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í•¨ â†’ ë©”íƒ€ ì—†ì´ ì§„í–‰\")\n",
    "        return None\n",
    "\n",
    "    use_cols = [\"content_id\"]\n",
    "    rename_map = {}\n",
    "    if name_key is not None:\n",
    "        use_cols.append(name_key)\n",
    "        rename_map[name_key] = \"content_name\"\n",
    "    if domain_key is not None:\n",
    "        use_cols.append(domain_key)\n",
    "        rename_map[domain_key] = \"domain\"\n",
    "\n",
    "    meta = meta[use_cols].drop_duplicates(subset=[\"content_id\"], keep=\"first\")\n",
    "    meta = meta.rename(columns=rename_map)\n",
    "\n",
    "    used_cols_str = \", \".join([c for c in [\"content_name\", \"domain\"] if c in meta.columns])\n",
    "    print(f\"âœ… contents ë©”íƒ€ ì‚¬ìš©: {used_cols_str}\")\n",
    "    return meta\n",
    "\n",
    "def load_content_genres_raw() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    content_raw_genres.csv ì—ì„œ\n",
    "      content_id, raw_genre, g_norm(lower)\n",
    "    ë¥¼ ë¦¬í„´.\n",
    "    (ì—†ìœ¼ë©´ ì¶”ì²œ ìì²´ë¥¼ ì¤‘ë‹¨)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(RAW_GENRES):\n",
    "        raise RuntimeError(\"content_raw_genres.csv ê°€ ì—†ìŠµë‹ˆë‹¤. ì¥ë¥´ ê¸°ë°˜ ì¶”ì²œ ë¶ˆê°€.\")\n",
    "\n",
    "    gdf = read_csv_retry(RAW_GENRES)\n",
    "    if gdf is None or gdf.empty:\n",
    "        raise RuntimeError(\"content_raw_genres.csv ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¥ë¥´ ê¸°ë°˜ ì¶”ì²œ ë¶ˆê°€.\")\n",
    "\n",
    "    gdf = gdf.copy()\n",
    "    gdf.columns = [c.strip() for c in gdf.columns]\n",
    "\n",
    "    if not {\"content_id\", \"raw_genre\"}.issubset(set(gdf.columns)):\n",
    "        raise RuntimeError(\"content_raw_genres.csv ì— content_id/raw_genre ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    gdf[\"content_id\"] = pd.to_numeric(gdf[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    gdf = gdf[gdf[\"content_id\"].notna()].copy()\n",
    "    gdf[\"content_id\"] = gdf[\"content_id\"].astype(int)\n",
    "\n",
    "    gdf[\"raw_genre\"] = gdf[\"raw_genre\"].astype(str).str.strip()\n",
    "    gdf = gdf[gdf[\"raw_genre\"] != \"\"].copy()\n",
    "\n",
    "    gdf[\"g_norm\"] = gdf[\"raw_genre\"].str.lower().str.strip()\n",
    "\n",
    "    if gdf.empty:\n",
    "        raise RuntimeError(\"content_raw_genres.csv ì— ìœ íš¨í•œ ì¥ë¥´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¥ë¥´ ê¸°ë°˜ ì¶”ì²œ ë¶ˆê°€.\")\n",
    "\n",
    "    print(f\"âœ… ì½˜í…ì¸  ì¥ë¥´ raw ë¡œë“œ: rows={len(gdf)}, unique_genres={gdf['g_norm'].nunique()}\")\n",
    "    return gdf[[\"content_id\", \"raw_genre\", \"g_norm\"]]\n",
    "\n",
    "def build_genre_centroids(\n",
    "    item_df: pd.DataFrame,\n",
    "    M: np.ndarray,\n",
    "    gdf_raw: pd.DataFrame\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    ê° ì¥ë¥´(g_norm)ë³„ë¡œ í•´ë‹¹ ì¥ë¥´ë¥¼ ê°€ì§„ ì•„ì´í…œ ì„ë² ë”© í‰ê·  â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ.\n",
    "    (item_df / M ì€ ì´ë¯¸ 'ì¥ë¥´ê°€ ìˆëŠ” ì•„ì´í…œ'ë§Œ ë‚¨ê²¨ì§„ ìƒíƒœë¼ê³  ê°€ì •)\n",
    "    \"\"\"\n",
    "    idx_map = {cid: i for i, cid in enumerate(item_df[\"content_id\"].tolist())}\n",
    "    genre_vecs: Dict[str, np.ndarray] = {}\n",
    "    for g, grp in gdf_raw.groupby(\"g_norm\"):\n",
    "        idxs = [idx_map[c] for c in grp[\"content_id\"].tolist() if c in idx_map]\n",
    "        if not idxs:\n",
    "            continue\n",
    "        v = M[idxs].mean(axis=0)\n",
    "        v = v / (np.linalg.norm(v) + 1e-12)\n",
    "        genre_vecs[g] = v\n",
    "    print(f\"âœ… ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìˆ˜: {len(genre_vecs)}\")\n",
    "    return genre_vecs\n",
    "\n",
    "def build_genres_agg(gdf_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    content_id ë³„ genres_str ('ì¥ë¥´1|ì¥ë¥´2|...') ìƒì„±.\n",
    "    \"\"\"\n",
    "    agg = (\n",
    "        gdf_raw.groupby(\"content_id\")[\"raw_genre\"]\n",
    "        .apply(lambda x: \"|\".join(pd.unique(x.tolist())))\n",
    "        .reset_index()\n",
    "        .rename(columns={\"raw_genre\": \"genres_str\"})\n",
    "    )\n",
    "    print(f\"âœ… ì¥ë¥´ ì§‘ê³„ ë§¤í•‘: {len(agg)}ê°œ content_id\")\n",
    "    return agg\n",
    "\n",
    "def load_user_genre_prefs() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    user_preferred_genres.csv (+ optional user_preferences.csv)ë¥¼ ì½ì–´ì„œ\n",
    "      user_id, username, genres(list of str, g_norm) ë¥¼ ë¦¬í„´.\n",
    "    \"\"\"\n",
    "    upg = read_csv_retry(UP_GENRES)\n",
    "    if upg is None or upg.empty:\n",
    "        raise RuntimeError(\"user_preferred_genres.csv í•„ìš”.\")\n",
    "\n",
    "    upg = upg.copy()\n",
    "    upg.columns = [c.strip() for c in upg.columns]\n",
    "\n",
    "    # ì¥ë¥´ ì»¬ëŸ¼ ì°¾ê¸°\n",
    "    lower_map = {c.lower(): c for c in upg.columns}\n",
    "    genre_col = None\n",
    "    for key in [\"genre\", \"raw_genre\"]:\n",
    "        if key in lower_map:\n",
    "            genre_col = lower_map[key]\n",
    "            break\n",
    "    if genre_col is None:\n",
    "        raise RuntimeError(\"user_preferred_genres.csv ì— genre/raw_genre ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    upg[\"genre_norm\"] = upg[genre_col].astype(str).str.strip().str.lower()\n",
    "    upg = upg[upg[\"genre_norm\"] != \"\"].copy()\n",
    "\n",
    "    # 1) user_preferences.csv ê¸°ë°˜ (preference_id -> id, username)\n",
    "    upref = read_csv_retry(UPREF)\n",
    "    rows = []\n",
    "\n",
    "    if upref is not None and not upref.empty and \\\n",
    "       \"id\" in [c.lower() for c in upref.columns] and \\\n",
    "       \"preference_id\" in [c.lower() for c in upg.columns]:\n",
    "\n",
    "        upref = upref.copy()\n",
    "        upref.columns = [c.strip() for c in upref.columns]\n",
    "        low_map_pref = {c.lower(): c for c in upref.columns}\n",
    "        id_col = low_map_pref.get(\"id\")\n",
    "        username_col = next((low_map_pref[c] for c in low_map_pref if c in [\"username\", \"user_name\"]), None)\n",
    "\n",
    "        low_map_upg = {c.lower(): c for c in upg.columns}\n",
    "        pref_id_col = low_map_upg[\"preference_id\"]\n",
    "\n",
    "        upg[pref_id_col] = pd.to_numeric(upg[pref_id_col], errors=\"coerce\").astype(\"Int64\")\n",
    "        upg = upg[upg[pref_id_col].notna()].copy()\n",
    "\n",
    "        for _, ur in upref.iterrows():\n",
    "            pid = ur[id_col]\n",
    "            try:\n",
    "                pid_int = int(pid)\n",
    "            except Exception:\n",
    "                continue\n",
    "            uname = str(ur[username_col]) if username_col else \"\"\n",
    "            g_list = upg.loc[upg[pref_id_col] == pid_int, \"genre_norm\"].tolist()\n",
    "            if not g_list:\n",
    "                continue\n",
    "            g_list = sorted(set(g_list))\n",
    "            rows.append([pid_int, uname, g_list])\n",
    "    else:\n",
    "        # 2) user_preferred_genres.csv ì•ˆì—ì„œ user_id / username ì°¾ê¸°\n",
    "        low_map = {c.lower(): c for c in upg.columns}\n",
    "        user_key = None\n",
    "        for key in [\"user_id\", \"userid\", \"user id\", \"id\"]:\n",
    "            if key in low_map:\n",
    "                user_key = low_map[key]\n",
    "                break\n",
    "        username_key = None\n",
    "        for key in [\"username\", \"user_name\"]:\n",
    "            if key in low_map:\n",
    "                username_key = low_map[key]\n",
    "                break\n",
    "\n",
    "        if user_key is not None:\n",
    "            upg[user_key] = pd.to_numeric(upg[user_key], errors=\"coerce\").astype(\"Int64\")\n",
    "            upg = upg[upg[user_key].notna()].copy()\n",
    "            upg[user_key] = upg[user_key].astype(int)\n",
    "\n",
    "            for uid, grp in upg.groupby(user_key):\n",
    "                uname = \"\"\n",
    "                if username_key:\n",
    "                    uname = str(grp[username_key].iloc[0])\n",
    "                g_list = sorted(set(grp[\"genre_norm\"].tolist()))\n",
    "                rows.append([int(uid), uname, g_list])\n",
    "        elif username_key is not None:\n",
    "            # usernameë§Œ ìˆì„ ë•Œ: ë‚´ë¶€ user_idë¥¼ 1,2,3...ìœ¼ë¡œ í• ë‹¹\n",
    "            for new_id, (uname, grp) in enumerate(upg.groupby(username_key), start=1):\n",
    "                g_list = sorted(set(grp[\"genre_norm\"].tolist()))\n",
    "                rows.append([new_id, str(uname), g_list])\n",
    "        else:\n",
    "            raise RuntimeError(\"user_preferred_genres.csv ì— user_idë‚˜ username ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"ìœ ì € ì„ í˜¸ ì¥ë¥´ ì •ë³´ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    prefs = pd.DataFrame(rows, columns=[\"user_id\", \"username\", \"genres\"])\n",
    "    print(f\"âœ… ìœ ì € ì„ í˜¸ ì¥ë¥´ ë¡œë“œ: users={len(prefs)}\")\n",
    "    return prefs\n",
    "\n",
    "def main():\n",
    "    # 1) ì•„ì´í…œ ì„ë² ë”©\n",
    "    item_df, M, emb_cols = load_item_embeddings()\n",
    "\n",
    "    # 2) ì½˜í…ì¸  ì¥ë¥´ raw (í•„ìˆ˜) + 'ì¥ë¥´ê°€ ìˆëŠ” ì•„ì´í…œë§Œ' í•„í„°ë§\n",
    "    gdf_raw = load_content_genres_raw()  # ì—¬ê¸°ì„œ ì—†ìœ¼ë©´ ë°”ë¡œ ì—ëŸ¬\n",
    "    has_genre_ids = set(gdf_raw[\"content_id\"].unique())\n",
    "\n",
    "    # â–¶ ì¥ë¥´ ì—†ëŠ” ì½˜í…ì¸ ëŠ” ì—¬ê¸°ì„œ ì œê±°\n",
    "    before_cnt = len(item_df)\n",
    "    mask = item_df[\"content_id\"].isin(has_genre_ids)\n",
    "    item_df = item_df[mask].reset_index(drop=True)\n",
    "    M = M[mask.to_numpy()]  # ì„ë² ë”© í–‰ë ¬ë„ ê°™ì´ í•„í„°ë§\n",
    "    after_cnt = len(item_df)\n",
    "    print(f\"âœ… ì¥ë¥´ê°€ ìˆëŠ” ì•„ì´í…œë§Œ ìœ ì§€: {before_cnt} â†’ {after_cnt}\")\n",
    "\n",
    "    if after_cnt == 0:\n",
    "        raise RuntimeError(\"ì¥ë¥´ê°€ ìˆëŠ” ì•„ì´í…œì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤. ì¶”ì²œ ë¶ˆê°€.\")\n",
    "\n",
    "    item_ids = item_df[\"content_id\"].to_numpy(int)\n",
    "\n",
    "    # 3) ì¥ë¥´ ì§‘ê³„ + ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ\n",
    "    genres_agg = build_genres_agg(gdf_raw)\n",
    "    genre_vecs = build_genre_centroids(item_df, M, gdf_raw)\n",
    "\n",
    "    # 4) ìœ ì €ë³„ ì„ í˜¸ ì¥ë¥´\n",
    "    prefs = load_user_genre_prefs()\n",
    "\n",
    "    # 5) content_name/domain ë¶™ì¼ ë©”íƒ€\n",
    "    meta_name = load_content_names()\n",
    "\n",
    "    # 6) ê¸€ë¡œë²Œ í‰ê·  ë²¡í„° (fallback) - ì´ë¯¸ ì¥ë¥´ ìˆëŠ” ì•„ì´í…œë§Œ ë‚¨ì€ ìƒíƒœ\n",
    "    v_global = M.mean(axis=0)\n",
    "    v_global = v_global / (np.linalg.norm(v_global) + 1e-12)\n",
    "\n",
    "    rec_rows = []\n",
    "    for _, ur in prefs.iterrows():\n",
    "        uid = int(ur[\"user_id\"])\n",
    "        uname = str(ur.get(\"username\", \"\") or \"\")\n",
    "        g_list: List[str] = ur[\"genres\"] or []\n",
    "\n",
    "        # ì´ ìœ ì €ì˜ ì„ í˜¸ ì¥ë¥´ ì¤‘ ì„¼íŠ¸ë¡œì´ë“œê°€ ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì‚¬ìš©\n",
    "        g_match = [g for g in g_list if g in genre_vecs]\n",
    "        if g_match:\n",
    "            g_vecs = np.stack([genre_vecs[g] for g in g_match], axis=0)\n",
    "            v_user = g_vecs.mean(axis=0)\n",
    "            v_user = v_user / (np.linalg.norm(v_user) + 1e-12)\n",
    "        else:\n",
    "            # í•´ë‹¹ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê¸€ë¡œë²Œ í‰ê·  (ê·¸ë˜ë„ í›„ë³´ëŠ” ì¥ë¥´ ìˆëŠ” ì•„ì´í…œë§Œ)\n",
    "            v_user = v_global\n",
    "\n",
    "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (M, v_user ëª¨ë‘ ì •ê·œí™”ëœ ìƒíƒœ)\n",
    "        sims = M @ v_user.reshape(-1, 1)\n",
    "        sims = sims.reshape(-1)\n",
    "\n",
    "        k = min(TOPK, len(sims))\n",
    "        if k == 0:\n",
    "            continue\n",
    "        part = np.argpartition(-sims, k-1)[:k]\n",
    "        order = part[np.argsort(-sims[part])]\n",
    "        top_ids = item_ids[order]\n",
    "        top_scs = sims[order]\n",
    "\n",
    "        for rank, (cid, sc) in enumerate(zip(top_ids, top_scs), start=1):\n",
    "            rec_rows.append([uid, uname, rank, int(cid), float(sc)])\n",
    "\n",
    "    rec = pd.DataFrame(rec_rows, columns=[\"user_id\", \"username\", \"rank\", \"content_id\", \"score\"])\n",
    "\n",
    "    # 7) ì´ë¦„ + ë„ë©”ì¸ ë¶™ì´ê¸°\n",
    "    if meta_name is not None:\n",
    "        rec = rec.merge(meta_name, on=\"content_id\", how=\"left\")\n",
    "\n",
    "    # 8) ì¥ë¥´ ë¶™ì´ê¸° (ì§‘ê³„)\n",
    "    if genres_agg is not None:\n",
    "        rec = rec.merge(genres_agg, on=\"content_id\", how=\"left\")\n",
    "\n",
    "    # 9) ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬\n",
    "    cols = [\"user_id\", \"username\", \"rank\", \"content_id\"]\n",
    "    if \"content_name\" in rec.columns:\n",
    "        cols.append(\"content_name\")\n",
    "    if \"domain\" in rec.columns:\n",
    "        cols.append(\"domain\")\n",
    "    if \"genres_str\" in rec.columns:\n",
    "        cols.append(\"genres_str\")\n",
    "    cols.append(\"score\")\n",
    "    rec = rec[cols]\n",
    "\n",
    "    rec.sort_values([\"user_id\", \"rank\"]).to_csv(OUT_RECS, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… recommendations_topK_genre.csv ì €ì¥: {OUT_RECS} (rows={len(rec)})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5de218",
   "metadata": {},
   "source": [
    "# ì¥ë¥´ê¸°ë°˜ ì¶”ì²œì¸ë° ìµœì‹ ì„± í•„í„° ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a83791b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: item_embeddings_torch.csv (items=495, dim=64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "content_raw_genres.csv ì— content_id/raw_genre ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 491\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… recommendations_topK_genre.csv ì €ì¥: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_RECS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (rows=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[68], line 394\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    391\u001b[0m item_df, M, emb_cols \u001b[38;5;241m=\u001b[39m load_item_embeddings()\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m# 2) ì½˜í…ì¸  ì¥ë¥´ raw (í•„ìˆ˜)\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m gdf_raw \u001b[38;5;241m=\u001b[39m \u001b[43mload_content_genres_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ì—¬ê¸°ì„œ ì—†ìœ¼ë©´ ë°”ë¡œ ì—ëŸ¬\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# 2-1) created_at ê¸°ì¤€ìœ¼ë¡œ 2020ë…„ ì´í›„ ì½˜í…ì¸ ë§Œ í—ˆìš© (ì˜µì…˜)\u001b[39;00m\n\u001b[0;32m    397\u001b[0m allowed_ids \u001b[38;5;241m=\u001b[39m load_allowed_ids_by_created_at()  \u001b[38;5;66;03m# ì—†ìœ¼ë©´ None\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[68], line 237\u001b[0m, in \u001b[0;36mload_content_genres_raw\u001b[1;34m()\u001b[0m\n\u001b[0;32m    234\u001b[0m gdf\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m gdf\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_genre\u001b[39m\u001b[38;5;124m\"\u001b[39m}\u001b[38;5;241m.\u001b[39missubset(\u001b[38;5;28mset\u001b[39m(gdf\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[1;32m--> 237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_raw_genres.csv ì— content_id/raw_genre ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    239\u001b[0m gdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(gdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInt64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m gdf \u001b[38;5;241m=\u001b[39m gdf[gdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: content_raw_genres.csv ì— content_id/raw_genre ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ì¥ë¥´ ê¸°ë°˜ ìœ ì €-ì•„ì´í…œ Top-K ì¶”ì²œ (user_embeddings.csv ì‚¬ìš© X)\n",
    "# â–¶ ì¥ë¥´ê°€ ì—†ëŠ” ì½˜í…ì¸ ëŠ” ì¶”ì²œ í›„ë³´ì—ì„œ ì œì™¸\n",
    "# â–¶ contents.csvì˜ created_at ê¸°ì¤€ìœ¼ë¡œ 2020ë…„ ì´í›„ ì½˜í…ì¸ ë§Œ ì‚¬ìš©\n",
    "#\n",
    "# ì…ë ¥:\n",
    "#   clean/item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv (content_id, emb_0..)\n",
    "#   clean/contents.csv                        (ì„ íƒ) content_id + ì œëª©/ë„ë©”ì¸/created_at ì»¬ëŸ¼\n",
    "#   clean/content_raw_genres.csv              (í•„ìˆ˜) content_id, raw_genre\n",
    "#   clean/user_preferred_genres.csv           (ìœ ì €ë³„ ì„ í˜¸ ì¥ë¥´)\n",
    "#   clean/user_preferences.csv                (ì„ íƒ) id, username ë“± (ìˆìœ¼ë©´ username ë§¤í•‘)\n",
    "#\n",
    "# ì¶œë ¥:\n",
    "#   clean/recommendations_topK_genre.csv\n",
    "#   (user_id, username, rank, content_id, content_name, domain, genres_str, score)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Dict, Tuple, Set\n",
    "\n",
    "BASE        = r\"csv ë°ì´í„°\\clean\"\n",
    "ITEM_EMB_1  = os.path.join(BASE, \"item_embeddings_torch.csv\")\n",
    "ITEM_EMB_2  = os.path.join(BASE, \"item_embeddings.csv\")\n",
    "CONTENTS    = os.path.join(BASE, \"contents.csv\")             # ì´ë¦„/ë„ë©”ì¸/created_at ë©”íƒ€\n",
    "RAW_GENRES  = os.path.join(BASE, \"content_raw_genres.csv\")   # ì¥ë¥´ ë©”íƒ€\n",
    "UPREF       = os.path.join(BASE, \"user_preferences.csv\")     # (ì„ íƒ) user_id, username\n",
    "UP_GENRES   = os.path.join(BASE, \"user_preferred_genres.csv\")# ìœ ì € ì„ í˜¸ ì¥ë¥´\n",
    "OUT_RECS    = os.path.join(BASE, \"recommendations_topK_genre_2020.csv\")\n",
    "\n",
    "TOPK = 5\n",
    "EXCLUDE_DUP_CONTENTS = True   # content_id ì¤‘ë³µí–‰ì´ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©\n",
    "ASSUME_NORMALIZED    = True   # ì„ë² ë”©ì´ ì´ë¯¸ L2 ì •ê·œí™”ë˜ì–´ ìˆë‹¤ë©´ True, ì•„ë‹ˆë©´ Falseë¡œ\n",
    "MIN_YEAR_CREATED     = 2020   # created_at ê¸°ì¤€ í•„í„° (2020ë…„ ì´í›„ë§Œ ì‚¬ìš©)\n",
    "\n",
    "\n",
    "def read_csv_retry(path, encodings=(\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"), **kw) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    last = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kw)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "\n",
    "def l2norm(X: np.ndarray) -> np.ndarray:\n",
    "    return X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "\n",
    "def _coerce_content_id_column(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"ì—¬ëŸ¬ ì´ë¦„/í˜•íƒœë¡œ ì €ì¥ëœ content_idë¥¼ ìµœëŒ€í•œ ë³µêµ¬.\"\"\"\n",
    "    df = df.copy()\n",
    "    orig_cols = list(df.columns)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    name_map = {c.lower(): c for c in df.columns}\n",
    "    for key in [\"content_id\", \"contentid\", \"content id\", \"id\", \"unnamed: 0\"]:\n",
    "        if key in name_map:\n",
    "            cand = name_map[key]\n",
    "            s = df[cand]\n",
    "            if s.dtype == object:\n",
    "                s = s.astype(str).str.strip()\n",
    "            s = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "            if s.isna().all():\n",
    "                continue\n",
    "            return s\n",
    "    raise RuntimeError(f\"content_id ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì»¬ëŸ¼: {orig_cols}\")\n",
    "\n",
    "\n",
    "def load_allowed_ids_by_created_at(min_year: int = MIN_YEAR_CREATED) -> Optional[Set[int]]:\n",
    "    \"\"\"\n",
    "    contents.csv ì˜ created_at ì»¬ëŸ¼ì„ ì´ìš©í•´ì„œ\n",
    "    ì§€ì • ì—°ë„ ì´ìƒ(min_year)ì¸ content_id ì§‘í•©ì„ ë¦¬í„´.\n",
    "    - contents.csv / created_at / content_id ê°€ ì—†ìœ¼ë©´ None ë¦¬í„´ (í•„í„° ë¯¸ì ìš©)\n",
    "    - ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì½˜í…ì¸ ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ RuntimeError ë°œìƒ\n",
    "    \"\"\"\n",
    "    if not os.path.exists(CONTENTS):\n",
    "        print(\"â„¹ï¸ contents.csv ì—†ìŒ â†’ created_at í•„í„° ìƒëµ\")\n",
    "        return None\n",
    "\n",
    "    df = read_csv_retry(CONTENTS)\n",
    "    if df is None or df.empty:\n",
    "        print(\"â„¹ï¸ contents.csv ë¹„ì–´ìˆìŒ â†’ created_at í•„í„° ìƒëµ\")\n",
    "        return None\n",
    "\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    if \"created_at\" not in df.columns:\n",
    "        print(\"â„¹ï¸ contents.csv ì— created_at ì»¬ëŸ¼ ì—†ìŒ â†’ created_at í•„í„° ìƒëµ\")\n",
    "        return None\n",
    "\n",
    "    # content_id ì¶”ì¶œ\n",
    "    try:\n",
    "        df[\"content_id\"] = _coerce_content_id_column(df)\n",
    "    except Exception as e:\n",
    "        print(\"â„¹ï¸ contents.csv ì—ì„œ content_id ì¶”ì¶œ ì‹¤íŒ¨ â†’ created_at í•„í„° ìƒëµ:\", e)\n",
    "        return None\n",
    "\n",
    "    df = df[df[\"content_id\"].notna()].copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    # created_at íŒŒì‹±\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\")\n",
    "    df = df[df[\"created_at\"].notna()].copy()\n",
    "\n",
    "    # min_year ì´ìƒë§Œ ë‚¨ê¸°ê¸°\n",
    "    df = df[df[\"created_at\"].dt.year >= min_year]\n",
    "    allowed_ids = set(df[\"content_id\"].tolist())\n",
    "\n",
    "    print(f\"âœ… created_at >= {min_year} ì¸ ì½˜í…ì¸  ìˆ˜: {len(allowed_ids)}\")\n",
    "    if not allowed_ids:\n",
    "        raise RuntimeError(f\"created_at >= {min_year} ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    return allowed_ids\n",
    "\n",
    "\n",
    "def load_item_embeddings() -> Tuple[pd.DataFrame, np.ndarray, List[str]]:\n",
    "    \"\"\"ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ (content_id + emb_*), numpy í–‰ë ¬, emb ì»¬ëŸ¼ ì´ë¦„ ë¦¬í„´\"\"\"\n",
    "    i1 = read_csv_retry(ITEM_EMB_1)\n",
    "    if i1 is not None and not i1.empty:\n",
    "        df = i1; src = os.path.basename(ITEM_EMB_1)\n",
    "    else:\n",
    "        i2 = read_csv_retry(ITEM_EMB_2)\n",
    "        if i2 is None or i2.empty:\n",
    "            raise RuntimeError(\"item_embeddings CSV í•„ìš”: item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv\")\n",
    "        df = i2; src = os.path.basename(ITEM_EMB_2)\n",
    "\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df[\"content_id\"] = _coerce_content_id_column(df)\n",
    "\n",
    "    emb_cols = [c for c in df.columns if c.lower().startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        alt = [c for c in df.columns if re.search(r\"\\d+$\", c)]\n",
    "        raise RuntimeError(f\"ì„ë² ë”© ì»¬ëŸ¼(emb_*) ì—†ìŒ. í™•ì¸ í•„ìš”. í›„ë³´: {alt}\")\n",
    "\n",
    "    for c in emb_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=emb_cols + [\"content_id\"]).copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    if EXCLUDE_DUP_CONTENTS:\n",
    "        df = df.drop_duplicates(subset=[\"content_id\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    if not ASSUME_NORMALIZED:\n",
    "        df.loc[:, emb_cols] = l2norm(df[emb_cols].to_numpy(np.float32))\n",
    "\n",
    "    M = df[emb_cols].to_numpy(np.float32)\n",
    "    M = l2norm(M)  # ì•ˆì „í•˜ê²Œ í•œ ë²ˆ ë” ì •ê·œí™”\n",
    "    df.loc[:, emb_cols] = M\n",
    "\n",
    "    print(f\"âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: {src} (items={len(df)}, dim={len(emb_cols)})\")\n",
    "    return df[[\"content_id\"] + emb_cols], M, emb_cols\n",
    "\n",
    "\n",
    "def load_content_names() -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    contents.csv ì—ì„œ content_id, content_name, domain ì„ ë½‘ì•„ì„œ ë¦¬í„´.\n",
    "    content_name í›„ë³´ ì»¬ëŸ¼:\n",
    "      content_name > master_title > original_title > title > name\n",
    "    domain ì»¬ëŸ¼ì€ 'domain'(ëŒ€ì†Œë¬¸ì ë¬´ì‹œ) ì‚¬ìš©.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(CONTENTS):\n",
    "        print(\"â„¹ï¸ contents.csv ì—†ìŒ â†’ ì´ë¦„/ë„ë©”ì¸ ì—†ì´ ì§„í–‰\")\n",
    "        return None\n",
    "\n",
    "    meta = read_csv_retry(CONTENTS)\n",
    "    if meta is None or meta.empty:\n",
    "        print(\"â„¹ï¸ contents.csv ë¹„ì–´ìˆìŒ â†’ ì´ë¦„/ë„ë©”ì¸ ì—†ì´ ì§„í–‰\")\n",
    "        return None\n",
    "\n",
    "    meta = meta.copy()\n",
    "    meta.columns = [c.strip() for c in meta.columns]\n",
    "\n",
    "    if \"content_id\" not in meta.columns:\n",
    "        print(\"â„¹ï¸ contents.csv ì— content_id ì—†ìŒ â†’ ì´ë¦„/ë„ë©”ì¸ ë¶™ì´ê¸° ìƒëµ\")\n",
    "        return None\n",
    "    meta[\"content_id\"] = pd.to_numeric(meta[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    meta = meta[meta[\"content_id\"].notna()].copy()\n",
    "    meta[\"content_id\"] = meta[\"content_id\"].astype(int)\n",
    "\n",
    "    lower_map = {c.lower(): c for c in meta.columns}\n",
    "\n",
    "    # ì´ë¦„ ì»¬ëŸ¼\n",
    "    name_key = None\n",
    "    for key in [\"content_name\", \"master_title\", \"original_title\", \"title\", \"name\"]:\n",
    "        if key in lower_map:\n",
    "            name_key = lower_map[key]\n",
    "            break\n",
    "\n",
    "    # ë„ë©”ì¸ ì»¬ëŸ¼\n",
    "    domain_key = lower_map.get(\"domain\")\n",
    "\n",
    "    if name_key is None and domain_key is None:\n",
    "        print(\"â„¹ï¸ contents.csv ì—ì„œ ì´ë¦„/ë„ë©”ì¸ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í•¨ â†’ ë©”íƒ€ ì—†ì´ ì§„í–‰\")\n",
    "        return None\n",
    "\n",
    "    use_cols = [\"content_id\"]\n",
    "    rename_map = {}\n",
    "    if name_key is not None:\n",
    "        use_cols.append(name_key)\n",
    "        rename_map[name_key] = \"content_name\"\n",
    "    if domain_key is not None:\n",
    "        use_cols.append(domain_key)\n",
    "        rename_map[domain_key] = \"domain\"\n",
    "\n",
    "    meta = meta[use_cols].drop_duplicates(subset=[\"content_id\"], keep=\"first\")\n",
    "    meta = meta.rename(columns=rename_map)\n",
    "\n",
    "    used_cols_str = \", \".join([c for c in [\"content_name\", \"domain\"] if c in meta.columns])\n",
    "    print(f\"âœ… contents ë©”íƒ€ ì‚¬ìš©: {used_cols_str}\")\n",
    "    return meta\n",
    "\n",
    "\n",
    "def load_content_genres_raw() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    content_raw_genres.csv ì—ì„œ\n",
    "      content_id, raw_genre, g_norm(lower)\n",
    "    ë¥¼ ë¦¬í„´.\n",
    "    (ì—†ìœ¼ë©´ ì¶”ì²œ ìì²´ë¥¼ ì¤‘ë‹¨)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(RAW_GENRES):\n",
    "        raise RuntimeError(\"content_raw_genres.csv ê°€ ì—†ìŠµë‹ˆë‹¤. ì¥ë¥´ ê¸°ë°˜ ì¶”ì²œ ë¶ˆê°€.\")\n",
    "\n",
    "    gdf = read_csv_retry(RAW_GENRES)\n",
    "    if gdf is None or gdf.empty:\n",
    "        raise RuntimeError(\"content_raw_genres.csv ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¥ë¥´ ê¸°ë°˜ ì¶”ì²œ ë¶ˆê°€.\")\n",
    "\n",
    "    gdf = gdf.copy()\n",
    "    gdf.columns = [c.strip() for c in gdf.columns]\n",
    "\n",
    "    if not {\"content_id\", \"raw_genre\"}.issubset(set(gdf.columns)):\n",
    "        raise RuntimeError(\"content_raw_genres.csv ì— content_id/raw_genre ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    gdf[\"content_id\"] = pd.to_numeric(gdf[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    gdf = gdf[gdf[\"content_id\"].notna()].copy()\n",
    "    gdf[\"content_id\"] = gdf[\"content_id\"].astype(int)\n",
    "\n",
    "    gdf[\"raw_genre\"] = gdf[\"raw_genre\"].astype(str).str.strip()\n",
    "    gdf = gdf[gdf[\"raw_genre\"] != \"\"].copy()\n",
    "\n",
    "    gdf[\"g_norm\"] = gdf[\"raw_genre\"].str.lower().str.strip()\n",
    "\n",
    "    if gdf.empty:\n",
    "        raise RuntimeError(\"content_raw_genres.csv ì— ìœ íš¨í•œ ì¥ë¥´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¥ë¥´ ê¸°ë°˜ ì¶”ì²œ ë¶ˆê°€.\")\n",
    "\n",
    "    print(f\"âœ… ì½˜í…ì¸  ì¥ë¥´ raw ë¡œë“œ: rows={len(gdf)}, unique_genres={gdf['g_norm'].nunique()}\")\n",
    "    return gdf[[\"content_id\", \"raw_genre\", \"g_norm\"]]\n",
    "\n",
    "\n",
    "def build_genre_centroids(\n",
    "    item_df: pd.DataFrame,\n",
    "    M: np.ndarray,\n",
    "    gdf_raw: pd.DataFrame\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    ê° ì¥ë¥´(g_norm)ë³„ë¡œ í•´ë‹¹ ì¥ë¥´ë¥¼ ê°€ì§„ ì•„ì´í…œ ì„ë² ë”© í‰ê·  â†’ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ.\n",
    "    (item_df / M ì€ ì´ë¯¸ 'ì¥ë¥´ê°€ ìˆëŠ” ì•„ì´í…œ'ë§Œ ë‚¨ê²¨ì§„ ìƒíƒœë¼ê³  ê°€ì •)\n",
    "    \"\"\"\n",
    "    idx_map = {cid: i for i, cid in enumerate(item_df[\"content_id\"].tolist())}\n",
    "    genre_vecs: Dict[str, np.ndarray] = {}\n",
    "    for g, grp in gdf_raw.groupby(\"g_norm\"):\n",
    "        idxs = [idx_map[c] for c in grp[\"content_id\"].tolist() if c in idx_map]\n",
    "        if not idxs:\n",
    "            continue\n",
    "        v = M[idxs].mean(axis=0)\n",
    "        v = v / (np.linalg.norm(v) + 1e-12)\n",
    "        genre_vecs[g] = v\n",
    "    print(f\"âœ… ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ ìˆ˜: {len(genre_vecs)}\")\n",
    "    return genre_vecs\n",
    "\n",
    "\n",
    "def build_genres_agg(gdf_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    content_id ë³„ genres_str ('ì¥ë¥´1|ì¥ë¥´2|...') ìƒì„±.\n",
    "    \"\"\"\n",
    "    agg = (\n",
    "        gdf_raw.groupby(\"content_id\")[\"raw_genre\"]\n",
    "        .apply(lambda x: \"|\".join(pd.unique(x.tolist())))\n",
    "        .reset_index()\n",
    "        .rename(columns={\"raw_genre\": \"genres_str\"})\n",
    "    )\n",
    "    print(f\"âœ… ì¥ë¥´ ì§‘ê³„ ë§¤í•‘: {len(agg)}ê°œ content_id\")\n",
    "    return agg\n",
    "\n",
    "\n",
    "def load_user_genre_prefs() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    user_preferred_genres.csv (+ optional user_preferences.csv)ë¥¼ ì½ì–´ì„œ\n",
    "      user_id, username, genres(list of str, g_norm) ë¥¼ ë¦¬í„´.\n",
    "    \"\"\"\n",
    "    upg = read_csv_retry(UP_GENRES)\n",
    "    if upg is None or upg.empty:\n",
    "        raise RuntimeError(\"user_preferred_genres.csv í•„ìš”.\")\n",
    "\n",
    "    upg = upg.copy()\n",
    "    upg.columns = [c.strip() for c in upg.columns]\n",
    "\n",
    "    # ì¥ë¥´ ì»¬ëŸ¼ ì°¾ê¸°\n",
    "    lower_map = {c.lower(): c for c in upg.columns}\n",
    "    genre_col = None\n",
    "    for key in [\"genre\", \"raw_genre\"]:\n",
    "        if key in lower_map:\n",
    "            genre_col = lower_map[key]\n",
    "            break\n",
    "    if genre_col is None:\n",
    "        raise RuntimeError(\"user_preferred_genres.csv ì— genre/raw_genre ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    upg[\"genre_norm\"] = upg[genre_col].astype(str).str.strip().str.lower()\n",
    "    upg = upg[upg[\"genre_norm\"] != \"\"].copy()\n",
    "\n",
    "    # 1) user_preferences.csv ê¸°ë°˜ (preference_id -> id, username)\n",
    "    upref = read_csv_retry(UPREF)\n",
    "    rows = []\n",
    "\n",
    "    if upref is not None and not upref.empty and \\\n",
    "       \"id\" in [c.lower() for c in upref.columns] and \\\n",
    "       \"preference_id\" in [c.lower() for c in upg.columns]:\n",
    "\n",
    "        upref = upref.copy()\n",
    "        upref.columns = [c.strip() for c in upref.columns]\n",
    "        low_map_pref = {c.lower(): c for c in upref.columns}\n",
    "        id_col = low_map_pref.get(\"id\")\n",
    "        username_col = next((low_map_pref[c] for c in low_map_pref if c in [\"username\", \"user_name\"]), None)\n",
    "\n",
    "        low_map_upg = {c.lower(): c for c in upg.columns}\n",
    "        pref_id_col = low_map_upg[\"preference_id\"]\n",
    "\n",
    "        upg[pref_id_col] = pd.to_numeric(upg[pref_id_col], errors=\"coerce\").astype(\"Int64\")\n",
    "        upg = upg[upg[pref_id_col].notna()].copy()\n",
    "\n",
    "        for _, ur in upref.iterrows():\n",
    "            pid = ur[id_col]\n",
    "            try:\n",
    "                pid_int = int(pid)\n",
    "            except Exception:\n",
    "                continue\n",
    "            uname = str(ur[username_col]) if username_col else \"\"\n",
    "            g_list = upg.loc[upg[pref_id_col] == pid_int, \"genre_norm\"].tolist()\n",
    "            if not g_list:\n",
    "                continue\n",
    "            g_list = sorted(set(g_list))\n",
    "            rows.append([pid_int, uname, g_list])\n",
    "    else:\n",
    "        # 2) user_preferred_genres.csv ì•ˆì—ì„œ user_id / username ì°¾ê¸°\n",
    "        low_map = {c.lower(): c for c in upg.columns}\n",
    "        user_key = None\n",
    "        for key in [\"user_id\", \"userid\", \"user id\", \"id\"]:\n",
    "            if key in low_map:\n",
    "                user_key = low_map[key]\n",
    "                break\n",
    "        username_key = None\n",
    "        for key in [\"username\", \"user_name\"]:\n",
    "            if key in low_map:\n",
    "                username_key = low_map[key]\n",
    "                break\n",
    "\n",
    "        if user_key is not None:\n",
    "            upg[user_key] = pd.to_numeric(upg[user_key], errors=\"coerce\").astype(\"Int64\")\n",
    "            upg = upg[upg[user_key].notna()].copy()\n",
    "            upg[user_key] = upg[user_key].astype(int)\n",
    "\n",
    "            for uid, grp in upg.groupby(user_key):\n",
    "                uname = \"\"\n",
    "                if username_key:\n",
    "                    uname = str(grp[username_key].iloc[0])\n",
    "                g_list = sorted(set(grp[\"genre_norm\"].tolist()))\n",
    "                rows.append([int(uid), uname, g_list])\n",
    "        elif username_key is not None:\n",
    "            # usernameë§Œ ìˆì„ ë•Œ: ë‚´ë¶€ user_idë¥¼ 1,2,3...ìœ¼ë¡œ í• ë‹¹\n",
    "            for new_id, (uname, grp) in enumerate(upg.groupby(username_key), start=1):\n",
    "                g_list = sorted(set(grp[\"genre_norm\"].tolist()))\n",
    "                rows.append([new_id, str(uname), g_list])\n",
    "        else:\n",
    "            raise RuntimeError(\"user_preferred_genres.csv ì— user_idë‚˜ username ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"ìœ ì € ì„ í˜¸ ì¥ë¥´ ì •ë³´ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    prefs = pd.DataFrame(rows, columns=[\"user_id\", \"username\", \"genres\"])\n",
    "    print(f\"âœ… ìœ ì € ì„ í˜¸ ì¥ë¥´ ë¡œë“œ: users={len(prefs)}\")\n",
    "    return prefs\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) ì•„ì´í…œ ì„ë² ë”©\n",
    "    item_df, M, emb_cols = load_item_embeddings()\n",
    "\n",
    "    # 2) ì½˜í…ì¸  ì¥ë¥´ raw (í•„ìˆ˜)\n",
    "    gdf_raw = load_content_genres_raw()  # ì—¬ê¸°ì„œ ì—†ìœ¼ë©´ ë°”ë¡œ ì—ëŸ¬\n",
    "\n",
    "    # 2-1) created_at ê¸°ì¤€ìœ¼ë¡œ 2020ë…„ ì´í›„ ì½˜í…ì¸ ë§Œ í—ˆìš© (ì˜µì…˜)\n",
    "    allowed_ids = load_allowed_ids_by_created_at()  # ì—†ìœ¼ë©´ None\n",
    "    if allowed_ids is not None:\n",
    "        before_g = len(gdf_raw)\n",
    "        gdf_raw = gdf_raw[gdf_raw[\"content_id\"].isin(allowed_ids)].reset_index(drop=True)\n",
    "        print(f\"âœ… created_at >= {MIN_YEAR_CREATED} í•„í„° ì ìš© (ì¥ë¥´ í…Œì´ë¸”): {before_g} â†’ {len(gdf_raw)}\")\n",
    "        if gdf_raw.empty:\n",
    "            raise RuntimeError(f\"created_at >= {MIN_YEAR_CREATED} ì´ë©´ì„œ ì¥ë¥´ê°€ ìˆëŠ” ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤. ì¶”ì²œ ë¶ˆê°€.\")\n",
    "\n",
    "    # 2-2) 'ì¥ë¥´ê°€ ìˆëŠ” ì•„ì´í…œë§Œ' ì•„ì´í…œ í…Œì´ë¸”ì—ì„œ ìœ ì§€\n",
    "    has_genre_ids = set(gdf_raw[\"content_id\"].unique())\n",
    "\n",
    "    before_cnt = len(item_df)\n",
    "    mask = item_df[\"content_id\"].isin(has_genre_ids)\n",
    "    item_df = item_df[mask].reset_index(drop=True)\n",
    "    M = M[mask.to_numpy()]  # ì„ë² ë”© í–‰ë ¬ë„ ê°™ì´ í•„í„°ë§\n",
    "    after_cnt = len(item_df)\n",
    "    print(f\"âœ… ì¥ë¥´+ë‚ ì§œ ì¡°ê±´ ë§Œì¡± ì•„ì´í…œë§Œ ìœ ì§€: {before_cnt} â†’ {after_cnt}\")\n",
    "\n",
    "    if after_cnt == 0:\n",
    "        raise RuntimeError(\"ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì•„ì´í…œì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤. ì¶”ì²œ ë¶ˆê°€.\")\n",
    "\n",
    "    item_ids = item_df[\"content_id\"].to_numpy(int)\n",
    "\n",
    "    # 3) ì¥ë¥´ ì§‘ê³„ + ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œ\n",
    "    genres_agg = build_genres_agg(gdf_raw)\n",
    "    genre_vecs = build_genre_centroids(item_df, M, gdf_raw)\n",
    "\n",
    "    # 4) ìœ ì €ë³„ ì„ í˜¸ ì¥ë¥´\n",
    "    prefs = load_user_genre_prefs()\n",
    "\n",
    "    # 5) content_name/domain ë¶™ì¼ ë©”íƒ€\n",
    "    meta_name = load_content_names()\n",
    "\n",
    "    # 6) ê¸€ë¡œë²Œ í‰ê·  ë²¡í„° (fallback) - ì´ë¯¸ ì¥ë¥´+ë‚ ì§œ ì¡°ê±´ ë§Œì¡± ì•„ì´í…œë§Œ ë‚¨ì€ ìƒíƒœ\n",
    "    v_global = M.mean(axis=0)\n",
    "    v_global = v_global / (np.linalg.norm(v_global) + 1e-12)\n",
    "\n",
    "    rec_rows = []\n",
    "    for _, ur in prefs.iterrows():\n",
    "        uid = int(ur[\"user_id\"])\n",
    "        uname = str(ur.get(\"username\", \"\") or \"\")\n",
    "        g_list: List[str] = ur[\"genres\"] or []\n",
    "\n",
    "        # ì´ ìœ ì €ì˜ ì„ í˜¸ ì¥ë¥´ ì¤‘ ì„¼íŠ¸ë¡œì´ë“œê°€ ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì‚¬ìš©\n",
    "        g_match = [g for g in g_list if g in genre_vecs]\n",
    "        if g_match:\n",
    "            g_vecs = np.stack([genre_vecs[g] for g in g_match], axis=0)\n",
    "            v_user = g_vecs.mean(axis=0)\n",
    "            v_user = v_user / (np.linalg.norm(v_user) + 1e-12)\n",
    "        else:\n",
    "            # í•´ë‹¹ ì¥ë¥´ ì„¼íŠ¸ë¡œì´ë“œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê¸€ë¡œë²Œ í‰ê·  (ê·¸ë˜ë„ í›„ë³´ëŠ” ì¥ë¥´ ìˆëŠ” ì•„ì´í…œë§Œ)\n",
    "            v_user = v_global\n",
    "\n",
    "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (M, v_user ëª¨ë‘ ì •ê·œí™”ëœ ìƒíƒœ)\n",
    "        sims = M @ v_user.reshape(-1, 1)\n",
    "        sims = sims.reshape(-1)\n",
    "\n",
    "        k = min(TOPK, len(sims))\n",
    "        if k == 0:\n",
    "            continue\n",
    "        part = np.argpartition(-sims, k-1)[:k]\n",
    "        order = part[np.argsort(-sims[part])]\n",
    "        top_ids = item_ids[order]\n",
    "        top_scs = sims[order]\n",
    "\n",
    "        for rank, (cid, sc) in enumerate(zip(top_ids, top_scs), start=1):\n",
    "            rec_rows.append([uid, uname, rank, int(cid), float(sc)])\n",
    "\n",
    "    rec = pd.DataFrame(rec_rows, columns=[\"user_id\", \"username\", \"rank\", \"content_id\", \"score\"])\n",
    "\n",
    "    # 7) ì´ë¦„ + ë„ë©”ì¸ ë¶™ì´ê¸°\n",
    "    if meta_name is not None:\n",
    "        rec = rec.merge(meta_name, on=\"content_id\", how=\"left\")\n",
    "\n",
    "    # 8) ì¥ë¥´ ë¶™ì´ê¸° (ì§‘ê³„)\n",
    "    if genres_agg is not None:\n",
    "        rec = rec.merge(genres_agg, on=\"content_id\", how=\"left\")\n",
    "\n",
    "    # 9) ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬\n",
    "    cols = [\"user_id\", \"username\", \"rank\", \"content_id\"]\n",
    "    if \"content_name\" in rec.columns:\n",
    "        cols.append(\"content_name\")\n",
    "    if \"domain\" in rec.columns:\n",
    "        cols.append(\"domain\")\n",
    "    if \"genres_str\" in rec.columns:\n",
    "        cols.append(\"genres_str\")\n",
    "    cols.append(\"score\")\n",
    "    rec = rec[cols]\n",
    "\n",
    "    rec.sort_values([\"user_id\", \"rank\"]).to_csv(OUT_RECS, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… recommendations_topK_genre.csv ì €ì¥: {OUT_RECS} (rows={len(rec)})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180a1b3",
   "metadata": {},
   "source": [
    "# ë„ë©”ì¸ ë³„ ì¶”ì²œ ì•„ì´í…œ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "320241e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: item_embeddings_torch.csv (items=495, dim=64)\n",
      "âœ… ì œëª© ì»¬ëŸ¼ ë°œê²¬: master_title\n",
      "âœ… recommendations_per_domain.csv ì €ì¥: csv ë°ì´í„°\\clean\\recommendations_per_domain.csv (rows=90)\n",
      "\n",
      "[User 1] ì´ 9ê°œ\n",
      "  # 1  cid=29  dom=AV  sim=0.4297  name=ì ì¸ê±¸ 3: ì‚¬ëŒ€ì²œì™•\n",
      "  # 2  cid=49  dom=AV  sim=0.3864  name=ì—ë¸Œë¦¬ì”½ ì—ë¸Œë¦¬ì›¨ì–´ ì˜¬ ì•³ ì›ìŠ¤\n",
      "  # 3  cid=21  dom=AV  sim=0.3615  name=ë² ë†ˆ: ë¼ìŠ¤íŠ¸ ëŒ„ìŠ¤\n",
      "  # 4  cid=485  dom=WEBNOVEL  sim=0.2978  name=ì˜ì—­ ì¹¨ë²”\n",
      "  # 5  cid=503  dom=WEBNOVEL  sim=0.2955  name=ê´‘ê³ ì²œì¬ì˜ ì—°ì˜ˆê³„ ê³µëµë²•\n",
      "  # 6  cid=235  dom=GAME  sim=0.2890  name=Disciples II: Rise of the Elves\n",
      "  # 7  cid=547  dom=WEBNOVEL  sim=0.2634  name=í”Œë ˆì´ì–´ 2ì„¸ë¡œ ì‚´ì•„ê°€ëŠ” ë²•\n",
      "  # 8  cid=295  dom=GAME  sim=0.2545  name=Typer Shark! Deluxe\n",
      "  # 9  cid=245  dom=GAME  sim=0.2274  name=Wolfenstein 3D\n",
      "\n",
      "[User 2] ì´ 9ê°œ\n",
      "  # 1  cid=76  dom=AV  sim=0.4220  name=ì˜¥ìŠ¤í¼ë“œì—ì„œì˜ ë‚ ë“¤\n",
      "  # 2  cid=207  dom=GAME  sim=0.4073  name=Half-Life\n",
      "  # 3  cid=193  dom=AV  sim=0.3437  name=ë³´ì­ í™€ìŠ¤ë§¨\n",
      "  # 4  cid=519  dom=WEBNOVEL  sim=0.3338  name=ì²˜ë§ìœ¼ë©° ë ˆë²¨ì—…\n",
      "  # 5  cid=474  dom=WEBNOVEL  sim=0.3119  name=íŒŒí˜¼ í›„\n",
      "  # 6  cid=159  dom=AV  sim=0.3101  name=ë¶€íŠ¸ìº í”„\n",
      "  # 7  cid=294  dom=GAME  sim=0.3056  name=Pizza Frenzy Deluxe\n",
      "  # 8  cid=276  dom=GAME  sim=0.3028  name=Xpand Rally Xtreme\n",
      "  # 9  cid=523  dom=WEBNOVEL  sim=0.2880  name=ê¸ˆìˆ˜ì € íˆ¬ìë°±ì„œ\n",
      "\n",
      "[User 3] ì´ 9ê°œ\n",
      "  # 1  cid=91  dom=AV  sim=0.4364  name=ê·¸ëŒ€ë“¤ì€ ì–´ë–»ê²Œ ì‚´ ê²ƒì¸ê°€\n",
      "  # 2  cid=22  dom=AV  sim=0.4205  name=ì„¼ê³¼ ì¹˜íˆë¡œì˜ í–‰ë°©ë¶ˆëª…\n",
      "  # 3  cid=488  dom=WEBNOVEL  sim=0.4181  name=ì•½ë¨¹ëŠ” ì²œì¬ë§ˆë²•ì‚¬\n",
      "  # 4  cid=7  dom=AV  sim=0.3811  name=ë©ì²­ì”¨ ë¶€ë¶€ ì´ì•¼ê¸°\n",
      "  # 5  cid=514  dom=WEBNOVEL  sim=0.2863  name=ë¯¸ì—°ì‹œ ê²Œì„ì˜ íí•´\n",
      "  # 6  cid=288  dom=GAME  sim=0.2711  name=Big Money! Deluxe\n",
      "  # 7  cid=555  dom=WEBNOVEL  sim=0.2663  name=ì°©ê°ê³„ ì†Œë“œë§ˆìŠ¤í„°\n",
      "  # 8  cid=226  dom=GAME  sim=0.2369  name=SiN Episodes: Emergence\n",
      "  # 9  cid=219  dom=GAME  sim=0.2270  name=Dota 2\n",
      "\n",
      "[User 4] ì´ 9ê°œ\n",
      "  # 1  cid=25  dom=AV  sim=0.4080  name=ìŠ¤í¬ë¦¼\n",
      "  # 2  cid=9  dom=AV  sim=0.3867  name=27ì¼ì˜ ë°¤\n",
      "  # 3  cid=276  dom=GAME  sim=0.3679  name=Xpand Rally Xtreme\n",
      "  # 4  cid=297  dom=GAME  sim=0.3575  name=Peggle Deluxe\n",
      "  # 5  cid=86  dom=AV  sim=0.3566  name=???\n",
      "  # 6  cid=543  dom=WEBNOVEL  sim=0.3101  name=EXê¸‰ ì–‘ìì»´í“¨í„°ë¡œ 500ì¡° ì¬ë²Œ\n",
      "  # 7  cid=468  dom=WEBNOVEL  sim=0.2584  name=ê²€ìˆ  ëª…ê°€ì˜ ëŒ€ë§ˆë²•ì‚¬ ë§‰ë‚´ë”¸\n",
      "  # 8  cid=503  dom=WEBNOVEL  sim=0.2574  name=ê´‘ê³ ì²œì¬ì˜ ì—°ì˜ˆê³„ ê³µëµë²•\n",
      "  # 9  cid=258  dom=GAME  sim=0.2420  name=Call of DutyÂ®\n",
      "\n",
      "[User 5] ì´ 9ê°œ\n",
      "  # 1  cid=74  dom=AV  sim=0.6940  name=ê·¸ë¦° ë¶\n",
      "  # 2  cid=6  dom=AV  sim=0.5786  name=íŒì˜ ë¯¸ë¡œ: ì˜¤í•„ë¦¬ì•„ì™€ ì„¸ê°œì˜ ì—´ì‡ \n",
      "  # 3  cid=13  dom=AV  sim=0.4763  name=êµ¿ë‰´ìŠ¤\n",
      "  # 4  cid=552  dom=WEBNOVEL  sim=0.4252  name=ì·¨ì‚¬ë³‘ì´ ìš”ë¦¬ë¥¼ ë„ˆë¬´ ì˜í•¨\n",
      "  # 5  cid=590  dom=WEBNOVEL  sim=0.3635  name=ì²œì¬ í‰ë¶€ì™¸ê³¼ ì˜ì‚¬ê°€ ë‹¤ì‚´ë¦¼\n",
      "  # 6  cid=486  dom=WEBNOVEL  sim=0.3479  name=ëŒ€ê¸°ì—… ë§ë‹¨ì´ ì¼ì„ ì˜í•¨\n",
      "  # 7  cid=209  dom=GAME  sim=0.3157  name=Half-Life: Blue Shift\n",
      "  # 8  cid=259  dom=GAME  sim=0.3064  name=Call of DutyÂ® 2\n",
      "  # 9  cid=300  dom=GAME  sim=0.2948  name=Mystery P.I.TM - The Lottery Ticket\n",
      "\n",
      "[User 6] ì´ 9ê°œ\n",
      "  # 1  cid=217  dom=GAME  sim=0.4524  name=Left 4 Dead\n",
      "  # 2  cid=4  dom=AV  sim=0.4047  name=ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤\n",
      "  # 3  cid=7  dom=AV  sim=0.4019  name=ë©ì²­ì”¨ ë¶€ë¶€ ì´ì•¼ê¸°\n",
      "  # 4  cid=558  dom=WEBNOVEL  sim=0.3771  name=ì˜¥íƒ€ê³¤ì—ì„œ ëª¨ë¥´ë©´ ë§ì•„ì•¼ì§€\n",
      "  # 5  cid=180  dom=AV  sim=0.3670  name=ì˜¤ë²„ë¡œë“œ\n",
      "  # 6  cid=555  dom=WEBNOVEL  sim=0.3210  name=ì°©ê°ê³„ ì†Œë“œë§ˆìŠ¤í„°\n",
      "  # 7  cid=586  dom=WEBNOVEL  sim=0.3051  name=ê³ ë ¤ì—ì„œ ì¹˜íŠ¸ ì—†ì´ ë¬¸ëª…í•©ë‹ˆë‹¤\n",
      "  # 8  cid=295  dom=GAME  sim=0.1863  name=Typer Shark! Deluxe\n",
      "  # 9  cid=229  dom=GAME  sim=0.1823  name=Uplink\n",
      "\n",
      "[User 7] ì´ 9ê°œ\n",
      "  # 1  cid=15  dom=AV  sim=0.3893  name=365ì¼: ì˜¤ëŠ˜\n",
      "  # 2  cid=493  dom=WEBNOVEL  sim=0.3616  name=ì–¸ë‹ˆ, ì´ë²ˆ ìƒì—” ë‚´ê°€ ì™•ë¹„ì•¼\n",
      "  # 3  cid=76  dom=AV  sim=0.3463  name=ì˜¥ìŠ¤í¼ë“œì—ì„œì˜ ë‚ ë“¤\n",
      "  # 4  cid=7  dom=AV  sim=0.3230  name=ë©ì²­ì”¨ ë¶€ë¶€ ì´ì•¼ê¸°\n",
      "  # 5  cid=427  dom=WEBNOVEL  sim=0.3145  name=ì‹œí•œë¶€ ì²œì¬ê°€ ì‚´ì•„ë‚¨ëŠ” ë²•\n",
      "  # 6  cid=488  dom=WEBNOVEL  sim=0.2970  name=ì•½ë¨¹ëŠ” ì²œì¬ë§ˆë²•ì‚¬\n",
      "  # 7  cid=202  dom=GAME  sim=0.2888  name=Team Fortress Classic\n",
      "  # 8  cid=207  dom=GAME  sim=0.2587  name=Half-Life\n",
      "  # 9  cid=229  dom=GAME  sim=0.2447  name=Uplink\n",
      "\n",
      "[User 8] ì´ 9ê°œ\n",
      "  # 1  cid=29  dom=AV  sim=0.3558  name=ì ì¸ê±¸ 3: ì‚¬ëŒ€ì²œì™•\n",
      "  # 2  cid=86  dom=AV  sim=0.3366  name=???\n",
      "  # 3  cid=97  dom=AV  sim=0.3357  name=ì–´ì©Œë‹¤ íŒŒíŠ¸ë„ˆ\n",
      "  # 4  cid=543  dom=WEBNOVEL  sim=0.2993  name=EXê¸‰ ì–‘ìì»´í“¨í„°ë¡œ 500ì¡° ì¬ë²Œ\n",
      "  # 5  cid=468  dom=WEBNOVEL  sim=0.2806  name=ê²€ìˆ  ëª…ê°€ì˜ ëŒ€ë§ˆë²•ì‚¬ ë§‰ë‚´ë”¸\n",
      "  # 6  cid=297  dom=GAME  sim=0.2763  name=Peggle Deluxe\n",
      "  # 7  cid=514  dom=WEBNOVEL  sim=0.2742  name=ë¯¸ì—°ì‹œ ê²Œì„ì˜ íí•´\n",
      "  # 8  cid=233  dom=GAME  sim=0.2494  name=Space Empires IV Deluxe\n",
      "  # 9  cid=258  dom=GAME  sim=0.2286  name=Call of DutyÂ®\n",
      "\n",
      "[User 9] ì´ 9ê°œ\n",
      "  # 1  cid=29  dom=AV  sim=0.4718  name=ì ì¸ê±¸ 3: ì‚¬ëŒ€ì²œì™•\n",
      "  # 2  cid=49  dom=AV  sim=0.3523  name=ì—ë¸Œë¦¬ì”½ ì—ë¸Œë¦¬ì›¨ì–´ ì˜¬ ì•³ ì›ìŠ¤\n",
      "  # 3  cid=547  dom=WEBNOVEL  sim=0.3376  name=í”Œë ˆì´ì–´ 2ì„¸ë¡œ ì‚´ì•„ê°€ëŠ” ë²•\n",
      "  # 4  cid=50  dom=AV  sim=0.3189  name=ì–´ë©”ì´ì§• ìŠ¤íŒŒì´ë”ë§¨ 2\n",
      "  # 5  cid=583  dom=WEBNOVEL  sim=0.3176  name=LAì—ì„œ ë¯¼ë°•ì§‘ìœ¼ë¡œ íë§í• ê²Œìš”\n",
      "  # 6  cid=503  dom=WEBNOVEL  sim=0.3145  name=ê´‘ê³ ì²œì¬ì˜ ì—°ì˜ˆê³„ ê³µëµë²•\n",
      "  # 7  cid=235  dom=GAME  sim=0.2862  name=Disciples II: Rise of the Elves\n",
      "  # 8  cid=295  dom=GAME  sim=0.2783  name=Typer Shark! Deluxe\n",
      "  # 9  cid=299  dom=GAME  sim=0.2693  name=Venice Deluxe\n",
      "\n",
      "[User 10] ì´ 9ê°œ\n",
      "  # 1  cid=76  dom=AV  sim=0.4246  name=ì˜¥ìŠ¤í¼ë“œì—ì„œì˜ ë‚ ë“¤\n",
      "  # 2  cid=30  dom=AV  sim=0.3642  name=ë˜ ë‹¤ë¥¸ 365ì¼\n",
      "  # 3  cid=267  dom=GAME  sim=0.3544  name=X: Beyond the Frontier\n",
      "  # 4  cid=207  dom=GAME  sim=0.3149  name=Half-Life\n",
      "  # 5  cid=523  dom=WEBNOVEL  sim=0.3148  name=ê¸ˆìˆ˜ì € íˆ¬ìë°±ì„œ\n",
      "  # 6  cid=559  dom=WEBNOVEL  sim=0.2971  name=ì œêµ­ ì—´ê°• ëšë°°ê¸° ê¹¨ëŠ” íŠ¸ë¦½ 1880\n",
      "  # 7  cid=20  dom=AV  sim=0.2838  name=ê·¹ì¥íŒ ê·€ë©¸ì˜ ì¹¼ë‚ : ë¬´í•œì—´ì°¨í¸\n",
      "  # 8  cid=287  dom=GAME  sim=0.2835  name=Bejeweled Deluxe\n",
      "  # 9  cid=488  dom=WEBNOVEL  sim=0.2819  name=ì•½ë¨¹ëŠ” ì²œì¬ë§ˆë²•ì‚¬\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#ë„ë©”ì¸ë³„ Top-N ì¶”ì²œ (ìœ ì €/ì•„ì´í…œ ì„ë² ë”© + contents.csv)\n",
    "#ì…ë ¥:\n",
    "#  clean/user_embeddings.csv                  (user_id, username, emb_0..)\n",
    "#  clean/item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv (content_id, emb_0..)\n",
    "#  clean/contents.csv                         (content_id, domain, title/name/...)\n",
    "#ì¶œë ¥:\n",
    "#  clean/recommendations_per_domain.csv       (user_id, username, rank, content_id, content_title, score, domain)\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict\n",
    "\n",
    "# ===== ê²½ë¡œ =====\n",
    "BASE = r\"csv ë°ì´í„°\\clean\"\n",
    "USER_EMB   = os.path.join(BASE, \"user_embeddings.csv\")\n",
    "ITEM_EMB_1 = os.path.join(BASE, \"item_embeddings_torch.csv\")\n",
    "ITEM_EMB_2 = os.path.join(BASE, \"item_embeddings.csv\")\n",
    "CONTENTS   = os.path.join(BASE, \"contents.csv\")\n",
    "OUT_CSV    = os.path.join(BASE, \"recommendations_per_domain.csv\")\n",
    "\n",
    "# ===== ì„¤ì • =====\n",
    "DOMAIN_QUOTA: Dict[str, int] = {\"AV\": 3, \"GAME\": 3, \"WEBNOVEL\": 3}   # ë„ë©”ì¸ë³„ ê°œìˆ˜\n",
    "FILL_WITH_GLOBAL = True    # ë„ë©”ì¸ í›„ë³´ ë¶€ì¡± ì‹œ ê¸€ë¡œë²Œ Topìœ¼ë¡œ ë³´ì¶©\n",
    "ASSUME_NORMALIZED = True   # ì„ë² ë”©ì´ ì´ë¯¸ L2 ì •ê·œí™”ë¼ë©´ True\n",
    "EXCLUDE_DUP_CONTENTS = True\n",
    "\n",
    "# ===== ìœ í‹¸ =====\n",
    "def read_csv_retry(path, encodings=(\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\",\"latin1\"), **kw) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    last=None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kw)\n",
    "        except Exception as e:\n",
    "            last=e\n",
    "    raise last\n",
    "\n",
    "def l2norm(X: np.ndarray) -> np.ndarray:\n",
    "    return X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def _coerce_content_id_column(df: pd.DataFrame) -> pd.Series:\n",
    "    df = df.copy()\n",
    "    orig_cols = list(df.columns)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    name_map = {c.lower(): c for c in df.columns}\n",
    "    for key in [\"content_id\", \"contentid\", \"content id\", \"id\", \"unnamed: 0\"]:\n",
    "        if key in name_map:\n",
    "            s = df[name_map[key]]\n",
    "            if s.dtype == object:\n",
    "                s = s.astype(str).str.strip()\n",
    "            s = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "            if not s.isna().all():\n",
    "                return s\n",
    "    raise RuntimeError(f\"content_id ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì»¬ëŸ¼: {orig_cols}\")\n",
    "\n",
    "def load_user_embeddings() -> pd.DataFrame:\n",
    "    u = read_csv_retry(USER_EMB)\n",
    "    if u is None or u.empty:\n",
    "        raise RuntimeError(\"user_embeddings.csv í•„ìš”.\")\n",
    "    u = u.copy()\n",
    "    u.columns = [c.strip() for c in u.columns]\n",
    "    if \"user_id\" not in u.columns:\n",
    "        if \"id\" in u.columns:\n",
    "            u = u.rename(columns={\"id\": \"user_id\"})\n",
    "        else:\n",
    "            raise RuntimeError(\"user_embeddings.csvì— user_id ì»¬ëŸ¼ í•„ìš”.\")\n",
    "    u[\"user_id\"] = pd.to_numeric(u[\"user_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    u = u[u[\"user_id\"].notna()].copy()\n",
    "    u[\"user_id\"] = u[\"user_id\"].astype(int)\n",
    "\n",
    "    emb_cols = [c for c in u.columns if c.lower().startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        raise RuntimeError(\"user_embeddings.csvì— emb_ë¡œ ì‹œì‘í•˜ëŠ” ì„ë² ë”© ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    for c in emb_cols:\n",
    "        u[c] = pd.to_numeric(u[c], errors=\"coerce\")\n",
    "    u.dropna(subset=emb_cols, inplace=True)\n",
    "    if not ASSUME_NORMALIZED:\n",
    "        u.loc[:, emb_cols] = l2norm(u[emb_cols].to_numpy(np.float32))\n",
    "\n",
    "    # username ì»¬ëŸ¼ ìœ ì—° ì²˜ë¦¬\n",
    "    uname_col = None\n",
    "    for c in u.columns:\n",
    "        if c.lower() == \"username\":\n",
    "            uname_col = c; break\n",
    "    if uname_col is None:\n",
    "        u[\"username\"] = \"\"\n",
    "        uname_col = \"username\"\n",
    "\n",
    "    return u[[\"user_id\", uname_col] + emb_cols].rename(columns={uname_col: \"username\"})\n",
    "\n",
    "def load_item_embeddings() -> pd.DataFrame:\n",
    "    i1 = read_csv_retry(ITEM_EMB_1)\n",
    "    if i1 is not None and not i1.empty:\n",
    "        df = i1; src = os.path.basename(ITEM_EMB_1)\n",
    "    else:\n",
    "        i2 = read_csv_retry(ITEM_EMB_2)\n",
    "        if i2 is None or i2.empty:\n",
    "            raise RuntimeError(\"item_embeddings CSV í•„ìš”: item_embeddings_torch.csv ë˜ëŠ” item_embeddings.csv\")\n",
    "        df = i2; src = os.path.basename(ITEM_EMB_2)\n",
    "\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df[\"content_id\"] = _coerce_content_id_column(df)\n",
    "\n",
    "    emb_cols = [c for c in df.columns if c.lower().startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        alt = [c for c in df.columns if re.search(r\"\\d+$\", c)]\n",
    "        raise RuntimeError(f\"ì„ë² ë”© ì»¬ëŸ¼(emb_*) ì—†ìŒ. í™•ì¸ í•„ìš”. í›„ë³´: {alt}\")\n",
    "\n",
    "    for c in emb_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=emb_cols + [\"content_id\"]).copy()\n",
    "    df[\"content_id\"] = df[\"content_id\"].astype(int)\n",
    "\n",
    "    if EXCLUDE_DUP_CONTENTS:\n",
    "        df = df.drop_duplicates(subset=[\"content_id\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    if not ASSUME_NORMALIZED:\n",
    "        df.loc[:, emb_cols] = l2norm(df[emb_cols].to_numpy(np.float32))\n",
    "\n",
    "    print(f\"âœ… ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ: {src} (items={len(df)}, dim={len(emb_cols)})\")\n",
    "    return df[[\"content_id\"] + emb_cols]\n",
    "\n",
    "# ì œëª© ì»¬ëŸ¼ ìë™ íƒì§€: title / name / content_name / ko_title / ko_name ë“±\n",
    "def detect_title_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    cand_list = [\"title\", \"name\", \"content_name\", \"ko_title\", \"ko_name\", \"master_title\"]\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for c in cand_list:\n",
    "        if c in cols_lower:\n",
    "            return cols_lower[c]\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    # --- ë°ì´í„° ë¡œë“œ ---\n",
    "    u = load_user_embeddings()\n",
    "    i = load_item_embeddings()\n",
    "\n",
    "    contents = read_csv_retry(CONTENTS)\n",
    "    if contents is None or contents.empty or {\"content_id\",\"domain\"} - set(contents.columns):\n",
    "        raise RuntimeError(\"contents.csv í•„ìš” (content_id, domain).\")\n",
    "    contents = contents.copy()\n",
    "    contents.columns = [c.strip() for c in contents.columns]\n",
    "\n",
    "    # content_id ì •ë¦¬\n",
    "    contents[\"content_id\"] = pd.to_numeric(contents[\"content_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    contents = contents[contents[\"content_id\"].notna()].copy()\n",
    "    contents[\"content_id\"] = contents[\"content_id\"].astype(int)\n",
    "\n",
    "    # â˜… ì œëª© ì»¬ëŸ¼ ì°¾ê¸°\n",
    "    title_col = detect_title_column(contents)\n",
    "    if title_col is None:\n",
    "        raise RuntimeError(\"contents.csvì—ì„œ ì œëª© ì»¬ëŸ¼(title/name/...)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (íƒ€ì´í‹€ ì—†ëŠ” ì½˜í…ì¸  ì œì™¸ë¥¼ ìœ„í•´ ì œëª© ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.)\")\n",
    "    else:\n",
    "        print(f\"âœ… ì œëª© ì»¬ëŸ¼ ë°œê²¬: {title_col}\")\n",
    "\n",
    "    # â˜… ì œëª©ì´ ë¹„ì–´ìˆëŠ” ì½˜í…ì¸  ì œì™¸\n",
    "    contents[title_col] = contents[title_col].astype(str).str.strip()\n",
    "    contents = contents[contents[title_col] != \"\"].copy()\n",
    "\n",
    "    # â˜… ì œëª© ìˆëŠ” ì½˜í…ì¸ ì˜ content_idë§Œ ìœ íš¨\n",
    "    valid_ids = set(contents[\"content_id\"].unique())\n",
    "\n",
    "    # â˜… ì•„ì´í…œ ì„ë² ë”©ë„ valid_idsë§Œ ë‚¨ê¸°ê¸° â†’ íƒ€ì´í‹€ ì—†ëŠ” ì½˜í…ì¸ ëŠ” ì¶”ì²œ í›„ë³´ì—ì„œ ì œê±°\n",
    "    i = i[i[\"content_id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "\n",
    "    # ë„ë©”ì¸ ë§µì€ í•„í„°ë§ëœ contents ê¸°ì¤€\n",
    "    dom_map = contents.set_index(\"content_id\")[\"domain\"].to_dict()\n",
    "\n",
    "    ucols = [c for c in u.columns if c.lower().startswith(\"emb_\")]\n",
    "    icols = [c for c in i.columns if c.lower().startswith(\"emb_\")]\n",
    "\n",
    "    U = u[ucols].to_numpy(np.float32); U = l2norm(U)   # ì•ˆì „í•˜ê²Œ í•œ ë²ˆ ë” ì •ê·œí™”\n",
    "    I = i[icols].to_numpy(np.float32); I = l2norm(I)\n",
    "    item_ids = i[\"content_id\"].to_numpy(int)\n",
    "\n",
    "    rec_rows = []\n",
    "    for r in range(U.shape[0]):\n",
    "        uvec = U[r:r+1, :]                   # [1, D]\n",
    "        sims = (I @ uvec.T).reshape(-1)      # [N,]\n",
    "\n",
    "        # 1) ë„ë©”ì¸ë³„ quota ì±„ìš°ê¸°\n",
    "        picked_mask = np.zeros_like(sims, dtype=bool)\n",
    "        per_domain_picks = []\n",
    "        for dom, need in DOMAIN_QUOTA.items():\n",
    "            if need <= 0: \n",
    "                continue\n",
    "            # í•´ë‹¹ ë„ë©”ì¸ ì•„ì´í…œ ë§ˆìŠ¤í¬\n",
    "            dom_mask = np.array([dom_map.get(int(cid)) == dom for cid in item_ids], dtype=bool)\n",
    "            if not dom_mask.any():\n",
    "                continue\n",
    "            # ì•„ì§ ì•ˆ ë½‘íŒ + í•´ë‹¹ ë„ë©”ì¸\n",
    "            cand_mask = dom_mask & (~picked_mask)\n",
    "            if not cand_mask.any():\n",
    "                continue\n",
    "            cand_idx = np.where(cand_mask)[0]\n",
    "            cand_scores = sims[cand_idx]\n",
    "\n",
    "            k = min(need, len(cand_idx))\n",
    "            part = np.argpartition(-cand_scores, k-1)[:k]\n",
    "            order = part[np.argsort(-cand_scores[part])]\n",
    "            chosen = cand_idx[order]\n",
    "\n",
    "            picked_mask[chosen] = True\n",
    "            for j in chosen:\n",
    "                per_domain_picks.append((int(item_ids[j]), float(sims[j]), dom))\n",
    "\n",
    "        # 2) ë¶€ì¡±ë¶„ ê¸€ë¡œë²Œ Topìœ¼ë¡œ ë³´ì¶©(ì˜µì…˜)\n",
    "        total_need = sum(max(0, n) for n in DOMAIN_QUOTA.values())\n",
    "        if FILL_WITH_GLOBAL and picked_mask.sum() < total_need:\n",
    "            remain = total_need - picked_mask.sum()\n",
    "            left_idx = np.where(~picked_mask)[0]\n",
    "            if len(left_idx) > 0:\n",
    "                left_scores = sims[left_idx]\n",
    "                k2 = min(remain, len(left_idx))\n",
    "                part = np.argpartition(-left_scores, k2-1)[:k2]\n",
    "                order = part[np.argsort(-left_scores[part])]\n",
    "\n",
    "                chosen = left_idx[order]\n",
    "                for j in chosen:\n",
    "                    c = int(item_ids[j])\n",
    "                    per_domain_picks.append((c, float(sims[j]), dom_map.get(c, \"\")))\n",
    "                    picked_mask[j] = True\n",
    "\n",
    "        # 3) rank ë§¤ê¸°ê³  ì €ì¥\n",
    "        per_domain_picks.sort(key=lambda x: -x[1])\n",
    "        uid = int(u.loc[r, \"user_id\"]); uname = str(u.loc[r, \"username\"])\n",
    "        for rank, (cid, sc, dom) in enumerate(per_domain_picks, start=1):\n",
    "            rec_rows.append([uid, uname, rank, cid, sc, dom])\n",
    "\n",
    "    rec = pd.DataFrame(rec_rows, columns=[\"user_id\",\"username\",\"rank\",\"content_id\",\"score\",\"domain\"])\n",
    "\n",
    "    # ì œëª© join í•´ì„œ content_title ì»¬ëŸ¼ ì¶”ê°€ (ì—¬ê¸°ì„œ contentsëŠ” ì´ë¯¸ \"ì œëª© ìˆëŠ” ê²ƒë§Œ\"ì´ë¼, ë¹ˆ íƒ€ì´í‹€ ì—†ìŒ)\n",
    "    name_df = contents[[\"content_id\", title_col]].drop_duplicates(\"content_id\")\n",
    "    rec = rec.merge(name_df, on=\"content_id\", how=\"left\")\n",
    "    rec = rec.rename(columns={title_col: \"content_title\"})\n",
    "\n",
    "    # ì €ì¥ ì»¬ëŸ¼ ìˆœì„œ\n",
    "    rec = rec.sort_values([\"user_id\",\"rank\"])\n",
    "    rec_cols = [\"user_id\",\"username\",\"rank\",\"content_id\",\"content_title\",\"score\",\"domain\"]\n",
    "    rec[rec_cols].to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… recommendations_per_domain.csv ì €ì¥: {OUT_CSV} (rows={len(rec)})\")\n",
    "\n",
    "    # ì½˜ì†” ìš”ì•½\n",
    "    try:\n",
    "        for uid in sorted(rec[\"user_id\"].unique()):\n",
    "            sub = rec[rec[\"user_id\"]==uid].sort_values(\"rank\")\n",
    "            print(f\"\\n[User {uid}] ì´ {len(sub)}ê°œ\")\n",
    "            for _, row in sub.iterrows():\n",
    "                title = \"\"\n",
    "                if isinstance(row[\"content_title\"], str) and row[\"content_title\"]:\n",
    "                    title = f\"  name={row['content_title']}\"\n",
    "                print(f\"  #{int(row['rank']):2d}  cid={int(row['content_id'])}  dom={row['domain']}  sim={row['score']:.4f}{title}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
